{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras DSVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This recipe shows how to run Keras using Batch AI on DSVM. DSVM supports tensorflow, cntk and theano backends for running Keras. Currently only tensorflow and cntk backends supports running on GPU.\n",
    "\n",
    "## Details\n",
    "\n",
    "- DSVM has Keras framework preinstalled;\n",
    "- Standard keras sample script [mnist_cnn.py](https://raw.githubusercontent.com/fchollet/keras/master/examples/mnist_cnn.py) is used;\n",
    "- The script downloads the standard MNIST Database on its own;\n",
    "- Standard output of the job will be stored on Azure File Share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "Follow [instructions](/recipes) to install all dependencies and create configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "from azure.storage.file import FileService\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# utilities.py contains helper functions used by different notebooks\n",
    "import utilities\n",
    "\n",
    "cfg = utilities.Configuration('./configuration.json')\n",
    "client = utilities.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaidsvmsample` under your storage account.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_share(azure_file_share_name, fail_on_exist=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "\n",
    "- For this example we will use a gpu cluster of `STANDARD_NC6` nodes. Number of nodes in the cluster is configured with `nodes_count` variable;\n",
    "- We will mount file share at folder with name `external`. Full path of this folder on a computer node will be `$AZ_BATCHAI_MOUNT_ROOT/external`;\n",
    "- We will call the cluster `nc6`;\n",
    "\n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "azure_file_share = 'external'\n",
    "nodes_count = 2\n",
    "cluster_name = 'nc6'\n",
    "\n",
    "volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            azure_file_url = 'https://{0}.file.core.windows.net/{1}'.format(\n",
    "                cfg.storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share)\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=cfg.location,\n",
    "    vm_size=\"STANDARD_NC6\",\n",
    "    virtual_machine_configuration=models.VirtualMachineConfiguration(\n",
    "        image_reference=models.ImageReference(\n",
    "            publisher=\"microsoft-ads\",\n",
    "            offer=\"linux-data-science-vm-ubuntu\",\n",
    "            sku=\"linuxdsvmubuntu\",\n",
    "            version=\"latest\")),\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        auto_scale=models.AutoScaleSettings(initial_node_count=1, minimum_node_count=1, maximum_node_count=2)\n",
    "    ),\n",
    "    node_setup=models.NodeSetup(\n",
    "        mount_volumes=volumes\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = client.clusters.create(cfg.resource_group, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "utilities.py contains a helper function allowing to wait for the cluster to become available - all nodes are allocated and finished preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Allocated: 1; Idle: 1; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n"
     ]
    }
   ],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cluster_name)\n",
    "utilities.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Sample Script and Configure the Input Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each job we will create a folder containing a copy of [train_mnist.py](https://github.com/chainer/chainer/blob/master/examples/mnist/train_mnist.py). This allows each job to have it's own copy of the sample script (in case you would like to change it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "keras_sample_dir = \"learner-attrition\"\n",
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_directory(\n",
    "    azure_file_share_name, keras_sample_dir, fail_on_exist=False)\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, keras_sample_dir, 'download_data.py', 'download_data.py')\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, keras_sample_dir, 'model.py', 'model.py')\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, keras_sample_dir, 'params.json', 'params.json')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The job needs to know where to find train_mnist.py script (the chainer will download MNIST dataset on its own). So, we will configure an input directory for the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_directories = [\n",
    "    models.InputDirectory(\n",
    "        id='SCRIPT',\n",
    "        path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, keras_sample_dir))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job will be able to reference those directories using ```$AZ_BATCHAI_INPUT_SCRIPT``` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Output Directories\n",
    "We will store standard and error output of the job in File Share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_output_path_prefix = \"$AZ_BATCHAI_MOUNT_ROOT/{0}\".format(azure_file_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directories = [\n",
    "    models.OutputDirectory(\n",
    "        id='MODEL',\n",
    "        path_prefix='$AZ_BATCHAI_MOUNT_ROOT/{0}'.format(azure_file_share),\n",
    "        path_suffix=\"Models\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "\n",
    "- Will use configured previously input and output directories;\n",
    "- Will run standard `mnist_cnn.py` from SCRIPT input directory using custom framework;\n",
    "- Keral will use theano backend; DSVM supports cntk, tensorflow and theano backends for keral, just change KERAS_BACKEND to \"tensorflow\" or \"theano\" to use corresponding backend. Note, theano backend will run on CPU. \n",
    "- Will output standard output and error streams to file share.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = datetime.utcnow().strftime(\"la_%m_%d_%Y_%H%M%S\")\n",
    "parameters = models.job_create_parameters.JobCreateParameters(\n",
    "    location=cfg.location,\n",
    "    cluster=models.ResourceId(cluster.id),\n",
    "    node_count=1,\n",
    "    input_directories=input_directories,\n",
    "    output_directories=output_directories,\n",
    "    std_out_err_path_prefix=std_output_path_prefix,\n",
    "    job_preparation=models.JobPreparation(\n",
    "        command_line=\"python $AZ_BATCHAI_INPUT_SCRIPT/download_data.py --az-tenant-id {} --az-sp-client-id {} --az-sp-client-secret {} --datalake-store-name {}\".format(\n",
    "            cfg.aad_tenant_id, cfg.aad_client_id, cfg.aad_secret_key, cfg.datalake_store_name\n",
    "        )\n",
    "    ),\n",
    "    custom_toolkit_settings = models.CustomToolkitSettings(\n",
    "        command_line='KERAS_BACKEND=cntk python $AZ_BATCHAI_INPUT_SCRIPT/model.py --course-id Microsoft+DAT206x+4T2017 --train --num-epochs 1 --batch-size 256 --positive-upweight 3 --lr 0.01 --layers-config-file $AZ_BATCHAI_INPUT_SCRIPT/params.json --outputdir $AZ_BATCHAI_OUTPUT_MODEL'\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Job: la_01_30_2018_023203\n"
     ]
    }
   ],
   "source": [
    "job = client.jobs.create(cfg.resource_group, job_name, parameters).result()\n",
    "print('Created Job: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enought idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stdout.txt.\n",
    "\n",
    "**Note** Execution may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Allocated: 1; Idle: 0; Unusable: 0; Running: 1; Preparing: 0; Leaving: 0\n",
      "Job state: running ExitCode: None\n",
      "Waiting for job output to become available...\n",
      "STARTING\n",
      "ARGS ALL GOOD {'outputdir': '/mnt/batch/tasks/shared/LS_root/mounts/external/fd74930d-c060-4ff4-a7f1-9470f7ad7f8f/learner-attrition-supp/jobs/la_01_30_2018_023203/8ed4d4d0-ed6e-4a3c-85b8-e2e71bbd2212/outputs/Models', 'num_epochs': 1, 'train': True, 'layers_config_file': '/mnt/batch/tasks/shared/LS_root/mounts/external/learner-attrition/params.json', 'batch_size': 256, 'course_id': 'Microsoft+DAT206x+4T2017', 'positive_upweight': 3.0, 'lr': 0.01}\n",
      "GETTING DATA: \n",
      "model_data.csv does not exist for course:  Microsoft+DAT206x+JPN+1T2017\n",
      "model_data.csv does not exist for course:  top_course_ids.txt\n",
      "model_data.csv does not exist for course:  Microsoft+DAT207x+1T2018\n",
      "model_data.csv does not exist for course:  Microsoft+DAT205x+3T2016\n",
      "model_data.csv does not exist for course:  Microsoft+DAT206x+1T2018\n",
      "model_data.csv does not exist for course:  Microsoft+DAT215.4x+1T2017\n",
      "model_data.csv does not exist for course:  Microsoft+DAT206x+6T2016\n",
      "model_data.csv does not exist for course:  Microsoft+DAT215.3x+3T2017\n",
      "model_data.csv does not exist for course:  _SUCCESS\n",
      "model_data.csv does not exist for course:  Microsoft+DAT206x+JPN+2T2017\n",
      "Training data done.\n",
      "Done.\n",
      "Fitting model\n",
      "Train on 1976682 samples, validate on 658894 samples\n",
      "Epoch 1/1\n",
      " - 90s - loss: 0.7988 - acc: 0.6995 - val_loss: 0.5521 - val_acc: 0.6849\n",
      "Job state: failed ExitCode: 1\n",
      "FailureDetails: \n",
      "ErrorCode:JobFailed\n",
      "ErrorMessage:Job failed with non-zero exit code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utilities.wait_for_job_completion(client, cfg.resource_group, job_name, cluster_name, 'stdouterr', 'stdout.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download stdout.txt and stderr.txt files for the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://learnerattrition.file.core.windows.net/batchaisample/fd74930d-c060-4ff4-a7f1-9470f7ad7f8f/learner-attrition-supp/jobs/la_01_30_2018_023203/8ed4d4d0-ed6e-4a3c-85b8-e2e71bbd2212/stderr-job_prep.txt?sv=2016-05-31&sr=f&sig=TP4jvXeA0F3Dg1rZOmKUv0ETZAecKPm25Pr7ih6cYQo%3D&se=2018-01-30T03%3A37%3A13Z&sp=rl ...Done\n",
      "Downloading https://learnerattrition.file.core.windows.net/batchaisample/fd74930d-c060-4ff4-a7f1-9470f7ad7f8f/learner-attrition-supp/jobs/la_01_30_2018_023203/8ed4d4d0-ed6e-4a3c-85b8-e2e71bbd2212/stderr.txt?sv=2016-05-31&sr=f&sig=5p%2FSsr9R4YRr%2BZbGe0LvfQqbZWczoM97mUQwTv5FOqw%3D&se=2018-01-30T03%3A37%3A13Z&sp=rl ...Done\n",
      "Downloading https://learnerattrition.file.core.windows.net/batchaisample/fd74930d-c060-4ff4-a7f1-9470f7ad7f8f/learner-attrition-supp/jobs/la_01_30_2018_023203/8ed4d4d0-ed6e-4a3c-85b8-e2e71bbd2212/stdout-job_prep.txt?sv=2016-05-31&sr=f&sig=JSOojAwb4%2BiGdgA8TDq2oDOlyRZ5foKXmokG5qVLjg0%3D&se=2018-01-30T03%3A37%3A13Z&sp=rl ...Done\n",
      "Downloading https://learnerattrition.file.core.windows.net/batchaisample/fd74930d-c060-4ff4-a7f1-9470f7ad7f8f/learner-attrition-supp/jobs/la_01_30_2018_023203/8ed4d4d0-ed6e-4a3c-85b8-e2e71bbd2212/stdout.txt?sv=2016-05-31&sr=f&sig=uq58fL%2FdROHip%2B%2BSXA4rF%2F8S0HXtZ317twq3vzW2Trc%3D&se=2018-01-30T03%3A37%3A13Z&sp=rl ...Done\n",
      "All files downloaded\n"
     ]
    }
   ],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, job_name, models.JobsListOutputFilesOptions(\"stdOuterr\")) \n",
    "for f in list(files):\n",
    "    utilities.download_file(f.download_url, f.name)\n",
    "print(\"All files downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout.txt content:\n",
      "bash: /mnt/batch/tasks/workitems/la_01_30_2018_023203_8ed4d4d0-ed6e-4a3c-85b8-e2e71bbd2212/job-1/la_01_30_2018_023203_8ed4d4d0-ed6e-4a3c-85b8-e2e71bbd2212/wd/.bashrc: No such file or directory\n",
      "Selected GPU[0] Tesla K80 as the process wide default device.\n",
      "[Note:] Trainer ctor: 10 of the model parameters are not covered by any of the specified Learners; these parameters will not be learned\n",
      "Using CNTK backend\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input768\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/learner-attrition/model.py\", line 265, in <module>\n",
      "    args['outputdir'])\n",
      "  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/learner-attrition/model.py\", line 224, in run_model\n",
      "    final_recall = metrics.recall_score(y_val_true, v_val_pred)\n",
      "NameError: name 'v_val_pred' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stdout.txt content:')\n",
    "with open('stderr.txt') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.jobs.delete(cfg.resource_group, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.delete(cfg.resource_group, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
