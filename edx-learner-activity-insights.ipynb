{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive modeling of edX Learner Attrition\n",
    "\n",
    "**Problem:** MOOC courses have notoriously high dropout rates and Microsoft courses are no exception. How can we understand dropout trends and do something about them to keep users engaged\n",
    "\n",
    "**Solution:** Intervene with users before they drop out.\n",
    "\n",
    "**How?** Predict whether a user in a course will drop out in the next week of the course and engage them with support resources.\n",
    "* Aggregate user events to understand engagement paterns\n",
    "* Determine drop out weeks for each user in a course\n",
    "* Build a model to predict the determined dropout weeks\n",
    "* Build a cloud service to email \"at-risk\" users with support resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymssql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import cntk as C\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREDS = [[-0.00086074],\n",
    " [-0.00159799],\n",
    " [-0.00169993],\n",
    " [-0.04512118],\n",
    " [-0.04508126],\n",
    " [ 0.51217449]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.],\n",
       "       [-0.],\n",
       "       [-0.],\n",
       "       [-0.],\n",
       "       [-0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.array(PREDS)\n",
    "np.round(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.94200321e-02],\n",
       "       [  7.80971257e-02],\n",
       "       [  7.79142067e-02],\n",
       "       [  0.00000000e+00],\n",
       "       [  7.16316350e-05],\n",
       "       [  1.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds - preds.min()) / (preds.max() - preds.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55729567000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.max() - preds.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microsoft+DAT206x+2T2017',\n",
       " 'Microsoft+DAT207x+3T2017',\n",
       " 'Microsoft+DAT207x+1T2017',\n",
       " 'Microsoft+DAT206x+1T2017',\n",
       " 'Microsoft+DAT201x+4T2017',\n",
       " 'Microsoft+DAT222x+4T2017',\n",
       " 'Microsoft+DAT207x+4T2017',\n",
       " 'Microsoft+DAT209x+4T2017',\n",
       " 'Microsoft+DAT201x+3T2017',\n",
       " 'Microsoft+DAT221x+4T2017',\n",
       " 'Microsoft+DAT203.2x+4T2017',\n",
       " 'Microsoft+DAT207x+2T2017',\n",
       " 'Microsoft+DAT206x+3T2017',\n",
       " 'Microsoft+DAT203.1x+4T2017']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_course_id = 'Microsoft+DAT206x+4T2017' \n",
    "\n",
    "path = 'data'\n",
    "past_course_ids = [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "try:\n",
    "    past_course_ids.remove(current_course_id)\n",
    "except ValueError:\n",
    "    print('Not in list')\n",
    "\n",
    "past_course_ids\n",
    "test = pd.read_csv('{}/{}/model_data.csv'.format('data', current_course_id))\n",
    "for course_id in past_course_ids:\n",
    "    course_data = pd.read_csv('{}/{}/model_data.csv'.format('data', course_id))\n",
    "    train_test_split(course_data, 0.7)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Let's look at a sample from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [ \n",
    "    'user_id', 'course_week', 'user_week', 'video_plays',\n",
    "    'subsections_viewed', 'problems_attempted', 'problems_correct',\n",
    "    'forum_posts', 'forum_votes_count', 'avg_forum_sentiment'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>course_week</th>\n",
       "      <th>user_started_week</th>\n",
       "      <th>num_video_plays</th>\n",
       "      <th>num_subsections_viewed</th>\n",
       "      <th>num_problems_attempted</th>\n",
       "      <th>num_problems_correct</th>\n",
       "      <th>num_forum_posts</th>\n",
       "      <th>num_forum_up_votes</th>\n",
       "      <th>avg_forum_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  course_week  user_started_week  num_video_plays  \\\n",
       "0     9999            4                  3             12.0   \n",
       "1   100001            4                  0              0.0   \n",
       "\n",
       "   num_subsections_viewed  num_problems_attempted  num_problems_correct  \\\n",
       "0                    23.0                     8.0                   5.0   \n",
       "1                     0.0                     0.0                   0.0   \n",
       "\n",
       "   num_forum_posts  num_forum_up_votes  avg_forum_sentiment  \n",
       "0              1.0                   3                 0.72  \n",
       "1              0.0                   0                 0.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.DataFrame([\n",
    "    {\n",
    "        'user_id': 9999,\n",
    "        'course_week': 4,\n",
    "        'user_started_week': 3,\n",
    "        'num_video_plays': 12.,\n",
    "        'num_subsections_viewed': 23.,\n",
    "        'num_problems_attempted': 8.,\n",
    "        'num_problems_correct': 5.,\n",
    "        'num_forum_posts': 1.,\n",
    "        'num_forum_up_votes': 3,\n",
    "        'avg_forum_sentiment': .72\n",
    "    },\n",
    "    {\n",
    "        'user_id': 100001,\n",
    "        'course_week': 4,\n",
    "        'user_started_week': 0,\n",
    "        'num_video_plays': 0.,\n",
    "        'num_subsections_viewed': 0.,\n",
    "        'num_problems_attempted': 0.,\n",
    "        'num_problems_correct': 0.,\n",
    "        'num_forum_posts': 0.,\n",
    "        'num_forum_up_votes': 0,\n",
    "        'avg_forum_sentiment': .0\n",
    "    }\n",
    "])\n",
    "\n",
    "features[[\n",
    "    'user_id', 'course_week', 'user_started_week', 'num_video_plays',\n",
    "    'num_subsections_viewed', 'num_problems_attempted', 'num_problems_correct',\n",
    "    'num_forum_posts', 'num_forum_up_votes', 'avg_forum_sentiment'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Value (0 or 1): user_dropped_out_next_course_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pull data from SQL Server Staging and Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = pymssql.connect(server=wwl_data_warehouse_server, user=user, password=password) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "course_id = 'Microsoft+DAT222x+4T2017' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_query = \"\"\"\n",
    "SELECT\n",
    "    Username, UserId, EventType, EventSource, CourseId,\n",
    "    EventGrade, EventAttempts, EventMaxGrade, EventSub_Correct, EventTime\n",
    "FROM [EdxStaging].[edx].[Edx_DailyEvents]\n",
    "WHERE (Host = 'courses.edx.org' and CourseId = '{}')\n",
    "AND UserId IS NOT NULL\n",
    "\"\"\".format(course_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = pd.read_sql(events_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2017, 10, 17, 0, 0), datetime.datetime(2018, 1, 1, 0, 0))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COURSE_DATES_QUERY = \"\"\"\n",
    "SELECT TOP(1) [CourseRunStartDate],[CourseRunEndDate]\n",
    "FROM [EdxDW].[edx].[DimCourse] C\n",
    "WHERE C.[CourseRunId] = '{}'\n",
    "\"\"\".format(course_id)\n",
    "course_dates = pd.read_sql(COURSE_DATES_QUERY, conn)\n",
    "course_start_date = datetime.strptime(course_dates['CourseRunStartDate'][0], '%Y-%m-%d')\n",
    "course_end_date = datetime.strptime(course_dates['CourseRunEndDate'][0], '%Y-%m-%d')\n",
    "(course_start_date, course_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forums_query = \"\"\"\n",
    "SELECT [Title]\n",
    "      ,[CommentText]\n",
    "      ,[AuthorId]\n",
    "      ,[AuthorUserName]\n",
    "      ,[VotesUpCount]\n",
    "      ,[VotesDownCount]\n",
    "      ,[VotesCount]\n",
    "      ,[VotesPoint]              \n",
    "      ,[CommentCount]      \n",
    "      ,[ParentId]\n",
    "      ,[CommentThreadId]\n",
    "      ,[CourseId]\n",
    "      ,[TextType]       \n",
    "      ,[UpdateTimestamp]\n",
    "  FROM [EdxStaging].[edx].[Edx_Forum]\n",
    "  WHERE CourseId = '{}'\n",
    "\"\"\".format(course_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forums = pd.read_sql(forums_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>username</th>\n",
       "      <th>votes_up</th>\n",
       "      <th>votes_down</th>\n",
       "      <th>votes_count</th>\n",
       "      <th>votes_point</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_thread_id</th>\n",
       "      <th>course_id</th>\n",
       "      <th>text_type</th>\n",
       "      <th>update_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Hi myusrn\\n\\nThe pre-test is used to assess yo...</td>\n",
       "      <td>16524953</td>\n",
       "      <td>JSmith-Lex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59f85002bc04a609f6002b76</td>\n",
       "      <td>59f60ae0bc04a609f0002893</td>\n",
       "      <td>Microsoft+DAT222x+4T2017</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2017-11-03T01:46:11.091Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Maybe you should consider changing the questio...</td>\n",
       "      <td>2556761</td>\n",
       "      <td>sjmathis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>59eeadb04ed74909e8002433</td>\n",
       "      <td>Microsoft+DAT222x+4T2017</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2017-11-03T01:38:15.181Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Your pre-test is ridiculous. Why would you mak...</td>\n",
       "      <td>10037972</td>\n",
       "      <td>B212bb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59f05eaebc04a609fc002775</td>\n",
       "      <td>59ef598fbc04a60a2b002557</td>\n",
       "      <td>Microsoft+DAT222x+4T2017</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2017-10-27T02:36:26.510Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to the course. Happy learning!</td>\n",
       "      <td>16524953</td>\n",
       "      <td>JSmith-Lex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>59f1dbb1bc04a60a2f0027b5</td>\n",
       "      <td>Microsoft+DAT222x+4T2017</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2017-10-27T01:31:45.164Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to the course. Happy learning!</td>\n",
       "      <td>16524953</td>\n",
       "      <td>JSmith-Lex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>59f2241dbc04a60a07002797</td>\n",
       "      <td>Microsoft+DAT222x+4T2017</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2017-10-27T01:31:21.657Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title                                       comment_text  author_id  \\\n",
       "0  None  Hi myusrn\\n\\nThe pre-test is used to assess yo...   16524953   \n",
       "1  None  Maybe you should consider changing the questio...    2556761   \n",
       "2  None  Your pre-test is ridiculous. Why would you mak...   10037972   \n",
       "3  None             Welcome to the course. Happy learning!   16524953   \n",
       "4  None             Welcome to the course. Happy learning!   16524953   \n",
       "\n",
       "     username  votes_up  votes_down  votes_count  votes_point  comment_count  \\\n",
       "0  JSmith-Lex         0           0            0            0            NaN   \n",
       "1    sjmathis         0           0            0            0            NaN   \n",
       "2      B212bb         0           0            0            0            NaN   \n",
       "3  JSmith-Lex         0           0            0            0            NaN   \n",
       "4  JSmith-Lex         0           0            0            0            NaN   \n",
       "\n",
       "                  parent_id         comment_thread_id  \\\n",
       "0  59f85002bc04a609f6002b76  59f60ae0bc04a609f0002893   \n",
       "1                      None  59eeadb04ed74909e8002433   \n",
       "2  59f05eaebc04a609fc002775  59ef598fbc04a60a2b002557   \n",
       "3                      None  59f1dbb1bc04a60a2f0027b5   \n",
       "4                      None  59f2241dbc04a60a07002797   \n",
       "\n",
       "                  course_id text_type          update_timestamp  \n",
       "0  Microsoft+DAT222x+4T2017   Comment  2017-11-03T01:46:11.091Z  \n",
       "1  Microsoft+DAT222x+4T2017   Comment  2017-11-03T01:38:15.181Z  \n",
       "2  Microsoft+DAT222x+4T2017   Comment  2017-10-27T02:36:26.510Z  \n",
       "3  Microsoft+DAT222x+4T2017   Comment  2017-10-27T01:31:45.164Z  \n",
       "4  Microsoft+DAT222x+4T2017   Comment  2017-10-27T01:31:21.657Z  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forums.columns=[\n",
    "    'title', 'comment_text', 'author_id', 'username', 'votes_up', 'votes_down', \n",
    "    'votes_count', 'votes_point', 'comment_count', 'parent_id', 'comment_thread_id',\n",
    "    'course_id', 'text_type', 'update_timestamp'\n",
    "]\n",
    "forums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "course_start_query = \"\"\"\n",
    "SELECT U.UserId as user_id\n",
    "      ,CS.[DateKey] as date_key\n",
    "  FROM [EdxDW].[edx].[FactCourseStart] CS\n",
    "  JOIN [EdxDW].[edx].[DimUser] U\n",
    "  ON CS.UserKey = U.UserKey\n",
    "  JOIN [EdxDW].[edx].[DimUserPII] PII\n",
    "  ON PII.UserKey = U.UserKey\n",
    "  JOIN [EdxDW].[edx].[DimCourse] C\n",
    "  ON CS.CourseKey = C.CourseKey\n",
    "  WHERE C.CourseRunId = '{}'\n",
    "  AND IsBadRow = 0\n",
    "  ORDER BY DateKey\n",
    "\"\"\".format(course_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "course_starts = pd.read_sql(course_start_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14868851</td>\n",
       "      <td>20170828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6467324</td>\n",
       "      <td>20170905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6654643</td>\n",
       "      <td>20170918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6675042</td>\n",
       "      <td>20171007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8775892</td>\n",
       "      <td>20171010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  date_key\n",
       "0  14868851  20170828\n",
       "1   6467324  20170905\n",
       "2   6654643  20170918\n",
       "3   6675042  20171007\n",
       "4   8775892  20171010"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_starts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "course_complete_query = \"\"\"\n",
    "SELECT U.UserId as user_id\n",
    "      ,CC.[DateKey] as date_key\n",
    "  FROM [EdxDW].[edx].[FactCourseCompletion] CC\n",
    "  JOIN [EdxDW].[edx].[DimUser] U\n",
    "  ON CC.UserKey = U.UserKey\n",
    "  JOIN [EdxDW].[edx].[DimUserPII] PII\n",
    "  ON PII.UserKey = U.UserKey\n",
    "  JOIN [EdxDW].[edx].[DimCourse] C\n",
    "  ON CC.CourseKey = C.CourseKey\n",
    "  WHERE C.CourseRunId = '{}'\n",
    "  AND IsBadRow = 0\n",
    "  ORDER BY DateKey\n",
    "\"\"\".format(course_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "course_completions = pd.read_sql(course_complete_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_source</th>\n",
       "      <th>course_id</th>\n",
       "      <th>event_grade</th>\n",
       "      <th>event_attempts</th>\n",
       "      <th>event_max_grade</th>\n",
       "      <th>event_sub_correct</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CFRandall</td>\n",
       "      <td>5267382</td>\n",
       "      <td>/courses/course-v1:Microsoft+DAT201x+4T2017/xb...</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT201x+4T2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-09-07T22:32:44.446885+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CFRandall</td>\n",
       "      <td>5267382</td>\n",
       "      <td>/courses/course-v1:Microsoft+DAT201x+4T2017/xb...</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT201x+4T2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-09-07T22:32:40.749985+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Graeme_Malcolm</td>\n",
       "      <td>6467324</td>\n",
       "      <td>/courses/course-v1:Microsoft+DAT201x+4T2017/xb...</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT201x+4T2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-09-07T17:19:29.860471+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Graeme_Malcolm</td>\n",
       "      <td>6467324</td>\n",
       "      <td>/courses/course-v1:Microsoft+DAT201x+4T2017/co...</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT201x+4T2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-09-07T17:19:13.849931+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Graeme_Malcolm</td>\n",
       "      <td>6467324</td>\n",
       "      <td>/courses/course-v1:Microsoft+DAT201x+4T2017/di...</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT201x+4T2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-09-07T18:05:04.894596+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         username  user_id                                         event_type  \\\n",
       "0       CFRandall  5267382  /courses/course-v1:Microsoft+DAT201x+4T2017/xb...   \n",
       "1       CFRandall  5267382  /courses/course-v1:Microsoft+DAT201x+4T2017/xb...   \n",
       "2  Graeme_Malcolm  6467324  /courses/course-v1:Microsoft+DAT201x+4T2017/xb...   \n",
       "3  Graeme_Malcolm  6467324  /courses/course-v1:Microsoft+DAT201x+4T2017/co...   \n",
       "4  Graeme_Malcolm  6467324  /courses/course-v1:Microsoft+DAT201x+4T2017/di...   \n",
       "\n",
       "  event_source                 course_id  event_grade  event_attempts  \\\n",
       "0       server  Microsoft+DAT201x+4T2017          NaN             NaN   \n",
       "1       server  Microsoft+DAT201x+4T2017          NaN             NaN   \n",
       "2       server  Microsoft+DAT201x+4T2017          NaN             NaN   \n",
       "3       server  Microsoft+DAT201x+4T2017          NaN             NaN   \n",
       "4       server  Microsoft+DAT201x+4T2017          NaN             NaN   \n",
       "\n",
       "   event_max_grade event_sub_correct                        event_time  \n",
       "0              NaN              None  2017-09-07T22:32:44.446885+00:00  \n",
       "1              NaN              None  2017-09-07T22:32:40.749985+00:00  \n",
       "2              NaN              None  2017-09-07T17:19:29.860471+00:00  \n",
       "3              NaN              None  2017-09-07T17:19:13.849931+00:00  \n",
       "4              NaN              None  2017-09-07T18:05:04.894596+00:00  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.columns = [\n",
    "    'username', 'user_id', 'event_type', 'event_source', 'course_id',\n",
    "    'event_grade', 'event_attempts', 'event_max_grade', 'event_sub_correct',\n",
    "    'event_time'\n",
    "]\n",
    "events['user_id'] = events['user_id'].astype('int64')\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Event Data to expose only the events we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only include events for users that have \"started\" the course \n",
    "# according to Data Warehouse definitions\n",
    "events = pd.merge(course_starts, events, how='inner', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: These dates need to be updated for scaling to multiple courses\n",
    "\n",
    "LONG_DATE_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "SHORT_DATE_FORMAT = \"%Y%m%d\"\n",
    "course_start_date = datetime.strptime(\"2017-07-01T00:00:00\", LONG_DATE_FORMAT)\n",
    "course_end_date = datetime.strptime(\"2017-09-30T23:59:00\", LONG_DATE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def course_week(date):\n",
    "    \"\"\"\n",
    "    Find the course week for the event time\n",
    "    \"\"\"\n",
    "    course_week = math.ceil((date - course_start_date).days / 7)\n",
    "    if course_week == 0:\n",
    "        course_week = 1\n",
    "    return course_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'datetime.datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-61f391a8fa47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcourse_week\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2017-07-01T00:00:00'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-232-9cd0f6ccf230>\u001b[0m in \u001b[0;36mcourse_week\u001b[0;34m(date)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcourse\u001b[0m \u001b[0mweek\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mevent\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcourse_week\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcourse_start_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcourse_week\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcourse_week\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'datetime.datetime'"
     ]
    }
   ],
   "source": [
    "course_week('2017-07-01T00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_events(events_df):\n",
    "    \"\"\"\n",
    "    1. Filter events to relevant (event_type)s\n",
    "    2. Define the course_week of the event based on event_time\n",
    "    3. Set correct problem attempts to a new event_type to \n",
    "       aggregate on.\n",
    "    \"\"\"\n",
    "    \n",
    "    events_to_capture = [\n",
    "        'problem_check',\n",
    "        'play_video',    \n",
    "        'book'\n",
    "    ]\n",
    "\n",
    "    events_sub = events_df.loc[events['event_type'].isin(events_to_capture),:]\n",
    "    events_sub = events_sub.drop(\n",
    "        events_sub.loc[(events_sub['event_type'] == 'problem_check') & (events_sub['event_source'] == 'browser'),:].index\n",
    "    )\n",
    "    SUBSECTION_VIEWED_MARKER = 'subsection_viewed'\n",
    "\n",
    "    events_df.loc[events_df['event_type'].str.contains('/courseware/'),'event_type'] = SUBSECTION_VIEWED_MARKER\n",
    "    subsection_events = events_df.loc[events_df['event_type'] == SUBSECTION_VIEWED_MARKER,:]\n",
    "    events_sub = events_sub.fillna(0.0)\n",
    "    events_ = events_sub.append(subsection_events)\n",
    "    events_ = events_.reset_index(drop=True)\n",
    "    \n",
    "    events_['course_week'] = events_['event_time'].apply(\n",
    "        lambda date_string: course_week(datetime.strptime(date_string[0:19], LONG_DATE_FORMAT))\n",
    "    )\n",
    "    events_.loc[events_['event_sub_correct'] == 'true', 'event_type'] = 'problem_check:correct'\n",
    "    events_ = events_.set_index('user_id')\n",
    "    \n",
    "    return events_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_ = filter_events(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1348132, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>username</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_source</th>\n",
       "      <th>course_id</th>\n",
       "      <th>event_grade</th>\n",
       "      <th>event_attempts</th>\n",
       "      <th>event_max_grade</th>\n",
       "      <th>event_sub_correct</th>\n",
       "      <th>event_time</th>\n",
       "      <th>course_week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3734412</th>\n",
       "      <td>20170628</td>\n",
       "      <td>WLitzenberg</td>\n",
       "      <td>play_video</td>\n",
       "      <td>browser</td>\n",
       "      <td>Microsoft+DAT206x+3T2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-04T04:05:52.049667+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734412</th>\n",
       "      <td>20170628</td>\n",
       "      <td>WLitzenberg</td>\n",
       "      <td>play_video</td>\n",
       "      <td>browser</td>\n",
       "      <td>Microsoft+DAT206x+3T2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-02T19:58:22.096168+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734412</th>\n",
       "      <td>20170628</td>\n",
       "      <td>WLitzenberg</td>\n",
       "      <td>problem_check:correct</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT206x+3T2017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2017-07-01T02:04:42.936831+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734412</th>\n",
       "      <td>20170628</td>\n",
       "      <td>WLitzenberg</td>\n",
       "      <td>problem_check:correct</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT206x+3T2017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2017-07-01T22:11:46.342661+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734412</th>\n",
       "      <td>20170628</td>\n",
       "      <td>WLitzenberg</td>\n",
       "      <td>problem_check:correct</td>\n",
       "      <td>server</td>\n",
       "      <td>Microsoft+DAT206x+3T2017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2017-07-01T22:10:40.935088+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_key     username             event_type event_source  \\\n",
       "user_id                                                              \n",
       "3734412  20170628  WLitzenberg             play_video      browser   \n",
       "3734412  20170628  WLitzenberg             play_video      browser   \n",
       "3734412  20170628  WLitzenberg  problem_check:correct       server   \n",
       "3734412  20170628  WLitzenberg  problem_check:correct       server   \n",
       "3734412  20170628  WLitzenberg  problem_check:correct       server   \n",
       "\n",
       "                        course_id  event_grade  event_attempts  \\\n",
       "user_id                                                          \n",
       "3734412  Microsoft+DAT206x+3T2017          0.0             0.0   \n",
       "3734412  Microsoft+DAT206x+3T2017          0.0             0.0   \n",
       "3734412  Microsoft+DAT206x+3T2017          1.0             1.0   \n",
       "3734412  Microsoft+DAT206x+3T2017          1.0             1.0   \n",
       "3734412  Microsoft+DAT206x+3T2017          1.0             1.0   \n",
       "\n",
       "         event_max_grade event_sub_correct                        event_time  \\\n",
       "user_id                                                                        \n",
       "3734412              0.0                 0  2017-07-04T04:05:52.049667+00:00   \n",
       "3734412              0.0                 0  2017-07-02T19:58:22.096168+00:00   \n",
       "3734412              1.0              true  2017-07-01T02:04:42.936831+00:00   \n",
       "3734412              1.0              true  2017-07-01T22:11:46.342661+00:00   \n",
       "3734412              1.0              true  2017-07-01T22:10:40.935088+00:00   \n",
       "\n",
       "         course_week  \n",
       "user_id               \n",
       "3734412            1  \n",
       "3734412            1  \n",
       "3734412            1  \n",
       "3734412            1  \n",
       "3734412            1  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(events_.shape)\n",
    "events_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['play_video', 'problem_check:correct', 'problem_check',\n",
       "       'subsection_viewed'], dtype=object)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(events_['event_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Aggregate events and forum posts per (user_id, courseweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_events_features(events_df):\n",
    "    \"\"\"\n",
    "    Aggregate events to create grouped feature DataFrame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(events_df.groupby(['course_week', 'user_id'])['event_type'].value_counts())\n",
    "    features.columns = ['event_counts']\n",
    "    features = features.unstack()\n",
    "    features = features.fillna(0.0)\n",
    "    \n",
    "    features = features.reset_index()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_forum_data(forums_):\n",
    "    forums_df = forums_.copy()\n",
    "    forums_df['course_week'] = forums_df['update_timestamp'].apply(\n",
    "        lambda x: course_week(datetime.strptime(x[0:19], LONG_DATE_FORMAT))\n",
    "    )\n",
    "    forums_df = forums_df[['author_id', 'comment_text', 'votes_count', 'course_week']]\n",
    "\n",
    "    forums_df['sentiment'] = forums_df['comment_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "    return forums_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_forum_features(forums_df):\n",
    "    forums_ = process_forum_data(forums_df)\n",
    "\n",
    "    forum_features = forums_.groupby(['course_week', 'author_id'])[['comment_text', 'votes_count', 'sentiment']].agg({\n",
    "        'comment_text': 'count',\n",
    "        'votes_count': 'sum',\n",
    "        'sentiment': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    forum_features.columns = ['course_week', 'user_id', 'comment_text', 'votes_count', 'sentiment']\n",
    "    \n",
    "    return forum_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>course_week</th>\n",
       "      <th>user_id</th>\n",
       "      <th colspan=\"4\" halign=\"left\">event_counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>play_video</th>\n",
       "      <th>problem_check</th>\n",
       "      <th>problem_check:correct</th>\n",
       "      <th>subsection_viewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17198</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24487</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28435</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>28963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           course_week user_id event_counts                \\\n",
       "event_type                       play_video problem_check   \n",
       "0                    1    9201          1.0           5.0   \n",
       "1                    1   17198         15.0           6.0   \n",
       "2                    1   24487         62.0           5.0   \n",
       "3                    1   28435         13.0           1.0   \n",
       "4                    1   28963          0.0           0.0   \n",
       "\n",
       "                                                    \n",
       "event_type problem_check:correct subsection_viewed  \n",
       "0                           10.0              10.0  \n",
       "1                           21.0              23.0  \n",
       "2                           19.0              15.0  \n",
       "3                           27.0              15.0  \n",
       "4                            1.0               7.0  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_features = build_events_features(events_)\n",
    "event_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_features.columns = event_features.columns.get_level_values(0)\n",
    "features.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_week</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_counts</th>\n",
       "      <th>event_counts</th>\n",
       "      <th>event_counts</th>\n",
       "      <th>event_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17198</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24487</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28435</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>28963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   course_week  user_id  event_counts  event_counts  event_counts  \\\n",
       "0            1     9201           1.0           5.0          10.0   \n",
       "1            1    17198          15.0           6.0          21.0   \n",
       "2            1    24487          62.0           5.0          19.0   \n",
       "3            1    28435          13.0           1.0          27.0   \n",
       "4            1    28963           0.0           0.0           1.0   \n",
       "\n",
       "   event_counts  \n",
       "0          10.0  \n",
       "1          23.0  \n",
       "2          15.0  \n",
       "3          15.0  \n",
       "4           7.0  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_week</th>\n",
       "      <th>user_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>votes_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>311622</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>346516</td>\n",
       "      <td>0</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>679567</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1489478</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.042857</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1949018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   course_week  user_id  comment_text  votes_count  sentiment\n",
       "0            1   311622             0     0.000000          1\n",
       "1            1   346516             0     0.243750          2\n",
       "2            1   679567             0     0.000000          1\n",
       "3            1  1489478             1    -0.042857          1\n",
       "4            1  1949018             0     0.171875          2"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_features = build_forum_features(forums)\n",
    "forum_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_week</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_video_plays</th>\n",
       "      <th>num_problems_incorrect</th>\n",
       "      <th>num_problems_correct</th>\n",
       "      <th>num_subsections_viewed</th>\n",
       "      <th>num_forum_posts</th>\n",
       "      <th>num_forum_votes</th>\n",
       "      <th>avg_forum_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17198</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24487</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28435</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>28963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   course_week  user_id  num_video_plays  num_problems_incorrect  \\\n",
       "0            1     9201              1.0                     5.0   \n",
       "1            1    17198             15.0                     6.0   \n",
       "2            1    24487             62.0                     5.0   \n",
       "3            1    28435             13.0                     1.0   \n",
       "4            1    28963              0.0                     0.0   \n",
       "\n",
       "   num_problems_correct  num_subsections_viewed  num_forum_posts  \\\n",
       "0                  10.0                    10.0              NaN   \n",
       "1                  21.0                    23.0              NaN   \n",
       "2                  19.0                    15.0              NaN   \n",
       "3                  27.0                    15.0              NaN   \n",
       "4                   1.0                     7.0              NaN   \n",
       "\n",
       "   num_forum_votes  avg_forum_sentiment  \n",
       "0              NaN                  NaN  \n",
       "1              NaN                  NaN  \n",
       "2              NaN                  NaN  \n",
       "3              NaN                  NaN  \n",
       "4              NaN                  NaN  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.merge(event_features, forum_features, how='left', on=['user_id', 'course_week'])\n",
    "# features = features.drop(['user_id', 'course_week'], axis=1)\n",
    "features.columns = [\n",
    "    'course_week', \n",
    "    'user_id', \n",
    "    'num_video_plays', \n",
    "    'num_problems_incorrect', \n",
    "    'num_problems_correct',\n",
    "    'num_subsections_viewed',\n",
    "    'num_forum_posts',\n",
    "    'num_forum_votes',\n",
    "    'avg_forum_sentiment'\n",
    "]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Join features with course starts and completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_data_sources(features_df, course_starts_df, course_completions_df):\n",
    "    \"\"\"\"\"\"\n",
    "    data = pd.merge(features_df, course_starts_df, how='inner', on='user_id')\n",
    "    data.columns = list(data.columns)[0:-1] + ['user_started_date_key']\n",
    "    data = pd.merge(data, course_completions_df, how='left', on='user_id')\n",
    "    data.columns = list(data.columns)[0:-1] + ['user_completed_date_key']\n",
    "    \n",
    "    def convert_col_to_datetime(column_name):\n",
    "        data[column_name] = data[column_name].astype(str)\n",
    "        data.loc[data[column_name] == 'nan', column_name] = '20170620' # fix this to be dynamic date string\n",
    "        data[column_name] = data[column_name].apply(lambda x: datetime.strptime(x[0:8], '%Y%m%d'))\n",
    "        return data\n",
    "    \n",
    "    data = convert_col_to_datetime('user_started_date_key')\n",
    "    data = convert_col_to_datetime('user_completed_date_key')\n",
    "    \n",
    "    \n",
    "    data['user_started_week'] = data['user_started_date_key'].apply(course_week)\n",
    "    data['user_completed_week'] = data['user_completed_date_key'].apply(course_week)\n",
    "        \n",
    "    last_active_week = data.sort_values('course_week').groupby('user_id').last()[['course_week']]\n",
    "    last_active_week.columns = ['user_last_active_week']\n",
    "    last_active_week = last_active_week.reset_index()\n",
    "    data = pd.merge(data, last_active_week, how='left', on='user_id')\n",
    "    \n",
    "    data['num_problems_attempted'] = data['num_problems_incorrect'] + data['num_problems_correct']\n",
    "    data = data[[\n",
    "        'user_id',\n",
    "        'course_week',\n",
    "        'num_video_plays',\n",
    "        'num_problems_attempted',\n",
    "        'num_problems_correct',\n",
    "        'num_subsections_viewed',\n",
    "        'num_forum_posts',\n",
    "        'num_forum_votes',\n",
    "        'avg_forum_sentiment',\n",
    "        'user_started_week',\n",
    "        'user_last_active_week',\n",
    "        'user_completed_week'\n",
    "    ]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = join_data_sources(features, course_starts, course_completions)\n",
    "data.head(20)\n",
    "\n",
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('all_features_revised_11-18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calculate user_dropped_out_next_week value for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restore features file to avoid pipeline\n",
    "data = pd.read_csv('all_features_revised_11-18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_extra_data_points_for_non_active_weeks(data_df):\n",
    "    data = data_df.copy()\n",
    "    for user_id in pd.unique(data['user_id']):\n",
    "        for week in sorted(pd.unique(data['course_week'])):\n",
    "            df = data.loc[(data['user_id'] == user_id) & (data['course_week'] == week), :]\n",
    "            user_info = data.loc[(data['user_id'] == user_id), :]\n",
    "            started_week = list(user_info['user_started_week'])[0]\n",
    "            last_active_week = list(user_info['user_last_active_week'])[0]\n",
    "            if len(df) == 0 and (week <= last_active_week):\n",
    "                new_zero_row = pd.DataFrame([{\n",
    "                    'user_id': user_id, \n",
    "                    'course_week': week,\n",
    "                    'user_started_week': started_week,\n",
    "                    'user_last_active_week': last_active_week,\n",
    "                    'user_completed_week': list(user_info['user_completed_week'])[0],\n",
    "                }], columns=data.columns).fillna(0.0)\n",
    "                data = data.append(new_zero_row)\n",
    "                data = data.reset_index(drop=True)\n",
    "\n",
    "    data = data.sort_values(['user_id', 'course_week']).reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "data = add_extra_data_points_for_non_active_weeks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_drop_out_next_week(data_df):\n",
    "    data_ = data_df.copy()\n",
    "    data_['user_dropped_out_next_week'] = np.where(data_['user_last_active_week'] == data_['course_week'], 1, 0)\n",
    "    data_.loc[data_['user_completed_week'] > -1, 'user_dropped_out_next_week'] = 0\n",
    "    return data_\n",
    "\n",
    "data = calculate_drop_out_next_week(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'user_id', 'course_week', 'num_video_plays',\n",
       "       'num_problems_attempted', 'num_problems_correct',\n",
       "       'num_subsections_viewed', 'num_forum_posts', 'num_forum_votes',\n",
       "       'avg_forum_sentiment', 'user_started_week', 'user_last_active_week',\n",
       "       'user_completed_week', 'user_dropped_out_next_week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147780, 14)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('all_features_added_negative_rows.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model to predict user_dropped_out_next_week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continue with all data as of 11/18 (calculated by the first 5 steps of this notebook)\n",
    "data = pd.read_csv('../edx_learner_attrition/all_data_model_step.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "    data.drop('user_dropped_out_next_week', axis=1), \n",
    "    data[['user_id', 'course_week', 'user_dropped_out_next_week']], \n",
    "    test_size=0.4, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cols = [\n",
    "    'course_week', 'num_video_plays', 'num_problems_attempted',\n",
    "    'num_problems_correct', 'num_subsections_viewed', 'num_forum_posts',\n",
    "    'num_forum_votes', 'avg_forum_sentiment', 'user_started_week',\n",
    "]\n",
    "\n",
    "X_train = np.array(X_train_df[X_cols])\n",
    "X_test = np.array(X_test_df[X_cols])\n",
    "\n",
    "y_train = np.array(y_train_df['user_dropped_out_next_week'])\n",
    "y_test = np.array(y_test_df['user_dropped_out_next_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88668, 9)\n",
      "(88668,)\n",
      "(59112, 9)\n",
      "(59112,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    88668.000000\n",
       "mean         3.794819\n",
       "std         19.847913\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max        866.000000\n",
       "Name: num_video_plays, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.num_video_plays.describe()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    88668.000000\n",
       "mean         0.418548\n",
       "std          1.007877\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          6.765039\n",
       "Name: num_video_plays, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.log(1 + X_train_df.num_video_plays)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>course_week</th>\n",
       "      <th>num_video_plays</th>\n",
       "      <th>num_problems_attempted</th>\n",
       "      <th>num_problems_correct</th>\n",
       "      <th>num_subsections_viewed</th>\n",
       "      <th>num_forum_posts</th>\n",
       "      <th>num_forum_votes</th>\n",
       "      <th>avg_forum_sentiment</th>\n",
       "      <th>user_started_week</th>\n",
       "      <th>user_last_active_week</th>\n",
       "      <th>user_completed_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107492</th>\n",
       "      <td>16.553661</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.639057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>14.329334</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29842</th>\n",
       "      <td>15.955475</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>13.955509</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75841</th>\n",
       "      <td>16.504076</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93599</th>\n",
       "      <td>16.539533</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>4.644391</td>\n",
       "      <td>4.127134</td>\n",
       "      <td>3.951244</td>\n",
       "      <td>3.663562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29325</th>\n",
       "      <td>15.938635</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80599</th>\n",
       "      <td>16.519191</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106200</th>\n",
       "      <td>16.552419</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39525</th>\n",
       "      <td>16.158903</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86174</th>\n",
       "      <td>16.531183</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106658</th>\n",
       "      <td>16.552898</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13309</th>\n",
       "      <td>15.259831</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56446</th>\n",
       "      <td>16.366453</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81357</th>\n",
       "      <td>16.521324</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74687</th>\n",
       "      <td>16.499364</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>2.564949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59611</th>\n",
       "      <td>16.392798</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146182</th>\n",
       "      <td>16.582582</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83077</th>\n",
       "      <td>16.525993</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39723</th>\n",
       "      <td>16.162235</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136179</th>\n",
       "      <td>16.575072</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108311</th>\n",
       "      <td>16.554450</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59407</th>\n",
       "      <td>16.390764</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23271</th>\n",
       "      <td>15.785748</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>12.930765</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129937</th>\n",
       "      <td>16.570621</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41136</th>\n",
       "      <td>16.183696</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81872</th>\n",
       "      <td>16.522598</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27183</th>\n",
       "      <td>15.887751</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130589</th>\n",
       "      <td>16.571121</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84654</th>\n",
       "      <td>16.529112</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129981</th>\n",
       "      <td>16.570670</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65725</th>\n",
       "      <td>16.449059</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123855</th>\n",
       "      <td>16.566349</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>13.545425</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130523</th>\n",
       "      <td>16.571065</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122537</th>\n",
       "      <td>16.565609</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84478</th>\n",
       "      <td>16.528946</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130608</th>\n",
       "      <td>16.571144</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.397895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>16.530042</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103355</th>\n",
       "      <td>16.549478</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>14.339404</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64925</th>\n",
       "      <td>16.442519</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>4.219508</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.663562</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.639057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59735</th>\n",
       "      <td>16.394076</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>11.963912</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64820</th>\n",
       "      <td>16.441632</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67221</th>\n",
       "      <td>16.459924</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41090</th>\n",
       "      <td>16.183114</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16023</th>\n",
       "      <td>15.468497</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126324</th>\n",
       "      <td>16.567994</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112727</th>\n",
       "      <td>16.558843</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87498</th>\n",
       "      <td>16.532587</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137337</th>\n",
       "      <td>16.575959</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54886</th>\n",
       "      <td>16.350792</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>16.556507</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>16.563781</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103694</th>\n",
       "      <td>16.549876</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>16.572161</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>16.583230</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>16.565225</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88668 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  course_week  num_video_plays  num_problems_attempted  \\\n",
       "107492  16.553661     2.484907         0.000000                0.000000   \n",
       "5208    14.329334     1.791759         0.693147                0.000000   \n",
       "29842   15.955475     1.098612         0.000000                0.000000   \n",
       "3931    13.955509     0.693147         0.000000                0.000000   \n",
       "75841   16.504076     1.386294         0.000000                0.000000   \n",
       "93599   16.539533     1.609438         4.644391                4.127134   \n",
       "29325   15.938635     2.564949         2.833213                4.007333   \n",
       "80599   16.519191     2.772589         0.000000                0.000000   \n",
       "106200  16.552419     1.386294         0.000000                0.000000   \n",
       "39525   16.158903     1.098612         0.000000                0.000000   \n",
       "86174   16.531183     1.098612         0.693147                0.000000   \n",
       "106658  16.552898     2.397895         0.000000                0.000000   \n",
       "13309   15.259831     2.564949         0.000000                0.000000   \n",
       "56446   16.366453     2.079442         0.000000                0.000000   \n",
       "81357   16.521324     0.693147         3.496508                2.484907   \n",
       "74687   16.499364     2.484907         2.639057                2.772589   \n",
       "59611   16.392798     1.609438         0.000000                0.000000   \n",
       "146182  16.582582     2.397895         0.000000                0.693147   \n",
       "83077   16.525993     2.397895         0.000000                0.000000   \n",
       "39723   16.162235     2.197225         0.000000                0.000000   \n",
       "136179  16.575072     2.197225         0.000000                0.000000   \n",
       "108311  16.554450     1.609438         0.000000                0.000000   \n",
       "59407   16.390764     1.386294         0.000000                0.000000   \n",
       "23271   15.785748     0.693147         0.000000                0.000000   \n",
       "1751    12.930765     1.791759         0.000000                0.000000   \n",
       "129937  16.570621     1.791759         0.000000                0.000000   \n",
       "41136   16.183696     2.397895         0.000000                0.000000   \n",
       "81872   16.522598     2.564949         1.098612                2.079442   \n",
       "27183   15.887751     0.693147         0.000000                0.000000   \n",
       "130589  16.571121     1.386294         0.000000                0.000000   \n",
       "...           ...          ...              ...                     ...   \n",
       "84654   16.529112     2.484907         0.000000                0.000000   \n",
       "129981  16.570670     0.693147         0.000000                0.000000   \n",
       "65725   16.449059     2.079442         0.000000                0.000000   \n",
       "123855  16.566349     1.791759         0.000000                0.000000   \n",
       "2747    13.545425     1.386294         0.000000                0.000000   \n",
       "130523  16.571065     2.197225         0.000000                0.000000   \n",
       "122537  16.565609     1.098612         0.000000                0.000000   \n",
       "84478   16.528946     1.791759         1.386294                1.386294   \n",
       "130608  16.571144     1.945910         0.000000                0.000000   \n",
       "85305   16.530042     1.386294         0.000000                0.000000   \n",
       "103355  16.549478     0.693147         0.000000                0.000000   \n",
       "5311    14.339404     1.791759         0.000000                0.000000   \n",
       "64925   16.442519     2.484907         4.219508                3.688879   \n",
       "59735   16.394076     2.302585         0.000000                2.639057   \n",
       "769     11.963912     2.302585         0.000000                0.000000   \n",
       "64820   16.441632     2.397895         0.000000                0.000000   \n",
       "67221   16.459924     1.609438         0.000000                0.000000   \n",
       "41090   16.183114     1.791759         0.000000                0.000000   \n",
       "16023   15.468497     1.386294         0.000000                0.000000   \n",
       "126324  16.567994     1.609438         0.000000                0.000000   \n",
       "112727  16.558843     2.302585         0.000000                0.000000   \n",
       "87498   16.532587     0.693147         2.197225                1.098612   \n",
       "137337  16.575959     1.791759         0.000000                0.000000   \n",
       "54886   16.350792     1.791759         0.000000                0.000000   \n",
       "110268  16.556507     1.609438         0.000000                0.000000   \n",
       "119879  16.563781     1.386294         0.000000                0.000000   \n",
       "103694  16.549876     1.609438         0.693147                0.000000   \n",
       "131932  16.572161     0.693147         0.000000                0.000000   \n",
       "146867  16.583230     1.386294         0.000000                0.000000   \n",
       "121958  16.565225     1.098612         0.000000                0.000000   \n",
       "\n",
       "        num_problems_correct  num_subsections_viewed  num_forum_posts  \\\n",
       "107492              0.000000                0.000000              0.0   \n",
       "5208                0.000000                2.833213              0.0   \n",
       "29842               0.000000                0.000000              0.0   \n",
       "3931                0.000000                0.000000              0.0   \n",
       "75841               0.000000                0.000000              0.0   \n",
       "93599               3.951244                3.663562              0.0   \n",
       "29325               2.995732                3.218876              0.0   \n",
       "80599               0.000000                0.000000              0.0   \n",
       "106200              0.000000                0.000000              0.0   \n",
       "39525               0.000000                0.693147              0.0   \n",
       "86174               0.000000                0.000000              0.0   \n",
       "106658              0.000000                0.000000              0.0   \n",
       "13309               0.000000                0.000000              0.0   \n",
       "56446               0.000000                0.000000              0.0   \n",
       "81357               2.397895                2.302585              0.0   \n",
       "74687               2.197225                2.639057              0.0   \n",
       "59611               0.000000                0.000000              0.0   \n",
       "146182              0.693147                1.791759              0.0   \n",
       "83077               0.000000                0.000000              0.0   \n",
       "39723               0.000000                0.000000              0.0   \n",
       "136179              0.000000                0.000000              0.0   \n",
       "108311              0.000000                0.000000              0.0   \n",
       "59407               0.000000                0.000000              0.0   \n",
       "23271               0.000000                0.000000              0.0   \n",
       "1751                0.000000                0.000000              0.0   \n",
       "129937              0.000000                0.000000              0.0   \n",
       "41136               0.000000                0.000000              0.0   \n",
       "81872               1.609438                1.609438              0.0   \n",
       "27183               0.000000                0.000000              0.0   \n",
       "130589              0.000000                0.000000              0.0   \n",
       "...                      ...                     ...              ...   \n",
       "84654               0.000000                0.000000              0.0   \n",
       "129981              0.000000                0.000000              0.0   \n",
       "65725               0.000000                0.000000              0.0   \n",
       "123855              0.000000                0.000000              0.0   \n",
       "2747                0.000000                0.000000              0.0   \n",
       "130523              0.000000                0.000000              0.0   \n",
       "122537              0.000000                0.000000              0.0   \n",
       "84478               1.386294                1.386294              0.0   \n",
       "130608              0.000000                0.000000              0.0   \n",
       "85305               0.000000                0.000000              0.0   \n",
       "103355              0.000000                0.000000              0.0   \n",
       "5311                0.000000                0.000000              0.0   \n",
       "64925               3.663562                2.833213              0.0   \n",
       "59735               1.791759                1.098612              0.0   \n",
       "769                 0.000000                0.000000              0.0   \n",
       "64820               0.000000                0.000000              0.0   \n",
       "67221               0.000000                0.000000              0.0   \n",
       "41090               0.000000                0.000000              0.0   \n",
       "16023               0.000000                0.000000              0.0   \n",
       "126324              0.000000                0.000000              0.0   \n",
       "112727              0.000000                0.000000              0.0   \n",
       "87498               0.693147                1.945910              0.0   \n",
       "137337              0.000000                0.000000              0.0   \n",
       "54886               0.000000                0.000000              0.0   \n",
       "110268              0.000000                0.000000              0.0   \n",
       "119879              0.000000                0.000000              0.0   \n",
       "103694              0.000000                1.791759              0.0   \n",
       "131932              0.000000                0.000000              0.0   \n",
       "146867              0.000000                0.000000              0.0   \n",
       "121958              0.000000                0.000000              0.0   \n",
       "\n",
       "        num_forum_votes  avg_forum_sentiment  user_started_week  \\\n",
       "107492              0.0                  0.0           1.791759   \n",
       "5208                0.0                  0.0           1.791759   \n",
       "29842               0.0                  0.0           2.197225   \n",
       "3931                0.0                  0.0           2.197225   \n",
       "75841               0.0                  0.0           1.791759   \n",
       "93599               0.0                  0.0           1.609438   \n",
       "29325               0.0                  0.0           1.791759   \n",
       "80599               0.0                  0.0           2.302585   \n",
       "106200              0.0                  0.0           1.791759   \n",
       "39525               0.0                  0.0           0.693147   \n",
       "86174               0.0                  0.0           0.693147   \n",
       "106658              0.0                  0.0           1.945910   \n",
       "13309               0.0                  0.0           2.639057   \n",
       "56446               0.0                  0.0           2.302585   \n",
       "81357               0.0                  0.0           0.693147   \n",
       "74687               0.0                  0.0           0.693147   \n",
       "59611               0.0                  0.0           2.197225   \n",
       "146182              0.0                  0.0           2.397895   \n",
       "83077               0.0                  0.0           1.609438   \n",
       "39723               0.0                  0.0           2.302585   \n",
       "136179              0.0                  0.0           2.302585   \n",
       "108311              0.0                  0.0           2.079442   \n",
       "59407               0.0                  0.0           1.945910   \n",
       "23271               0.0                  0.0           1.945910   \n",
       "1751                0.0                  0.0           0.693147   \n",
       "129937              0.0                  0.0           2.197225   \n",
       "41136               0.0                  0.0           0.693147   \n",
       "81872               0.0                  0.0           2.397895   \n",
       "27183               0.0                  0.0           2.302585   \n",
       "130589              0.0                  0.0           2.197225   \n",
       "...                 ...                  ...                ...   \n",
       "84654               0.0                  0.0           1.609438   \n",
       "129981              0.0                  0.0           2.197225   \n",
       "65725               0.0                  0.0           2.639057   \n",
       "123855              0.0                  0.0           2.302585   \n",
       "2747                0.0                  0.0           1.791759   \n",
       "130523              0.0                  0.0           2.639057   \n",
       "122537              0.0                  0.0           2.079442   \n",
       "84478               0.0                  0.0           0.693147   \n",
       "130608              0.0                  0.0           2.302585   \n",
       "85305               0.0                  0.0           2.484907   \n",
       "103355              0.0                  0.0           2.302585   \n",
       "5311                0.0                  0.0           0.693147   \n",
       "64925               0.0                  0.0           2.197225   \n",
       "59735               0.0                  0.0           2.302585   \n",
       "769                 0.0                  0.0           1.386294   \n",
       "64820               0.0                  0.0           2.639057   \n",
       "67221               0.0                  0.0           1.791759   \n",
       "41090               0.0                  0.0           2.397895   \n",
       "16023               0.0                  0.0           2.197225   \n",
       "126324              0.0                  0.0           2.197225   \n",
       "112727              0.0                  0.0           2.564949   \n",
       "87498               0.0                  0.0           0.693147   \n",
       "137337              0.0                  0.0           2.397895   \n",
       "54886               0.0                  0.0           2.484907   \n",
       "110268              0.0                  0.0           1.791759   \n",
       "119879              0.0                  0.0           2.564949   \n",
       "103694              0.0                  0.0           1.609438   \n",
       "131932              0.0                  0.0           2.197225   \n",
       "146867              0.0                  0.0           2.564949   \n",
       "121958              0.0                  0.0           2.197225   \n",
       "\n",
       "        user_last_active_week  user_completed_week  \n",
       "107492               2.639057             2.639057  \n",
       "5208                 1.791759                 -inf  \n",
       "29842                2.564949                 -inf  \n",
       "3931                 2.639057                 -inf  \n",
       "75841                2.397895                 -inf  \n",
       "93599                1.609438                 -inf  \n",
       "29325                2.564949                 -inf  \n",
       "80599                2.890372                 -inf  \n",
       "106200               1.791759                 -inf  \n",
       "39525                2.833213             0.693147  \n",
       "86174                2.564949                 -inf  \n",
       "106658               2.833213                 -inf  \n",
       "13309                2.639057                 -inf  \n",
       "56446                2.484907                 -inf  \n",
       "81357                2.564949                 -inf  \n",
       "74687                2.564949             2.564949  \n",
       "59611                2.197225                 -inf  \n",
       "146182               2.564949                 -inf  \n",
       "83077                2.484907                 -inf  \n",
       "39723                2.302585                 -inf  \n",
       "136179               2.397895                 -inf  \n",
       "108311               2.197225                 -inf  \n",
       "59407                1.945910                 -inf  \n",
       "23271                1.945910                 -inf  \n",
       "1751                 2.708050                 -inf  \n",
       "129937               2.197225                 -inf  \n",
       "41136                2.639057             1.945910  \n",
       "81872                2.564949                 -inf  \n",
       "27183                2.302585                 -inf  \n",
       "130589               2.197225                 -inf  \n",
       "...                       ...                  ...  \n",
       "84654                2.639057                 -inf  \n",
       "129981               2.890372             2.197225  \n",
       "65725                2.890372                 -inf  \n",
       "123855               2.397895                 -inf  \n",
       "2747                 2.197225                 -inf  \n",
       "130523               2.639057                 -inf  \n",
       "122537               2.079442                 -inf  \n",
       "84478                2.079442                 -inf  \n",
       "130608               2.397895             2.397895  \n",
       "85305                2.484907             2.484907  \n",
       "103355               2.564949                 -inf  \n",
       "5311                 2.484907                 -inf  \n",
       "64925                2.639057             2.639057  \n",
       "59735                2.302585                 -inf  \n",
       "769                  3.044522                 -inf  \n",
       "64820                2.639057                 -inf  \n",
       "67221                1.791759                 -inf  \n",
       "41090                2.639057                 -inf  \n",
       "16023                2.197225                 -inf  \n",
       "126324               2.639057                 -inf  \n",
       "112727               2.564949                 -inf  \n",
       "87498                1.609438                 -inf  \n",
       "137337               2.397895                 -inf  \n",
       "54886                2.833213                 -inf  \n",
       "110268               1.791759                 -inf  \n",
       "119879               2.564949                 -inf  \n",
       "103694               1.609438                 -inf  \n",
       "131932               2.197225                 -inf  \n",
       "146867               2.564949                 -inf  \n",
       "121958               2.197225                 -inf  \n",
       "\n",
       "[88668 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1 + X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNTK implementation of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the data dimensions\n",
    "input_dim = 9\n",
    "num_output_classes = 2\n",
    "num_hidden_layers = 1\n",
    "hidden_layers_dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = C.input_variable(input_dim)\n",
    "label = C.input_variable(num_output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init=C.layers.glorot_uniform(), activation=C.sigmoid):\n",
    "        h = features\n",
    "        for _ in range(num_hidden_layers):\n",
    "            h = C.layers.Dense(hidden_layers_dim)(h)\n",
    "        last_layer = C.layers.Dense(num_output_classes, activation = None)\n",
    "        \n",
    "        return last_layer(h)\n",
    "        \n",
    "z = create_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = C.cross_entropy_with_softmax(z, label)\n",
    "eval_error = C.classification_error(z, label)\n",
    "# Instantiate the trainer object to drive the model training\n",
    "learning_rate = 0.5\n",
    "lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch) \n",
    "learner = C.sgd(z.parameters, lr_schedule)\n",
    "trainer = C.Trainer(z, (loss, eval_error), [learner])\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):    \n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb % frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        print(\"Minibatch: {}, Train Loss: {}, Train Error: {}\".format(mb, training_loss, eval_error))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters for the trainer\n",
    "minibatch_size = 25\n",
    "num_samples = len(X_train)\n",
    "num_minibatches_to_train = num_samples / minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the trainer and perform model training\n",
    "training_progress_output_freq = 20\n",
    "\n",
    "plotdata = {\"batchsize\":[], \"loss\":[], \"error\":[]}\n",
    "\n",
    "for i in range(0, int(num_minibatches_to_train)):\n",
    "    idx = i * minibatch_size\n",
    "    features = X_train[idx:idx+minibatch_size]\n",
    "    labels = [[1, 0] if y == 0 else [0, 1] for y in y_train[idx:idx+minibatch_size]]\n",
    "    \n",
    "    # Specify the input variables mapping in the model to actual minibatch data for training\n",
    "    trainer.train_minibatch({input : features, label : labels})\n",
    "    batchsize, loss, error = print_training_progress(\n",
    "        trainer, i, training_progress_output_freq, verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = X_test\n",
    "labels = [[1, 0] if y == 0 else [0, 1] for y in y_test]\n",
    "trainer.test_minibatch({input: features, label: labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = C.softmax(z)\n",
    "predicted_label_probs = out.eval({input : features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_label_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_threshold = Counter(y_test)[1] / (Counter(y_test)[1] + Counter(y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "# Calculate preds from softmax with significant bias\n",
    "# to any prediction with greater than {softmax_threshold} \n",
    "# probability (between .1 and .2)\n",
    "# of positive (user_dropped_out_next_week) prediction.\n",
    "# This helps to offset the significant data imbalance\n",
    "# of negative entries.\n",
    "#\n",
    "# e.g.\n",
    "#    softmax_threshold = .13\n",
    "#    [94, 6] => 0\n",
    "#    [85, 15] => 1\n",
    "\n",
    "for row in predicted_label_probs:\n",
    "    if row[1] > softmax_threshold:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_maxed = [np.argmax(label) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy score for the model\n",
    "np.mean(np.array(labels_maxed == np.array(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(labels_maxed, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.cohen_kappa_score(features, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_df['predicted_user_dropped_out_next_week'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_data = pd.merge(y_test_df, data, on=['user_id', 'course_week']).sort_values(['user_id', 'course_week'])[[\n",
    "    'user_id', 'course_week', 'user_dropped_out_next_week_x', 'predicted_user_dropped_out_next_week'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_data.pivot_table(\n",
    "    index='user_id', columns=['course_week'], values='predicted_user_dropped_out_next_week', fill_value=-1\n",
    ").to_csv('user_journey_predicted_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.pivot_table(\n",
    "    index='user_id', columns=['course_week'], values='user_dropped_out_next_week', fill_value=-1\n",
    ").to_csv('user_journey_actual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('all_data_model_step.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. More experiments to deliver a better algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "cart = CNTKClassifier(input_dim=9, hidden_layers_dim=8, num_hidden_layers=1, num_output_classes=2, learning_rate=0.5)\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>course_week</th>\n",
       "      <th>num_video_plays</th>\n",
       "      <th>num_problems_attempted</th>\n",
       "      <th>num_problems_correct</th>\n",
       "      <th>num_subsections_viewed</th>\n",
       "      <th>num_forum_posts</th>\n",
       "      <th>num_forum_votes</th>\n",
       "      <th>avg_forum_sentiment</th>\n",
       "      <th>user_started_week</th>\n",
       "      <th>user_last_active_week</th>\n",
       "      <th>user_completed_week</th>\n",
       "      <th>user_dropped_out_next_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67071</th>\n",
       "      <td>67071</td>\n",
       "      <td>14059633</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106843</th>\n",
       "      <td>106843</td>\n",
       "      <td>15449413</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321</th>\n",
       "      <td>10321</td>\n",
       "      <td>3182606</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116597</th>\n",
       "      <td>116597</td>\n",
       "      <td>15583373</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55267</th>\n",
       "      <td>55267</td>\n",
       "      <td>12673869</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   user_id  course_week  num_video_plays  \\\n",
       "67071        67071  14059633           13              0.0   \n",
       "106843      106843  15449413            5              0.0   \n",
       "10321        10321   3182606            9              0.0   \n",
       "116597      116597  15583373            2              0.0   \n",
       "55267        55267  12673869            1              3.0   \n",
       "\n",
       "        num_problems_attempted  num_problems_correct  num_subsections_viewed  \\\n",
       "67071                      0.0                   0.0                     0.0   \n",
       "106843                     1.0                   1.0                     5.0   \n",
       "10321                      2.0                   0.0                    12.0   \n",
       "116597                     0.0                   0.0                     0.0   \n",
       "55267                      0.0                   0.0                     1.0   \n",
       "\n",
       "        num_forum_posts  num_forum_votes  avg_forum_sentiment  \\\n",
       "67071               0.0              0.0                  0.0   \n",
       "106843              0.0              0.0                  0.0   \n",
       "10321               0.0              0.0                  0.0   \n",
       "116597              0.0              0.0                  0.0   \n",
       "55267               0.0              0.0                  0.0   \n",
       "\n",
       "        user_started_week  user_last_active_week  user_completed_week  \\\n",
       "67071                   9                     16                   -1   \n",
       "106843                  5                      7                   -1   \n",
       "10321                   1                     12                   10   \n",
       "116597                  6                      7                   -1   \n",
       "55267                   1                      7                   -1   \n",
       "\n",
       "        user_dropped_out_next_week  \n",
       "67071                            0  \n",
       "106843                           0  \n",
       "10321                            0  \n",
       "116597                           0  \n",
       "55267                            0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_sample_sizes = [.5, .6, .7, .8]\n",
    "resample(data, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>course_week</th>\n",
       "      <th>num_video_plays</th>\n",
       "      <th>num_problems_attempted</th>\n",
       "      <th>num_problems_correct</th>\n",
       "      <th>num_subsections_viewed</th>\n",
       "      <th>num_forum_posts</th>\n",
       "      <th>num_forum_votes</th>\n",
       "      <th>avg_forum_sentiment</th>\n",
       "      <th>user_started_week</th>\n",
       "      <th>user_last_active_week</th>\n",
       "      <th>user_completed_week</th>\n",
       "      <th>user_dropped_out_next_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>516</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>516</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>516</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>516</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>516</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  user_id  course_week  num_video_plays  num_problems_attempted  \\\n",
       "0           0      516            1              0.0                     0.0   \n",
       "1           1      516            2              0.0                     0.0   \n",
       "2           2      516            3              0.0                     0.0   \n",
       "3           3      516            4              0.0                     0.0   \n",
       "4           4      516            5              0.0                     0.0   \n",
       "\n",
       "   num_problems_correct  num_subsections_viewed  num_forum_posts  \\\n",
       "0                   0.0                     0.0              0.0   \n",
       "1                   0.0                     0.0              0.0   \n",
       "2                   0.0                     0.0              0.0   \n",
       "3                   0.0                     0.0              0.0   \n",
       "4                   0.0                     0.0              0.0   \n",
       "\n",
       "   num_forum_votes  avg_forum_sentiment  user_started_week  \\\n",
       "0              0.0                  0.0                 10   \n",
       "1              0.0                  0.0                 10   \n",
       "2              0.0                  0.0                 10   \n",
       "3              0.0                  0.0                 10   \n",
       "4              0.0                  0.0                 10   \n",
       "\n",
       "   user_last_active_week  user_completed_week  user_dropped_out_next_week  \n",
       "0                     10                   -1                           0  \n",
       "1                     10                   -1                           0  \n",
       "2                     10                   -1                           0  \n",
       "3                     10                   -1                           0  \n",
       "4                     10                   -1                           0  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object bootstrap_resample at 0x7f6636d1bbf8>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pima = read_csv('pima-indians-diabetes.data.csv', header=None)\n",
    "values = data_pima.values\n",
    "# configure bootstrap\n",
    "n_iterations = 100\n",
    "\n",
    "bootstrap_sample_sizes = [.5, .6, .7]\n",
    "\n",
    "def bootstrap_resample(data, n_iterations=1, boostrap_sample_sizes=[.6, .7, .8]):\n",
    "#     for size in bootstrap_sample_sizes:\n",
    "    n_size = .8 # int(len(data) * size)\n",
    "    X = data\n",
    "    y = data[['user_id', 'course_week', 'user_dropped_out_next_week']]\n",
    "    for i in range(n_iterations):\n",
    "        # prepare train and test sets\n",
    "        num_samples = int(len(data) * n_size)\n",
    "        train = resample(np.array(data), n_samples=num_samples)\n",
    "        test = np.array([x for x in values if x.tolist() not in train.tolist()])\n",
    "        yield (train, test)\n",
    "        \n",
    "            \n",
    "from collections import Iterable\n",
    "bootstrap_resample(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bootstrap_resample(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.utils import indexable\n",
    "from sklearn.utils.validation import _num_samples\n",
    "\n",
    "class BootstrapCrossValidator(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Bootstrap sampling Cross Validator\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iterations=10, bootstrap_sample_sizes=[.6, .7, .8]):\n",
    "        self.n_iterations = n_iterations\n",
    "        self.bootstrap_sample_sizes = bootstrap_sample_sizes\n",
    "        \n",
    "    def split(self, X, y=None, groups=None):        \n",
    "        for train, test in super(BootstrapCrossValidator, self).split(X, y):\n",
    "            yield train, test\n",
    "            \n",
    "    def _iter_test_indices(self, X, y=None, groups=None):\n",
    "        n_samples = _num_samples(X)\n",
    "        for size in self.bootstrap_sample_sizes:\n",
    "            n_size = int(len(X) * size)\n",
    "            for i in range(self.n_iterations):\n",
    "                # prepare train and test sets            \n",
    "                yield np.random.randint(0, n_samples, n_size)\n",
    "                \n",
    "    def get_n_splits(self, X, y=None, groups=None):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            raise ValueError(\"The 'X' parameter should not be None.\")\n",
    "        return _num_samples(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BootstrapCrossValidator(bootstrap_sample_sizes=[0.6, 0.7, 0.8],\n",
       "            n_iterations=10)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for indices in BootstrapCrossValidator()._iter_test_indices(data):\n",
    "    print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>course_week</th>\n",
       "      <th>num_video_plays</th>\n",
       "      <th>num_problems_attempted</th>\n",
       "      <th>num_problems_correct</th>\n",
       "      <th>num_subsections_viewed</th>\n",
       "      <th>num_forum_posts</th>\n",
       "      <th>num_forum_votes</th>\n",
       "      <th>avg_forum_sentiment</th>\n",
       "      <th>user_started_week</th>\n",
       "      <th>user_last_active_week</th>\n",
       "      <th>user_completed_week</th>\n",
       "      <th>user_dropped_out_next_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131690</th>\n",
       "      <td>131690</td>\n",
       "      <td>15743435</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119562</th>\n",
       "      <td>119562</td>\n",
       "      <td>15613591</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104838</th>\n",
       "      <td>104838</td>\n",
       "      <td>15415897</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61504</th>\n",
       "      <td>61504</td>\n",
       "      <td>13408482</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94894</th>\n",
       "      <td>94894</td>\n",
       "      <td>15259570</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59351</th>\n",
       "      <td>59351</td>\n",
       "      <td>13129931</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53390</th>\n",
       "      <td>53390</td>\n",
       "      <td>12421975</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88989</th>\n",
       "      <td>88989</td>\n",
       "      <td>15165000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60777</th>\n",
       "      <td>60777</td>\n",
       "      <td>13305504</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35714</th>\n",
       "      <td>35714</td>\n",
       "      <td>9649617</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40095</th>\n",
       "      <td>40095</td>\n",
       "      <td>10509596</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80027</th>\n",
       "      <td>80027</td>\n",
       "      <td>14913083</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59421</th>\n",
       "      <td>59421</td>\n",
       "      <td>13136058</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92083</th>\n",
       "      <td>92083</td>\n",
       "      <td>15216473</td>\n",
       "      <td>2</td>\n",
       "      <td>245.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21290</th>\n",
       "      <td>21290</td>\n",
       "      <td>6790374</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32324</th>\n",
       "      <td>32324</td>\n",
       "      <td>9017361</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19677</th>\n",
       "      <td>19677</td>\n",
       "      <td>6476582</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76144</th>\n",
       "      <td>76144</td>\n",
       "      <td>14731453</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106256</th>\n",
       "      <td>106256</td>\n",
       "      <td>15439778</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109520</th>\n",
       "      <td>109520</td>\n",
       "      <td>15488810</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130583</th>\n",
       "      <td>130583</td>\n",
       "      <td>15730443</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81537</th>\n",
       "      <td>81537</td>\n",
       "      <td>14971740</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140020</th>\n",
       "      <td>140020</td>\n",
       "      <td>15841241</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68345</th>\n",
       "      <td>68345</td>\n",
       "      <td>14175815</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6158</th>\n",
       "      <td>6158</td>\n",
       "      <td>1927493</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82222</th>\n",
       "      <td>82222</td>\n",
       "      <td>15000925</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129831</th>\n",
       "      <td>129831</td>\n",
       "      <td>15721650</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38420</th>\n",
       "      <td>38420</td>\n",
       "      <td>10214344</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72829</th>\n",
       "      <td>72829</td>\n",
       "      <td>14519208</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43439</th>\n",
       "      <td>43439</td>\n",
       "      <td>11039070</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102955</th>\n",
       "      <td>102955</td>\n",
       "      <td>15385725</td>\n",
       "      <td>9</td>\n",
       "      <td>265.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11350</th>\n",
       "      <td>11350</td>\n",
       "      <td>3514281</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137293</th>\n",
       "      <td>137293</td>\n",
       "      <td>15806022</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30152</th>\n",
       "      <td>30152</td>\n",
       "      <td>8565234</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18662</th>\n",
       "      <td>18662</td>\n",
       "      <td>6135260</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89958</th>\n",
       "      <td>89958</td>\n",
       "      <td>15180568</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135214</th>\n",
       "      <td>135214</td>\n",
       "      <td>15783610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80686</th>\n",
       "      <td>80686</td>\n",
       "      <td>14938629</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92821</th>\n",
       "      <td>92821</td>\n",
       "      <td>15228393</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91828</th>\n",
       "      <td>91828</td>\n",
       "      <td>15211643</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127420</th>\n",
       "      <td>127420</td>\n",
       "      <td>15692699</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96525</th>\n",
       "      <td>96525</td>\n",
       "      <td>15287030</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72850</th>\n",
       "      <td>72850</td>\n",
       "      <td>14520544</td>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104967</th>\n",
       "      <td>104967</td>\n",
       "      <td>15419205</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40593</th>\n",
       "      <td>40593</td>\n",
       "      <td>10588052</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138923</th>\n",
       "      <td>138923</td>\n",
       "      <td>15827902</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32352</th>\n",
       "      <td>32352</td>\n",
       "      <td>9019123</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72813</th>\n",
       "      <td>72813</td>\n",
       "      <td>14518094</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77924</th>\n",
       "      <td>77924</td>\n",
       "      <td>14826530</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60113</th>\n",
       "      <td>60113</td>\n",
       "      <td>13222170</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125843</th>\n",
       "      <td>125843</td>\n",
       "      <td>15675079</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84073</th>\n",
       "      <td>84073</td>\n",
       "      <td>15074306</td>\n",
       "      <td>9</td>\n",
       "      <td>244.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84776</th>\n",
       "      <td>84776</td>\n",
       "      <td>15084921</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95512</th>\n",
       "      <td>95512</td>\n",
       "      <td>15270692</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127427</th>\n",
       "      <td>127427</td>\n",
       "      <td>15692798</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58888</th>\n",
       "      <td>58888</td>\n",
       "      <td>13088640</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93337</th>\n",
       "      <td>93337</td>\n",
       "      <td>15236395</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131662</th>\n",
       "      <td>131662</td>\n",
       "      <td>15743118</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20100</th>\n",
       "      <td>20100</td>\n",
       "      <td>6556801</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46272</th>\n",
       "      <td>46272</td>\n",
       "      <td>11480445</td>\n",
       "      <td>8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147780 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   user_id  course_week  num_video_plays  \\\n",
       "131690      131690  15743435            7              0.0   \n",
       "119562      119562  15613591            7              0.0   \n",
       "104838      104838  15415897            2              0.0   \n",
       "61504        61504  13408482            7              0.0   \n",
       "94894        94894  15259570            4              0.0   \n",
       "59351        59351  13129931            5              0.0   \n",
       "53390        53390  12421975           11              0.0   \n",
       "88989        88989  15165000            3              2.0   \n",
       "60777        60777  13305504            1              0.0   \n",
       "35714        35714   9649617            7              0.0   \n",
       "40095        40095  10509596            5              0.0   \n",
       "80027        80027  14913083            2              5.0   \n",
       "59421        59421  13136058            5              1.0   \n",
       "92083        92083  15216473            2            245.0   \n",
       "21290        21290   6790374            3              0.0   \n",
       "32324        32324   9017361            3              0.0   \n",
       "19677        19677   6476582            2              0.0   \n",
       "76144        76144  14731453            3              0.0   \n",
       "106256      106256  15439778            4              0.0   \n",
       "109520      109520  15488810            3              0.0   \n",
       "130583      130583  15730443            5              0.0   \n",
       "81537        81537  14971740            3              8.0   \n",
       "140020      140020  15841241            2              0.0   \n",
       "68345        68345  14175815            5              0.0   \n",
       "6158          6158   1927493           12              1.0   \n",
       "82222        82222  15000925            1              0.0   \n",
       "129831      129831  15721650            1              0.0   \n",
       "38420        38420  10214344            1              4.0   \n",
       "72829        72829  14519208            1              0.0   \n",
       "43439        43439  11039070            4              9.0   \n",
       "...            ...       ...          ...              ...   \n",
       "102955      102955  15385725            9            265.0   \n",
       "11350        11350   3514281            5              0.0   \n",
       "137293      137293  15806022            9             11.0   \n",
       "30152        30152   8565234            1              0.0   \n",
       "18662        18662   6135260            5              0.0   \n",
       "89958        89958  15180568            3              0.0   \n",
       "135214      135214  15783610            1              0.0   \n",
       "80686        80686  14938629            5              0.0   \n",
       "92821        92821  15228393            2              0.0   \n",
       "91828        91828  15211643            4              0.0   \n",
       "127420      127420  15692699            6              0.0   \n",
       "96525        96525  15287030            1              0.0   \n",
       "72850        72850  14520544            1             55.0   \n",
       "104967      104967  15419205            3              0.0   \n",
       "40593        40593  10588052            2              0.0   \n",
       "138923      138923  15827902            8              0.0   \n",
       "32352        32352   9019123            6              0.0   \n",
       "72813        72813  14518094            4              0.0   \n",
       "77924        77924  14826530            7              0.0   \n",
       "60113        60113  13222170            4              0.0   \n",
       "125843      125843  15675079            5              0.0   \n",
       "84073        84073  15074306            9            244.0   \n",
       "84776        84776  15084921            5              1.0   \n",
       "95512        95512  15270692            3              0.0   \n",
       "127427      127427  15692798            5              0.0   \n",
       "58888        58888  13088640            6              4.0   \n",
       "93337        93337  15236395            5              4.0   \n",
       "131662      131662  15743118            3              0.0   \n",
       "20100        20100   6556801            1              0.0   \n",
       "46272        46272  11480445            8             63.0   \n",
       "\n",
       "        num_problems_attempted  num_problems_correct  num_subsections_viewed  \\\n",
       "131690                     0.0                   0.0                     0.0   \n",
       "119562                     0.0                   0.0                     0.0   \n",
       "104838                     0.0                   0.0                     0.0   \n",
       "61504                      0.0                   0.0                     0.0   \n",
       "94894                      0.0                   0.0                     0.0   \n",
       "59351                      0.0                   0.0                     5.0   \n",
       "53390                      0.0                   0.0                     0.0   \n",
       "88989                      0.0                   0.0                     2.0   \n",
       "60777                      0.0                   0.0                     0.0   \n",
       "35714                      0.0                   0.0                     0.0   \n",
       "40095                      0.0                   0.0                     0.0   \n",
       "80027                      1.0                   1.0                     6.0   \n",
       "59421                      1.0                   1.0                     5.0   \n",
       "92083                     15.0                   8.0                    27.0   \n",
       "21290                      0.0                   0.0                     0.0   \n",
       "32324                      0.0                   0.0                     0.0   \n",
       "19677                      0.0                   0.0                     0.0   \n",
       "76144                      0.0                   0.0                     0.0   \n",
       "106256                     0.0                   0.0                     0.0   \n",
       "109520                     0.0                   0.0                     0.0   \n",
       "130583                     0.0                   0.0                     0.0   \n",
       "81537                      7.0                   5.0                     6.0   \n",
       "140020                     0.0                   0.0                     0.0   \n",
       "68345                      0.0                   0.0                     0.0   \n",
       "6158                       0.0                   0.0                     0.0   \n",
       "82222                      0.0                   0.0                     0.0   \n",
       "129831                     0.0                   0.0                     0.0   \n",
       "38420                      1.0                   1.0                     6.0   \n",
       "72829                      0.0                   0.0                     0.0   \n",
       "43439                     10.0                   6.0                     8.0   \n",
       "...                        ...                   ...                     ...   \n",
       "102955                    84.0                  73.0                    38.0   \n",
       "11350                      0.0                   0.0                     0.0   \n",
       "137293                    21.0                  17.0                     9.0   \n",
       "30152                      0.0                   0.0                     0.0   \n",
       "18662                      0.0                   0.0                     0.0   \n",
       "89958                      0.0                   0.0                     0.0   \n",
       "135214                     0.0                   0.0                     0.0   \n",
       "80686                      0.0                   0.0                     0.0   \n",
       "92821                      5.0                   5.0                     4.0   \n",
       "91828                      0.0                   0.0                     0.0   \n",
       "127420                     0.0                   0.0                     0.0   \n",
       "96525                      0.0                   0.0                     0.0   \n",
       "72850                     58.0                  56.0                    43.0   \n",
       "104967                     0.0                   0.0                     0.0   \n",
       "40593                      0.0                   0.0                     0.0   \n",
       "138923                     0.0                   0.0                     0.0   \n",
       "32352                      0.0                   0.0                     1.0   \n",
       "72813                      0.0                   0.0                     0.0   \n",
       "77924                      0.0                   0.0                     0.0   \n",
       "60113                      0.0                   0.0                     0.0   \n",
       "125843                     0.0                   0.0                     0.0   \n",
       "84073                    110.0                  83.0                    36.0   \n",
       "84776                      0.0                   0.0                     1.0   \n",
       "95512                      0.0                   0.0                     0.0   \n",
       "127427                     0.0                   0.0                     0.0   \n",
       "58888                      0.0                   0.0                     0.0   \n",
       "93337                      0.0                   0.0                     3.0   \n",
       "131662                     0.0                   0.0                     0.0   \n",
       "20100                      0.0                   0.0                     0.0   \n",
       "46272                     14.0                   4.0                    14.0   \n",
       "\n",
       "        num_forum_posts  num_forum_votes  avg_forum_sentiment  \\\n",
       "131690              0.0              0.0                  0.0   \n",
       "119562              0.0              0.0                  0.0   \n",
       "104838              0.0              0.0                  0.0   \n",
       "61504               0.0              0.0                  0.0   \n",
       "94894               0.0              0.0                  0.0   \n",
       "59351               0.0              0.0                  0.0   \n",
       "53390               0.0              0.0                  0.0   \n",
       "88989               0.0              0.0                  0.0   \n",
       "60777               0.0              0.0                  0.0   \n",
       "35714               0.0              0.0                  0.0   \n",
       "40095               0.0              0.0                  0.0   \n",
       "80027               0.0              0.0                  0.0   \n",
       "59421               0.0              0.0                  0.0   \n",
       "92083               0.0              0.0                  0.0   \n",
       "21290               0.0              0.0                  0.0   \n",
       "32324               0.0              0.0                  0.0   \n",
       "19677               0.0              0.0                  0.0   \n",
       "76144               0.0              0.0                  0.0   \n",
       "106256              0.0              0.0                  0.0   \n",
       "109520              0.0              0.0                  0.0   \n",
       "130583              0.0              0.0                  0.0   \n",
       "81537               0.0              0.0                  0.0   \n",
       "140020              0.0              0.0                  0.0   \n",
       "68345               0.0              0.0                  0.0   \n",
       "6158                0.0              0.0                  0.0   \n",
       "82222               0.0              0.0                  0.0   \n",
       "129831              0.0              0.0                  0.0   \n",
       "38420               0.0              0.0                  0.0   \n",
       "72829               0.0              0.0                  0.0   \n",
       "43439               0.0              0.0                  0.0   \n",
       "...                 ...              ...                  ...   \n",
       "102955              0.0              0.0                  0.0   \n",
       "11350               0.0              0.0                  0.0   \n",
       "137293              0.0              0.0                  0.0   \n",
       "30152               0.0              0.0                  0.0   \n",
       "18662               0.0              0.0                  0.0   \n",
       "89958               0.0              0.0                  0.0   \n",
       "135214              0.0              0.0                  0.0   \n",
       "80686               0.0              0.0                  0.0   \n",
       "92821               0.0              0.0                  0.0   \n",
       "91828               0.0              0.0                  0.0   \n",
       "127420              0.0              0.0                  0.0   \n",
       "96525               0.0              0.0                  0.0   \n",
       "72850               0.0              0.0                  0.0   \n",
       "104967              0.0              0.0                  0.0   \n",
       "40593               0.0              0.0                  0.0   \n",
       "138923              0.0              0.0                  0.0   \n",
       "32352               0.0              0.0                  0.0   \n",
       "72813               0.0              0.0                  0.0   \n",
       "77924               0.0              0.0                  0.0   \n",
       "60113               0.0              0.0                  0.0   \n",
       "125843              0.0              0.0                  0.0   \n",
       "84073               0.0              0.0                  0.0   \n",
       "84776               0.0              0.0                  0.0   \n",
       "95512               0.0              0.0                  0.0   \n",
       "127427              0.0              0.0                  0.0   \n",
       "58888               0.0              0.0                  0.0   \n",
       "93337               0.0              0.0                  0.0   \n",
       "131662              0.0              0.0                  0.0   \n",
       "20100               0.0              0.0                  0.0   \n",
       "46272               0.0              0.0                  0.0   \n",
       "\n",
       "        user_started_week  user_last_active_week  user_completed_week  \\\n",
       "131690                  8                     11                   -1   \n",
       "119562                 10                     13                   12   \n",
       "104838                  4                      4                   -1   \n",
       "61504                   4                      8                    8   \n",
       "94894                   8                      8                   -1   \n",
       "59351                   7                      9                   -1   \n",
       "53390                  12                     17                   -1   \n",
       "88989                   1                      3                   -1   \n",
       "60777                   5                     11                   11   \n",
       "35714                   8                     10                   -1   \n",
       "40095                   7                      7                   -1   \n",
       "80027                   2                      2                   -1   \n",
       "59421                   5                      5                   -1   \n",
       "92083                   2                      5                   -1   \n",
       "21290                   4                     12                   -1   \n",
       "32324                  10                     12                   -1   \n",
       "19677                   8                      8                   -1   \n",
       "76144                   7                     11                   10   \n",
       "106256                  5                      5                   -1   \n",
       "109520                  5                      5                   -1   \n",
       "130583                  8                      8                   -1   \n",
       "81537                   1                      3                   -1   \n",
       "140020                  9                      9                   -1   \n",
       "68345                   6                      6                   -1   \n",
       "6158                   12                     12                   -1   \n",
       "82222                   2                      8                   -1   \n",
       "129831                 12                     12                   -1   \n",
       "38420                   1                      1                   -1   \n",
       "72829                   2                      2                    2   \n",
       "43439                   4                      4                   -1   \n",
       "...                   ...                    ...                  ...   \n",
       "102955                  8                      9                    9   \n",
       "11350                  11                     12                   -1   \n",
       "137293                  9                     13                   -1   \n",
       "30152                   3                      3                   -1   \n",
       "18662                   1                     18                   -1   \n",
       "89958                   6                      6                   -1   \n",
       "135214                  9                      9                   -1   \n",
       "80686                   3                      8                   -1   \n",
       "92821                   2                     13                   -1   \n",
       "91828                   8                     13                   -1   \n",
       "127420                  8                      8                   -1   \n",
       "96525                   8                      8                   -1   \n",
       "72850                   1                      2                    2   \n",
       "104967                  4                      4                   -1   \n",
       "40593                   7                      7                   -1   \n",
       "138923                  9                      9                   -1   \n",
       "32352                   3                      6                   -1   \n",
       "72813                   8                      9                   -1   \n",
       "77924                   8                      8                   -1   \n",
       "60113                   5                      6                   -1   \n",
       "125843                  8                      8                   -1   \n",
       "84073                   8                      9                    9   \n",
       "84776                   5                      5                   -1   \n",
       "95512                   8                     20                   -1   \n",
       "127427                  8                      8                   -1   \n",
       "58888                   1                      6                   -1   \n",
       "93337                   4                      6                   -1   \n",
       "131662                  8                      8                   -1   \n",
       "20100                   8                      8                   -1   \n",
       "46272                   8                      8                   -1   \n",
       "\n",
       "        user_dropped_out_next_week  \n",
       "131690                           0  \n",
       "119562                           0  \n",
       "104838                           0  \n",
       "61504                            0  \n",
       "94894                            0  \n",
       "59351                            0  \n",
       "53390                            0  \n",
       "88989                            1  \n",
       "60777                            0  \n",
       "35714                            0  \n",
       "40095                            0  \n",
       "80027                            1  \n",
       "59421                            1  \n",
       "92083                            0  \n",
       "21290                            0  \n",
       "32324                            0  \n",
       "19677                            0  \n",
       "76144                            0  \n",
       "106256                           0  \n",
       "109520                           0  \n",
       "130583                           0  \n",
       "81537                            1  \n",
       "140020                           0  \n",
       "68345                            0  \n",
       "6158                             1  \n",
       "82222                            0  \n",
       "129831                           0  \n",
       "38420                            1  \n",
       "72829                            0  \n",
       "43439                            1  \n",
       "...                            ...  \n",
       "102955                           0  \n",
       "11350                            0  \n",
       "137293                           0  \n",
       "30152                            0  \n",
       "18662                            0  \n",
       "89958                            0  \n",
       "135214                           0  \n",
       "80686                            0  \n",
       "92821                            0  \n",
       "91828                            0  \n",
       "127420                           0  \n",
       "96525                            0  \n",
       "72850                            0  \n",
       "104967                           0  \n",
       "40593                            0  \n",
       "138923                           0  \n",
       "32352                            1  \n",
       "72813                            0  \n",
       "77924                            0  \n",
       "60113                            0  \n",
       "125843                           0  \n",
       "84073                            0  \n",
       "84776                            1  \n",
       "95512                            0  \n",
       "127427                           0  \n",
       "58888                            1  \n",
       "93337                            0  \n",
       "131662                           0  \n",
       "20100                            0  \n",
       "46272                            1  \n",
       "\n",
       "[147780 rows x 14 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683982683983\n",
      "0.683544303797\n",
      "0.671673819742\n",
      "0.725877192982\n",
      "0.698481561822\n",
      "0.701754385965\n",
      "0.688559322034\n",
      "0.703056768559\n",
      "0.693304535637\n",
      "0.721982758621\n",
      "0.665966386555\n",
      "0.704255319149\n",
      "0.718681318681\n",
      "0.717062634989\n",
      "0.696390658174\n",
      "0.678260869565\n",
      "0.727659574468\n",
      "0.658798283262\n",
      "0.716129032258\n",
      "0.690831556503\n",
      "0.679569892473\n",
      "0.704016913319\n",
      "0.654736842105\n",
      "0.670235546039\n",
      "0.700431034483\n",
      "0.693654266958\n",
      "0.685106382979\n",
      "0.657446808511\n",
      "0.670235546039\n",
      "0.734475374732\n",
      "0.715835140998\n",
      "0.669491525424\n",
      "0.679569892473\n",
      "0.719565217391\n",
      "0.684647302905\n",
      "0.664543524416\n",
      "0.692796610169\n",
      "0.702127659574\n",
      "0.681034482759\n",
      "0.713978494624\n",
      "0.678879310345\n",
      "0.7096069869\n",
      "0.713333333333\n",
      "0.695652173913\n",
      "0.701754385965\n",
      "0.700209643606\n",
      "0.69392033543\n",
      "0.674468085106\n",
      "0.70626349892\n",
      "0.706638115632\n",
      "0.6852248394\n",
      "0.673728813559\n",
      "0.686847599165\n",
      "0.686567164179\n",
      "0.689507494647\n",
      "0.681720430108\n",
      "0.662473794549\n",
      "0.694805194805\n",
      "0.710583153348\n",
      "0.726086956522\n",
      "0.701271186441\n",
      "0.677631578947\n",
      "0.663043478261\n",
      "0.680761099366\n",
      "0.70715835141\n",
      "0.663043478261\n",
      "0.694623655914\n",
      "0.698072805139\n",
      "0.683083511777\n",
      "0.646551724138\n",
      "0.710300429185\n",
      "0.675213675214\n",
      "0.659292035398\n",
      "0.655172413793\n",
      "0.690021231423\n",
      "0.712446351931\n",
      "0.644820295983\n",
      "0.665245202559\n",
      "0.681720430108\n",
      "0.675324675325\n",
      "0.706638115632\n",
      "0.717391304348\n",
      "0.676211453744\n",
      "0.647311827957\n",
      "0.667381974249\n",
      "0.691810344828\n",
      "0.631469979296\n",
      "0.677215189873\n",
      "0.689804772234\n",
      "0.672376873662\n",
      "0.683870967742\n",
      "0.710869565217\n",
      "0.679487179487\n",
      "0.680761099366\n",
      "0.680084745763\n",
      "0.668076109937\n",
      "0.68898488121\n",
      "0.672877846791\n",
      "0.705128205128\n",
      "0.681818181818\n",
      "0.699052132701\n",
      "0.70243902439\n",
      "0.690531177829\n",
      "0.693975903614\n",
      "0.719257540603\n",
      "0.691415313225\n",
      "0.668246445498\n",
      "0.641860465116\n",
      "0.691211401425\n",
      "0.706024096386\n",
      "0.714622641509\n",
      "0.653206650831\n",
      "0.688340807175\n",
      "0.667464114833\n",
      "0.694915254237\n",
      "0.691244239631\n",
      "0.701421800948\n",
      "0.684964200477\n",
      "0.699029126214\n",
      "0.680190930788\n",
      "0.68\n",
      "0.695652173913\n",
      "0.665083135392\n",
      "0.682464454976\n",
      "0.68119266055\n",
      "0.665071770335\n",
      "0.657471264368\n",
      "0.727699530516\n",
      "0.682464454976\n",
      "0.714953271028\n",
      "0.680190930788\n",
      "0.640661938534\n",
      "0.702702702703\n",
      "0.670588235294\n",
      "0.677725118483\n",
      "0.685851318945\n",
      "0.661938534279\n",
      "0.715596330275\n",
      "0.67619047619\n",
      "0.673031026253\n",
      "0.689411764706\n",
      "0.708333333333\n",
      "0.730582524272\n",
      "0.673860911271\n",
      "0.693045563549\n",
      "0.687058823529\n",
      "0.730952380952\n",
      "0.709302325581\n",
      "0.680952380952\n",
      "0.692307692308\n",
      "0.722352941176\n",
      "0.679518072289\n",
      "0.685096153846\n",
      "0.636579572447\n",
      "0.675057208238\n",
      "0.705741626794\n",
      "0.667469879518\n",
      "0.706976744186\n",
      "0.669789227166\n",
      "0.693396226415\n",
      "0.717647058824\n",
      "0.672209026128\n",
      "0.726635514019\n",
      "0.714953271028\n",
      "0.666666666667\n",
      "0.668235294118\n",
      "0.695652173913\n",
      "0.707729468599\n",
      "0.657957244656\n",
      "0.665060240964\n",
      "0.707762557078\n",
      "0.671361502347\n",
      "0.643518518519\n",
      "0.640963855422\n",
      "0.71186440678\n",
      "0.676258992806\n",
      "0.708530805687\n",
      "0.691176470588\n",
      "0.732558139535\n",
      "0.68085106383\n",
      "0.713603818616\n",
      "0.705463182898\n",
      "0.697336561743\n",
      "0.682692307692\n",
      "0.708737864078\n",
      "0.684466019417\n",
      "0.669789227166\n",
      "0.676056338028\n",
      "0.676056338028\n",
      "0.69756097561\n",
      "0.72247706422\n",
      "0.669950738916\n",
      "0.673076923077\n",
      "0.675417661098\n",
      "0.693779904306\n",
      "0.70843373494\n",
      "0.678321678322\n",
      "0.699074074074\n",
      "0.661252900232\n",
      "0.661137440758\n",
      "0.694300518135\n",
      "0.735135135135\n",
      "0.689567430025\n",
      "0.648148148148\n",
      "0.648936170213\n",
      "0.651041666667\n",
      "0.712820512821\n",
      "0.676092544987\n",
      "0.695538057743\n",
      "0.660668380463\n",
      "0.675257731959\n",
      "0.708894878706\n",
      "0.702349869452\n",
      "0.677333333333\n",
      "0.690860215054\n",
      "0.700808625337\n",
      "0.697201017812\n",
      "0.693670886076\n",
      "0.712365591398\n",
      "0.70523415978\n",
      "0.705426356589\n",
      "0.693333333333\n",
      "0.645244215938\n",
      "0.689008042895\n",
      "0.712846347607\n",
      "0.697297297297\n",
      "0.671957671958\n",
      "0.6875\n",
      "0.679790026247\n",
      "0.676315789474\n",
      "0.6875\n",
      "0.69436997319\n",
      "0.686224489796\n",
      "0.667532467532\n",
      "0.663129973475\n",
      "0.730478589421\n",
      "0.647959183673\n",
      "0.6735218509\n",
      "0.693717277487\n",
      "0.691489361702\n",
      "0.748677248677\n",
      "0.717616580311\n",
      "0.725593667546\n",
      "0.709090909091\n",
      "0.68449197861\n",
      "0.719072164948\n",
      "0.678756476684\n",
      "0.659400544959\n",
      "0.671916010499\n",
      "0.692695214106\n",
      "0.720547945205\n",
      "0.665809768638\n",
      "0.670157068063\n",
      "0.684636118598\n",
      "0.710182767624\n",
      "0.712793733681\n",
      "0.704188481675\n",
      "0.7265625\n",
      "0.690217391304\n",
      "0.75\n",
      "0.666666666667\n",
      "0.677165354331\n",
      "0.69623655914\n",
      "0.715039577836\n",
      "0.717557251908\n",
      "0.701570680628\n",
      "0.677506775068\n",
      "0.685421994885\n",
      "0.690104166667\n",
      "0.67029972752\n",
      "0.686528497409\n",
      "0.664082687339\n",
      "0.707894736842\n",
      "0.716535433071\n",
      "0.675324675325\n",
      "0.688654353562\n",
      "0.694736842105\n",
      "0.688654353562\n",
      "0.7\n",
      "0.687338501292\n",
      "0.685567010309\n",
      "0.713527851459\n",
      "0.711055276382\n",
      "0.691919191919\n",
      "0.712468193384\n",
      "0.668407310705\n",
      "0.69918699187\n",
      "0.658792650919\n",
      "0.653439153439\n",
      "0.715053763441\n",
      "0.669211195929\n",
      "0.698952879581\n",
      "0.657824933687\n",
      "0.693121693122\n",
      "0.697612732095\n",
      "0.693548387097\n",
      "0.630490956072\n",
      "0.670212765957\n",
      "0.670157068063\n",
      "0.693298969072\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: float() >= DecisionTreeClassifier()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-49691053b877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# plot scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# confidence intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3079\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m                       stacked=stacked, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   3082\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   6179\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6180\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6181\u001b[0;31m                     \u001b[0mxmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6182\u001b[0m                     \u001b[0mxmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6183\u001b[0m             \u001b[0mbin_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: float() >= DecisionTreeClassifier()"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "data_pima = read_csv('pima-indians-diabetes.data.csv', header=None)\n",
    "values = data_pima.values\n",
    "# configure bootstrap\n",
    "n_iterations = 100\n",
    "\n",
    "bootstrap_sample_sizes = [.5, .6, .7]\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for size in bootstrap_sample_sizes:\n",
    "    n_size = int(len(data_pima) * size)\n",
    "    for i in range(n_iterations):\n",
    "        # prepare train and test sets\n",
    "        train = resample(values, n_samples=n_size)\n",
    "        test = np.array([x for x in values if x.tolist() not in train.tolist()])\n",
    "        # fit model\n",
    "#         model = DecisionTreeClassifier()\n",
    "#         model.fit(train[:,:-1], train[:,-1])\n",
    "#         # evaluate model\n",
    "#         predictions = model.predict(test[:,:-1])\n",
    "#         score = accuracy_score(test[:,-1], predictions)\n",
    "#         print(score)\n",
    "#         stats.append((model, score))\n",
    "\n",
    "        yield (train, test)\n",
    "\n",
    "# plot scores\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower = max(0.0, np.percentile(stats, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75878220140515218"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[np.argsort(stats)[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Integrating with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88668, 9)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(data[[\n",
    "    'course_week', 'num_video_plays', 'num_problems_attempted',\n",
    "    'num_problems_correct', 'num_subsections_viewed', 'num_forum_posts',\n",
    "    'num_forum_votes', 'avg_forum_sentiment', 'user_started_week',\n",
    "]])\n",
    "Y = np.array(data['user_dropped_out_next_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(X)\n",
    "\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "\n",
    "def false_negative_percentage(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred)[1][0] / len(y_true)\n",
    "\n",
    "typeII_err = make_scorer(false_negative_percentage, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers_dim': [4, 6, 8], 'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'], 'epochs': [1], 'batch_size': [10], 'num_hidden_layers': [1, 2, 3]}\n",
      "{'type2_min': make_scorer(false_negative_percentage, greater_is_better=False), 'accuracy': make_scorer(accuracy_score)}\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 4:54 - loss: 0.6877 - acc: 0.8500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input34848\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 150s 2ms/step - loss: 0.3914 - acc: 0.8696\n",
      "49260/49260 [==============================] - 32s 645us/step\n",
      "98520/98520 [==============================] - 82s 832us/step\n",
      "Epoch 1/1\n",
      "   90/98520 [..............................] - ETA: 3:45 - loss: 0.6771 - acc: 0.8889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input35269\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 161s 2ms/step - loss: 0.3695 - acc: 0.8766\n",
      "49260/49260 [==============================] - 30s 609us/step\n",
      "98520/98520 [==============================] - 73s 744us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 30:14 - loss: 0.8455 - acc: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input35690\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 119s 1ms/step - loss: 0.3554 - acc: 0.8569\n",
      "49260/49260 [==============================] - 8s 163us/step\n",
      "98520/98520 [==============================] - 17s 178us/step\n",
      "Epoch 1/1\n",
      "  110/98520 [..............................] - ETA: 2:27 - loss: 0.7233 - acc: 0.2091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input36111\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 55s 561us/step - loss: 0.3051 - acc: 0.8718\n",
      "49260/49260 [==============================] - 9s 186us/step\n",
      "98520/98520 [==============================] - 16s 159us/step\n",
      "Epoch 1/1\n",
      "  750/98520 [..............................] - ETA: 24s - loss: 0.6621 - acc: 0.8853"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input36712\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 57s 575us/step - loss: 0.3780 - acc: 0.8766\n",
      "49260/49260 [==============================] - 10s 205us/step\n",
      "98520/98520 [==============================] - 18s 180us/step\n",
      "Epoch 1/1\n",
      "  150/98520 [..............................] - ETA: 1:38 - loss: 0.5486 - acc: 0.8467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input37313\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 65s 659us/step - loss: 0.3085 - acc: 0.8598\n",
      "49260/49260 [==============================] - 11s 218us/step\n",
      "98520/98520 [==============================] - 18s 182us/step\n",
      "Epoch 1/1\n",
      "  740/98520 [..............................] - ETA: 24s - loss: 0.4922 - acc: 0.8635"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input37913\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 66s 668us/step - loss: 0.3604 - acc: 0.8696\n",
      "49260/49260 [==============================] - 12s 241us/step\n",
      "98520/98520 [==============================] - 20s 204us/step\n",
      "Epoch 1/1\n",
      "  230/98520 [..............................] - ETA: 1:21 - loss: 0.6335 - acc: 0.8826"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input38435\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 57s 582us/step - loss: 0.3627 - acc: 0.8766\n",
      "49260/49260 [==============================] - 8s 172us/step\n",
      "98520/98520 [==============================] - 21s 212us/step\n",
      "Epoch 1/1\n",
      "  260/98520 [..............................] - ETA: 1:07 - loss: 0.6688 - acc: 0.7538   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input38957\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 72s 731us/step - loss: 0.4205 - acc: 0.85771s - \n",
      "49260/49260 [==============================] - 8s 170us/step\n",
      "98520/98520 [==============================] - 21s 216us/step\n",
      "Epoch 1/1\n",
      "  710/98520 [..............................] - ETA: 26s - loss: 0.6135 - acc: 0.8761"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input39479\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 77s 782us/step - loss: 0.3704 - acc: 0.8696\n",
      "49260/49260 [==============================] - 15s 304us/step\n",
      "98520/98520 [==============================] - 18s 185us/step\n",
      "Epoch 1/1\n",
      "   80/98520 [..............................] - ETA: 3:59 - loss: 0.6908 - acc: 0.9125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input40295\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 76s 771us/step - loss: 0.3562 - acc: 0.8766\n",
      "49260/49260 [==============================] - 14s 287us/step\n",
      "98520/98520 [==============================] - 20s 202us/step\n",
      "Epoch 1/1\n",
      "  440/98520 [..............................] - ETA: 43s - loss: 0.6028 - acc: 0.8477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input41111\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 73s 737us/step - loss: 0.3645 - acc: 0.8580\n",
      "49260/49260 [==============================] - 9s 186us/step\n",
      "98520/98520 [==============================] - 18s 187us/step\n",
      "Epoch 1/1\n",
      "  740/98520 [..............................] - ETA: 25s - loss: 0.5925 - acc: 0.8581"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input41929\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 65s 665us/step - loss: 0.3023 - acc: 0.8696\n",
      "49260/49260 [==============================] - 8s 163us/step\n",
      "98520/98520 [==============================] - 18s 187us/step\n",
      "Epoch 1/1\n",
      "  840/98520 [..............................] - ETA: 23s - loss: 0.6680 - acc: 0.7690 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input42720\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 62s 631us/step - loss: 0.2715 - acc: 0.8758\n",
      "49260/49260 [==============================] - 10s 196us/step\n",
      "98520/98520 [==============================] - 21s 217us/step\n",
      "Epoch 1/1\n",
      "   50/98520 [..............................] - ETA: 6:34 - loss: 0.6504 - acc: 0.9200 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input43511\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 72s 733us/step - loss: 0.3041 - acc: 0.8662\n",
      "49260/49260 [==============================] - 8s 164us/step\n",
      "98520/98520 [==============================] - 19s 198us/step\n",
      "Epoch 1/1\n",
      "  840/98520 [..............................] - ETA: 21s - loss: 0.5952 - acc: 0.8095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input44302\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 128s 1ms/step - loss: 0.2988 - acc: 0.8691\n",
      "49260/49260 [==============================] - ETA:  - 35s 703us/step\n",
      "98520/98520 [==============================] - 68s 691us/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 4:08 - loss: 0.6892 - acc: 0.9333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input45034\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 180s 2ms/step - loss: 0.3636 - acc: 0.8766\n",
      "49260/49260 [==============================] - 31s 629us/step\n",
      "98520/98520 [==============================] - 63s 641us/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 5:19 - loss: 0.6867 - acc: 0.6833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input45766\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 184s 2ms/step - loss: 0.3836 - acc: 0.8579\n",
      "49260/49260 [==============================] - 35s 702us/step\n",
      "98520/98520 [==============================] - 63s 639us/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 7:54 - loss: 0.7245 - acc: 0.0667 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input46498\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 218s 2ms/step - loss: 0.2283 - acc: 0.8948\n",
      "49260/49260 [==============================] - 37s 756us/step\n",
      "98520/98520 [==============================] - 65s 655us/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 6:30 - loss: 0.6917 - acc: 0.6500     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input47687\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 212s 2ms/step - loss: 0.2253 - acc: 0.8940\n",
      "49260/49260 [==============================] - 30s 602us/step\n",
      "98520/98520 [==============================] - 63s 636us/step\n",
      "Epoch 1/1\n",
      "  100/98520 [..............................] - ETA: 3:13 - loss: 0.6002 - acc: 0.8900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input48876\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 215s 2ms/step - loss: 0.3209 - acc: 0.8580\n",
      "49260/49260 [==============================] - 30s 617us/step\n",
      "98520/98520 [==============================] - 69s 698us/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input50081\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 177s 2ms/step - loss: 0.2995 - acc: 0.8696\n",
      "49260/49260 [==============================] - 42s 847us/step\n",
      "98520/98520 [==============================] - 52s 528us/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input50591\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 59s 600us/step - loss: 0.3627 - acc: 0.8766\n",
      "49260/49260 [==============================] - 8s 169us/step\n",
      "98520/98520 [==============================] - 21s 212us/step\n",
      "Epoch 1/1\n",
      "  880/98520 [..............................] - ETA: 20s - loss: 0.6066 - acc: 0.8034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input51101\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 58s 586us/step - loss: 0.3887 - acc: 0.8575\n",
      "49260/49260 [==============================] - 13s 255us/step\n",
      "98520/98520 [==============================] - 28s 280us/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 7:35 - loss: 0.6682 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input51611\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 74s 748us/step - loss: 0.3052 - acc: 0.8696\n",
      "49260/49260 [==============================] - 13s 269us/step\n",
      "98520/98520 [==============================] - 21s 213us/step\n",
      "Epoch 1/1\n",
      "  170/98520 [..............................] - ETA: 1:36 - loss: 0.6684 - acc: 0.8176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input52361\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 78s 792us/step - loss: 0.2268 - acc: 0.8814\n",
      "49260/49260 [==============================] - 8s 170us/step\n",
      "98520/98520 [==============================] - 21s 211us/step\n",
      "Epoch 1/1\n",
      "  780/98520 [..............................] - ETA: 24s - loss: 0.6187 - acc: 0.8410"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input53111\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 73s 737us/step - loss: 0.3372 - acc: 0.8585\n",
      "49260/49260 [==============================] - 13s 273us/step\n",
      "98520/98520 [==============================] - 21s 216us/step\n",
      "Epoch 1/1\n",
      "  790/98520 [..............................] - ETA: 22s - loss: 0.6161 - acc: 0.8367"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input53860\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 93s 942us/step - loss: 0.3774 - acc: 0.8692\n",
      "49260/49260 [==============================] - 41s 826us/step\n",
      "98520/98520 [==============================] - 37s 374us/step\n",
      "Epoch 1/1\n",
      "  830/98520 [..............................] - ETA: 21s - loss: 0.5830 - acc: 0.8217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input54505\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 68s 691us/step - loss: 0.3655 - acc: 0.8763\n",
      "49260/49260 [==============================] - 15s 302us/step\n",
      "98520/98520 [==============================] - 39s 395us/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 4:34 - loss: 0.6853 - acc: 0.7000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input55150\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 95s 960us/step - loss: 0.3933 - acc: 0.8579\n",
      "49260/49260 [==============================] - 8s 167us/step\n",
      "98520/98520 [==============================] - 20s 207us/step\n",
      "Epoch 1/1\n",
      "   90/98520 [..............................] - ETA: 3:13 - loss: 0.7279 - acc: 0.1444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input55795\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 78s 795us/step - loss: 0.3034 - acc: 0.8670\n",
      "49260/49260 [==============================] - 11s 213us/step\n",
      "98520/98520 [==============================] - 23s 234us/step\n",
      "Epoch 1/1\n",
      "   80/98520 [..............................] - ETA: 3:35 - loss: 0.7086 - acc: 0.2250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input56832\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 89s 902us/step - loss: 0.3160 - acc: 0.8732\n",
      "49260/49260 [==============================] - 15s 299us/step\n",
      "98520/98520 [==============================] - 21s 218us/step\n",
      "Epoch 1/1\n",
      "  100/98520 [..............................] - ETA: 2:51 - loss: 0.6925 - acc: 0.6400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input57869\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 107s 1ms/step - loss: 0.3843 - acc: 0.8578\n",
      "49260/49260 [==============================] - 14s 283us/step\n",
      "98520/98520 [==============================] - 23s 229us/step\n",
      "Epoch 1/1\n",
      "  450/98520 [..............................] - ETA: 35s - loss: 0.5810 - acc: 0.8667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input58908\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 132s 1ms/step - loss: 0.2749 - acc: 0.8766\n",
      "49260/49260 [==============================] - 11s 214us/step\n",
      "98520/98520 [==============================] - 20s 206us/step\n",
      "Epoch 1/1\n",
      "  560/98520 [..............................] - ETA: 34s - loss: 0.6072 - acc: 0.8714 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input59898\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 71s 720us/step - loss: 0.2492 - acc: 0.8877\n",
      "49260/49260 [==============================] - 12s 253us/step\n",
      "98520/98520 [==============================] - 23s 229us/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 7:09 - loss: 0.6962 - acc: 0.2250 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input60888\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 124s 1ms/step - loss: 0.4200 - acc: 0.8577\n",
      "49260/49260 [==============================] - 10s 205us/step\n",
      "98520/98520 [==============================] - 21s 209us/step\n",
      "Epoch 1/1\n",
      "  750/98520 [..............................] - ETA: 25s - loss: 0.7122 - acc: 0.5120 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input61878\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 142s 1ms/step - loss: 0.3146 - acc: 0.8669\n",
      "49260/49260 [==============================] - 40s 819us/step\n",
      "98520/98520 [==============================] - 92s 930us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 18:14 - loss: 0.6895 - acc: 0.8000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input62795\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 232s 2ms/step - loss: 0.3460 - acc: 0.8766\n",
      "49260/49260 [==============================] - 10s 197us/step\n",
      "98520/98520 [==============================] - 24s 240us/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 4:15 - loss: 0.6896 - acc: 0.8286 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input63712\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 91s 926us/step - loss: 0.3168 - acc: 0.8583\n",
      "49260/49260 [==============================] - 13s 268us/step\n",
      "98520/98520 [==============================] - 22s 228us/step\n",
      "Epoch 1/1\n",
      "  270/98520 [..............................] - ETA: 1:13 - loss: 0.5628 - acc: 0.8630"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input64629\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 103s 1ms/step - loss: 0.2514 - acc: 0.8829\n",
      "49260/49260 [==============================] - 11s 230us/step\n",
      "98520/98520 [==============================] - 26s 263us/step\n",
      "Epoch 1/1\n",
      "   90/98520 [..............................] - ETA: 3:39 - loss: 0.7048 - acc: 0.2556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input66137\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 113s 1ms/step - loss: 0.2218 - acc: 0.8959\n",
      "49260/49260 [==============================] - 13s 263us/step\n",
      "98520/98520 [==============================] - 36s 361us/step\n",
      "Epoch 1/1\n",
      "  100/98520 [..............................] - ETA: 3:07 - loss: 0.6391 - acc: 0.8700"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input67645\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 233s 2ms/step - loss: 0.2541 - acc: 0.8785\n",
      "49260/49260 [==============================] - 47s 963us/step\n",
      "98520/98520 [==============================] - 100s 1ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 6:23 - loss: 0.6492 - acc: 0.9500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input69169\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 248s 3ms/step - loss: 0.2914 - acc: 0.8696\n",
      "49260/49260 [==============================] - 49s 996us/step\n",
      "98520/98520 [==============================] - 119s 1ms/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 4:43 - loss: 0.5227 - acc: 0.9143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input69768\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 238s 2ms/step - loss: 0.2582 - acc: 0.8766\n",
      "49260/49260 [==============================] - 43s 882us/step\n",
      "98520/98520 [==============================] - 94s 951us/step\n",
      "Epoch 1/1\n",
      "  150/98520 [..............................] - ETA: 2:08 - loss: 0.6764 - acc: 0.8200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input70367\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 203s 2ms/step - loss: 0.3526 - acc: 0.8580\n",
      "49260/49260 [==============================] - 54s 1ms/step\n",
      "98520/98520 [==============================] - 101s 1ms/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 5:22 - loss: 0.6849 - acc: 0.8500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input70966\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 266s 3ms/step - loss: 0.2569 - acc: 0.8715\n",
      "49260/49260 [==============================] - 47s 944us/step\n",
      "98520/98520 [==============================] - 95s 969us/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 18:49 - loss: 0.6842 - acc: 0.90 - ETA: 10:48 - loss: 0.6767 - acc: 0.8667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input71865\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 264s 3ms/step - loss: 0.2590 - acc: 0.8772\n",
      "49260/49260 [==============================] - 50s 1ms/step\n",
      "98520/98520 [==============================] - 93s 946us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 15:30 - loss: 0.6906 - acc: 0.7000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input72764\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 252s 3ms/step - loss: 0.3535 - acc: 0.8589\n",
      "49260/49260 [==============================] - 38s 779us/step\n",
      "98520/98520 [==============================] - 100s 1ms/step\n",
      "Epoch 1/1\n",
      "   50/98520 [..............................] - ETA: 3:43 - loss: 0.6683 - acc: 0.7800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input73662\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 248s 3ms/step - loss: 0.3718 - acc: 0.8695 3s - loss:\n",
      "49260/49260 [==============================] - 50s 1ms/step\n",
      "98520/98520 [==============================] - 104s 1ms/step\n",
      "Epoch 1/1\n",
      "   20/98520 [..............................] - ETA: 13:04 - loss: 0.6881 - acc: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input74430\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 258s 3ms/step - loss: 0.3958 - acc: 0.8766\n",
      "49260/49260 [==============================] - 15s 306us/step\n",
      "98520/98520 [==============================] - 22s 225us/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 10:30 - loss: 0.6829 - acc: 0.8000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input75198\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 76s 776us/step - loss: 0.3846 - acc: 0.8580\n",
      "49260/49260 [==============================] - 8s 159us/step\n",
      "98520/98520 [==============================] - 22s 220us/step\n",
      "Epoch 1/1\n",
      "  110/98520 [..............................] - ETA: 2:27 - loss: 0.6419 - acc: 0.8182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input75966\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 83s 844us/step - loss: 0.3330 - acc: 0.8696\n",
      "49260/49260 [==============================] - 8s 155us/step\n",
      "98520/98520 [==============================] - 72s 731us/step\n",
      "Epoch 1/1\n",
      "  210/98520 [..............................] - ETA: 1:16 - loss: 0.6156 - acc: 0.8905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input77224\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 317s 3ms/step - loss: 0.3255 - acc: 0.8766\n",
      "49260/49260 [==============================] - 59s 1ms/step\n",
      "98520/98520 [==============================] - 81s 825us/step\n",
      "Epoch 1/1\n",
      "   50/98520 [..............................] - ETA: 5:14 - loss: 0.6560 - acc: 0.9200 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input78482\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 93s 944us/step - loss: 0.3561 - acc: 0.85800s - loss: 0.3562 - acc: 0.\n",
      "49260/49260 [==============================] - 10s 194us/step\n",
      "98520/98520 [==============================] - 27s 272us/step\n",
      "Epoch 1/1\n",
      "   80/98520 [..............................] - ETA: 3:28 - loss: 0.6846 - acc: 0.7625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input79742\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 86s 869us/step - loss: 0.2688 - acc: 0.8696\n",
      "49260/49260 [==============================] - 14s 294us/step\n",
      "98520/98520 [==============================] - 23s 233us/step\n",
      "Epoch 1/1\n",
      "  230/98520 [..............................] - ETA: 1:07 - loss: 0.6845 - acc: 0.8957 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input80931\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 93s 940us/step - loss: 0.3867 - acc: 0.8766\n",
      "49260/49260 [==============================] - 9s 185us/step\n",
      "98520/98520 [==============================] - 26s 260us/step\n",
      "Epoch 1/1\n",
      "  390/98520 [..............................] - ETA: 39s - loss: 0.6810 - acc: 0.7359 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input82120\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 144s 1ms/step - loss: 0.2783 - acc: 0.8736\n",
      "49260/49260 [==============================] - 33s 669us/step\n",
      "98520/98520 [==============================] - 25s 256us/step\n",
      "Epoch 1/1\n",
      "  120/98520 [..............................] - ETA: 2:41 - loss: 0.6344 - acc: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input83309\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 204s 2ms/step - loss: 0.2762 - acc: 0.8722\n",
      "49260/49260 [==============================] - 105s 2ms/step\n",
      "98520/98520 [==============================] - 208s 2ms/step\n",
      "Epoch 1/1\n",
      "   20/98520 [..............................] - ETA: 14:43 - loss: 0.7102 - acc: 0.0500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input84411\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 512s 5ms/step - loss: 0.2983 - acc: 0.8715\n",
      "49260/49260 [==============================] - 87s 2ms/step\n",
      "98520/98520 [==============================] - 191s 2ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input85513\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 489s 5ms/step - loss: 0.3743 - acc: 0.8580\n",
      "49260/49260 [==============================] - 92s 2ms/step\n",
      "98520/98520 [==============================] - 192s 2ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input86615\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 561s 6ms/step - loss: 0.2220 - acc: 0.8956\n",
      "49260/49260 [==============================] - 92s 2ms/step\n",
      "98520/98520 [==============================] - 195s 2ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input88442\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 556s 6ms/step - loss: 0.1997 - acc: 0.9032\n",
      "49260/49260 [==============================] - 95s 2ms/step\n",
      "98520/98520 [==============================] - 191s 2ms/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 29:59 - loss: 0.6120 - acc: 0.7000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input90269\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 567s 6ms/step - loss: 0.3011 - acc: 0.8580\n",
      "49260/49260 [==============================] - 96s 2ms/step\n",
      "98520/98520 [==============================] - 186s 2ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 7:57 - loss: 0.6903 - acc: 0.8250 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input92061\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 244s 2ms/step - loss: 0.3892 - acc: 0.8696\n",
      "49260/49260 [==============================] - 60s 1ms/step\n",
      "98520/98520 [==============================] - 120s 1ms/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 27:45 - loss: 0.7216 - acc: 0.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input92482\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 254s 3ms/step - loss: 0.3120 - acc: 0.8763\n",
      "49260/49260 [==============================] - 64s 1ms/step\n",
      "98520/98520 [==============================] - 125s 1ms/step\n",
      "Epoch 1/1\n",
      "   90/98520 [..............................] - ETA: 2:51 - loss: 0.6090 - acc: 0.8556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input92903\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 243s 2ms/step - loss: 0.3667 - acc: 0.8579\n",
      "49260/49260 [==============================] - 64s 1ms/step\n",
      "98520/98520 [==============================] - 122s 1ms/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 5:16 - loss: 0.6606 - acc: 0.9333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input93324\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 328s 3ms/step - loss: 0.2727 - acc: 0.8753\n",
      "49260/49260 [==============================] - 60s 1ms/step\n",
      "98520/98520 [==============================] - 123s 1ms/step\n",
      "Epoch 1/1\n",
      "   50/98520 [..............................] - ETA: 6:56 - loss: 0.5363 - acc: 0.8800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input93925\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 324s 3ms/step - loss: 0.2335 - acc: 0.8920\n",
      "49260/49260 [==============================] - 64s 1ms/step\n",
      "98520/98520 [==============================] - 123s 1ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input94526\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 327s 3ms/step - loss: 0.3207 - acc: 0.8650\n",
      "49260/49260 [==============================] - 61s 1ms/step\n",
      "98520/98520 [==============================] - 115s 1ms/step\n",
      "Epoch 1/1\n",
      "   50/98520 [..............................] - ETA: 6:33 - loss: 0.6840 - acc: 0.6000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input95126\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 323s 3ms/step - loss: 0.3734 - acc: 0.8694\n",
      "49260/49260 [==============================] - 60s 1ms/step\n",
      "98520/98520 [==============================] - 119s 1ms/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 3:11 - loss: 0.6599 - acc: 0.8571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input95648\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 331s 3ms/step - loss: 0.3351 - acc: 0.8766\n",
      "49260/49260 [==============================] - 60s 1ms/step\n",
      "98520/98520 [==============================] - 126s 1ms/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 32:46 - loss: 0.7085 - acc: 0.4000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input96170\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 335s 3ms/step - loss: 0.3844 - acc: 0.8579\n",
      "49260/49260 [==============================] - 61s 1ms/step\n",
      "98520/98520 [==============================] - 118s 1ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input96692\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 402s 4ms/step - loss: 0.3615 - acc: 0.8615\n",
      "49260/49260 [==============================] - 63s 1ms/step\n",
      "98520/98520 [==============================] - 118s 1ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 9:54 - loss: 0.6770 - acc: 0.7333 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input97508\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 404s 4ms/step - loss: 0.2534 - acc: 0.8768\n",
      "49260/49260 [==============================] - 59s 1ms/step\n",
      "98520/98520 [==============================] - 122s 1ms/step\n",
      "Epoch 1/1\n",
      "   20/98520 [..............................] - ETA: 15:57 - loss: 0.7683 - acc: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input98324\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 401s 4ms/step - loss: 0.3658 - acc: 0.8538\n",
      "49260/49260 [==============================] - 62s 1ms/step\n",
      "98520/98520 [==============================] - 123s 1ms/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 4:44 - loss: 0.5704 - acc: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input99142\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 360s 4ms/step - loss: 0.2612 - acc: 0.8765\n",
      "49260/49260 [==============================] - 62s 1ms/step\n",
      "98520/98520 [==============================] - 125s 1ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 7:56 - loss: 0.5807 - acc: 0.9667 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input99933\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 351s 4ms/step - loss: 0.2352 - acc: 0.8910\n",
      "49260/49260 [==============================] - 60s 1ms/step\n",
      "98520/98520 [==============================] - 125s 1ms/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 4:29 - loss: 0.7933 - acc: 0.0714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input100724\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 344s 3ms/step - loss: 0.2887 - acc: 0.8655\n",
      "49260/49260 [==============================] - 66s 1ms/step\n",
      "98520/98520 [==============================] - 119s 1ms/step\n",
      "Epoch 1/1\n",
      "   50/98520 [..............................] - ETA: 4:28 - loss: 0.6535 - acc: 0.9200 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input101515\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 334s 3ms/step - loss: 0.3513 - acc: 0.8696\n",
      "49260/49260 [==============================] - 65s 1ms/step\n",
      "98520/98520 [==============================] - 125s 1ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 9:20 - loss: 0.7017 - acc: 0.3667 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input102247\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 335s 3ms/step - loss: 0.3199 - acc: 0.8762\n",
      "49260/49260 [==============================] - 61s 1ms/step\n",
      "98520/98520 [==============================] - 111s 1ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input102979\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 337s 3ms/step - loss: 0.3715 - acc: 0.8571\n",
      "49260/49260 [==============================] - 58s 1ms/step\n",
      "98520/98520 [==============================] - 120s 1ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input103711\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 375s 4ms/step - loss: 0.2392 - acc: 0.8928\n",
      "49260/49260 [==============================] - 63s 1ms/step\n",
      "98520/98520 [==============================] - 126s 1ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input104900\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 374s 4ms/step - loss: 0.2087 - acc: 0.8990\n",
      "49260/49260 [==============================] - 58s 1ms/step\n",
      "98520/98520 [==============================] - 120s 1ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input106089\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 378s 4ms/step - loss: 0.2915 - acc: 0.8718\n",
      "49260/49260 [==============================] - 65s 1ms/step\n",
      "98520/98520 [==============================] - 125s 1ms/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 4:32 - loss: 0.8534 - acc: 0.2143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input107294\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 296s 3ms/step - loss: 0.3612 - acc: 0.8679\n",
      "49260/49260 [==============================] - 79s 2ms/step\n",
      "98520/98520 [==============================] - 159s 2ms/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 5:06 - loss: 0.6812 - acc: 0.6833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input107804\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 309s 3ms/step - loss: 0.3199 - acc: 0.8765\n",
      "49260/49260 [==============================] - 80s 2ms/step\n",
      "98520/98520 [==============================] - 158s 2ms/step\n",
      "Epoch 1/1\n",
      "  110/98520 [..............................] - ETA: 2:26 - loss: 0.6493 - acc: 0.7909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input108314\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 305s 3ms/step - loss: 0.3955 - acc: 0.8579\n",
      "49260/49260 [==============================] - 77s 2ms/step\n",
      "98520/98520 [==============================] - 163s 2ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 9:35 - loss: 0.6782 - acc: 0.9333 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input108824\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 398s 4ms/step - loss: 0.2680 - acc: 0.8849\n",
      "49260/49260 [==============================] - 79s 2ms/step\n",
      "98520/98520 [==============================] - 154s 2ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 7:28 - loss: 0.8396 - acc: 0.1000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input109574\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 420s 4ms/step - loss: 0.2456 - acc: 0.8894\n",
      "49260/49260 [==============================] - 76s 2ms/step\n",
      "98520/98520 [==============================] - 161s 2ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 5:54 - loss: 0.7007 - acc: 0.3250 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input110324\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 407s 4ms/step - loss: 0.2787 - acc: 0.8719\n",
      "49260/49260 [==============================] - 79s 2ms/step\n",
      "98520/98520 [==============================] - 156s 2ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 8:32 - loss: 0.6626 - acc: 0.8000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input111073\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 392s 4ms/step - loss: 0.3277 - acc: 0.8695\n",
      "49260/49260 [==============================] - 81s 2ms/step\n",
      "98520/98520 [==============================] - 151s 2ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 7:54 - loss: 0.6612 - acc: 0.7333 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input111718\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 395s 4ms/step - loss: 0.3211 - acc: 0.8766\n",
      "49260/49260 [==============================] - 76s 2ms/step\n",
      "98520/98520 [==============================] - 158s 2ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 5:09 - loss: 0.6172 - acc: 0.8000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input112363\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 405s 4ms/step - loss: 0.3863 - acc: 0.8580\n",
      "49260/49260 [==============================] - 81s 2ms/step\n",
      "98520/98520 [==============================] - 162s 2ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 10:47 - loss: 0.6088 - acc: 0.8667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input113008\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 488s 5ms/step - loss: 0.3348 - acc: 0.8696\n",
      "49260/49260 [==============================] - 74s 2ms/step\n",
      "98520/98520 [==============================] - 151s 2ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 5:53 - loss: 0.6313 - acc: 0.8250 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input114045\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 504s 5ms/step - loss: 0.3089 - acc: 0.8766\n",
      "49260/49260 [==============================] - 83s 2ms/step\n",
      "98520/98520 [==============================] - 150s 2ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 7:46 - loss: 0.6657 - acc: 0.8000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input115082\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 502s 5ms/step - loss: 0.3065 - acc: 0.8567\n",
      "49260/49260 [==============================] - 72s 1ms/step\n",
      "98520/98520 [==============================] - 146s 1ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 7:21 - loss: 0.6493 - acc: 0.8750 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input116121\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 426s 4ms/step - loss: 0.2381 - acc: 0.8910\n",
      "49260/49260 [==============================] - 74s 2ms/step\n",
      "98520/98520 [==============================] - 144s 1ms/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 4:42 - loss: 0.7297 - acc: 0.1429"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input117111\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 433s 4ms/step - loss: 0.2167 - acc: 0.8954\n",
      "49260/49260 [==============================] - 81s 2ms/step\n",
      "98520/98520 [==============================] - 159s 2ms/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 10:26 - loss: 0.6836 - acc: 0.8667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input118101\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 430s 4ms/step - loss: 0.2865 - acc: 0.8666\n",
      "49260/49260 [==============================] - 80s 2ms/step\n",
      "98520/98520 [==============================] - 154s 2ms/step\n",
      "Epoch 1/1\n",
      "   80/98520 [..............................] - ETA: 4:08 - loss: 0.6994 - acc: 0.2875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input119091\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 406s 4ms/step - loss: 0.2954 - acc: 0.8695 0s - loss: 0.2955 - acc: \n",
      "49260/49260 [==============================] - 80s 2ms/step\n",
      "98520/98520 [==============================] - 155s 2ms/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 26:41 - loss: 0.6867 - acc: 0.7000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input120008\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 406s 4ms/step - loss: 0.2515 - acc: 0.8815\n",
      "49260/49260 [==============================] - 74s 2ms/step\n",
      "98520/98520 [==============================] - 158s 2ms/step\n",
      "Epoch 1/1\n",
      "   20/98520 [..............................] - ETA: 14:47 - loss: 0.7098 - acc: 0.4500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input120925\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 411s 4ms/step - loss: 0.3812 - acc: 0.8577\n",
      "49260/49260 [==============================] - 79s 2ms/step\n",
      "98520/98520 [==============================] - 149s 2ms/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 17:42 - loss: 0.6570 - acc: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input121842\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 475s 5ms/step - loss: 0.2236 - acc: 0.8949\n",
      "49260/49260 [==============================] - 79s 2ms/step\n",
      "98520/98520 [==============================] - 154s 2ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input123350\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 467s 5ms/step - loss: 0.2277 - acc: 0.8805\n",
      "49260/49260 [==============================] - 78s 2ms/step\n",
      "98520/98520 [==============================] - 156s 2ms/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 29:27 - loss: 0.6637 - acc: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input124858\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 463s 5ms/step - loss: 0.2420 - acc: 0.8785\n",
      "49260/49260 [==============================] - 78s 2ms/step\n",
      "98520/98520 [==============================] - 156s 2ms/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 6:55 - loss: 0.7051 - acc: 0.2500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input126382\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 357s 4ms/step - loss: 0.3845 - acc: 0.8693\n",
      "49260/49260 [==============================] - 98s 2ms/step\n",
      "98520/98520 [==============================] - 201s 2ms/step\n",
      "Epoch 1/1\n",
      "   20/98520 [..............................] - ETA: 14:37 - loss: 0.6812 - acc: 0.8500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input126981\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 354s 4ms/step - loss: 0.2639 - acc: 0.8822\n",
      "49260/49260 [==============================] - 95s 2ms/step\n",
      "98520/98520 [==============================] - 197s 2ms/step\n",
      "Epoch 1/1\n",
      "   50/98520 [..............................] - ETA: 5:14 - loss: 0.6594 - acc: 0.8400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input127580\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 355s 4ms/step - loss: 0.3521 - acc: 0.8580\n",
      "49260/49260 [==============================] - 95s 2ms/step\n",
      "98520/98520 [==============================] - 194s 2ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input128179\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 459s 5ms/step - loss: 0.2418 - acc: 0.8871\n",
      "49260/49260 [==============================] - 85s 2ms/step\n",
      "98520/98520 [==============================] - 173s 2ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input129078\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 488s 5ms/step - loss: 0.2218 - acc: 0.8993\n",
      "49260/49260 [==============================] - 96s 2ms/step\n",
      "98520/98520 [==============================] - 196s 2ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input129977\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 492s 5ms/step - loss: 0.2742 - acc: 0.8772\n",
      "49260/49260 [==============================] - 97s 2ms/step\n",
      "98520/98520 [==============================] - 186s 2ms/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 23:00 - loss: 0.7012 - acc: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input130875\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 187s 2ms/step - loss: 0.3398 - acc: 0.8690\n",
      "49260/49260 [==============================] - 8s 154us/step\n",
      "98520/98520 [==============================] - 21s 209us/step\n",
      "Epoch 1/1\n",
      "  820/98520 [..............................] - ETA: 21s - loss: 0.4564 - acc: 0.8720"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input131643\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 70s 707us/step - loss: 0.2835 - acc: 0.8766\n",
      "49260/49260 [==============================] - 8s 163us/step\n",
      "98520/98520 [==============================] - 20s 202us/step\n",
      "Epoch 1/1\n",
      "  290/98520 [..............................] - ETA: 57s - loss: 0.5841 - acc: 0.8414"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input132411\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 69s 695us/step - loss: 0.3581 - acc: 0.8579\n",
      "49260/49260 [==============================] - 8s 161us/step\n",
      "98520/98520 [==============================] - 21s 214us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 16:48 - loss: 0.6988 - acc: 0.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input133179\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 79s 804us/step - loss: 0.2702 - acc: 0.8691\n",
      "49260/49260 [==============================] - 10s 194us/step\n",
      "98520/98520 [==============================] - 17s 173us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 23:27 - loss: 0.8124 - acc: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input134437\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 79s 804us/step - loss: 0.2293 - acc: 0.8817\n",
      "49260/49260 [==============================] - 8s 164us/step\n",
      "98520/98520 [==============================] - 21s 209us/step\n",
      "Epoch 1/1\n",
      "  100/98520 [..............................] - ETA: 3:02 - loss: 0.6576 - acc: 0.8400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input135695\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 77s 784us/step - loss: 0.3364 - acc: 0.8580\n",
      "49260/49260 [==============================] - 13s 262us/step\n",
      "98520/98520 [==============================] - 20s 205us/step\n",
      "Epoch 1/1\n",
      "  440/98520 [..............................] - ETA: 36s - loss: 0.6510 - acc: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input136955\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 72s 728us/step - loss: 0.2475 - acc: 0.8869\n",
      "49260/49260 [==============================] - 13s 257us/step\n",
      "98520/98520 [==============================] - 21s 213us/step\n",
      "Epoch 1/1\n",
      "  480/98520 [..............................] - ETA: 31s - loss: 0.6745 - acc: 0.7479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input138144\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 76s 770us/step - loss: 0.2164 - acc: 0.8925\n",
      "49260/49260 [==============================] - 8s 158us/step\n",
      "98520/98520 [==============================] - 20s 207us/step\n",
      "Epoch 1/1\n",
      "  130/98520 [..............................] - ETA: 2:17 - loss: 0.6921 - acc: 0.5462"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input139333\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 77s 783us/step - loss: 0.2564 - acc: 0.8847\n",
      "49260/49260 [==============================] - 13s 270us/step\n",
      "98520/98520 [==============================] - 20s 207us/step\n",
      "Epoch 1/1\n",
      "  340/98520 [..............................] - ETA: 56s - loss: 0.6731 - acc: 0.8235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input140522\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 73s 744us/step - loss: 0.3025 - acc: 0.8754\n",
      "49260/49260 [==============================] - 11s 218us/step\n",
      "98520/98520 [==============================] - 19s 197us/step\n",
      "Epoch 1/1\n",
      "  100/98520 [..............................] - ETA: 2:44 - loss: 0.6537 - acc: 0.9200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input141624\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 71s 717us/step - loss: 0.2455 - acc: 0.8887\n",
      "49260/49260 [==============================] - 8s 162us/step\n",
      "98520/98520 [==============================] - 20s 207us/step\n",
      "Epoch 1/1\n",
      "  480/98520 [..............................] - ETA: 31s - loss: 0.6668 - acc: 0.8375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input142726\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 69s 705us/step - loss: 0.3024 - acc: 0.8654\n",
      "49260/49260 [==============================] - 8s 163us/step\n",
      "98520/98520 [==============================] - 20s 204us/step\n",
      "Epoch 1/1\n",
      "  420/98520 [..............................] - ETA: 42s - loss: 0.5235 - acc: 0.8905 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input143828\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 90s 912us/step - loss: 0.2021 - acc: 0.9054\n",
      "49260/49260 [==============================] - 8s 156us/step\n",
      "98520/98520 [==============================] - 21s 210us/step\n",
      "Epoch 1/1\n",
      "  220/98520 [..............................] - ETA: 1:27 - loss: 0.6362 - acc: 0.8455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input145655\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 88s 897us/step - loss: 0.1930 - acc: 0.9123\n",
      "49260/49260 [==============================] - 8s 159us/step\n",
      "98520/98520 [==============================] - 20s 207us/step\n",
      "Epoch 1/1\n",
      "  390/98520 [..............................] - ETA: 45s - loss: 0.6362 - acc: 0.7615     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input147482\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 84s 850us/step - loss: 0.2546 - acc: 0.8839\n",
      "49260/49260 [==============================] - 10s 213us/step\n",
      "98520/98520 [==============================] - 19s 195us/step\n",
      "Epoch 1/1\n",
      "   80/98520 [..............................] - ETA: 3:28 - loss: 0.6532 - acc: 0.8875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input149274\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 46s 468us/step - loss: 0.3368 - acc: 0.8696\n",
      "49260/49260 [==============================] - 7s 141us/step\n",
      "98520/98520 [==============================] - 16s 162us/step\n",
      "Epoch 1/1\n",
      "  160/98520 [..............................] - ETA: 1:36 - loss: 0.5992 - acc: 0.8688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input149695\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 46s 466us/step - loss: 0.2893 - acc: 0.8766\n",
      "49260/49260 [==============================] - 7s 149us/step\n",
      "98520/98520 [==============================] - 15s 152us/step\n",
      "Epoch 1/1\n",
      "  240/98520 [..............................] - ETA: 1:21 - loss: 0.7524 - acc: 0.4500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input150116\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 47s 476us/step - loss: 0.3684 - acc: 0.8570\n",
      "49260/49260 [==============================] - 7s 140us/step\n",
      "98520/98520 [==============================] - 18s 178us/step\n",
      "Epoch 1/1\n",
      "  220/98520 [..............................] - ETA: 1:26 - loss: 0.6252 - acc: 0.8909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input150537\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 51s 514us/step - loss: 0.2656 - acc: 0.8809\n",
      "49260/49260 [==============================] - 8s 165us/step\n",
      "98520/98520 [==============================] - 15s 153us/step\n",
      "Epoch 1/1\n",
      "  930/98520 [..............................] - ETA: 18s - loss: 0.4459 - acc: 0.8914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input151138\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 53s 542us/step - loss: 0.2365 - acc: 0.8926\n",
      "49260/49260 [==============================] - 9s 177us/step\n",
      "98520/98520 [==============================] - 15s 151us/step\n",
      "Epoch 1/1\n",
      "  570/98520 [..............................] - ETA: 35s - loss: 0.6446 - acc: 0.6474 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input151739\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 51s 522us/step - loss: 0.2759 - acc: 0.8698\n",
      "49260/49260 [==============================] - 8s 169us/step\n",
      "98520/98520 [==============================] - 15s 157us/step\n",
      "Epoch 1/1\n",
      "  900/98520 [..............................] - ETA: 18s - loss: 0.4320 - acc: 0.8622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input152339\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 52s 526us/step - loss: 0.3283 - acc: 0.8696\n",
      "49260/49260 [==============================] - 9s 175us/step\n",
      "98520/98520 [==============================] - 16s 163us/step\n",
      "Epoch 1/1\n",
      "  510/98520 [..............................] - ETA: 31s - loss: 0.5832 - acc: 0.8490 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input152861\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 50s 512us/step - loss: 0.3479 - acc: 0.8765\n",
      "49260/49260 [==============================] - 9s 189us/step\n",
      "98520/98520 [==============================] - 17s 173us/step\n",
      "Epoch 1/1\n",
      "  440/98520 [..............................] - ETA: 43s - loss: 0.5044 - acc: 0.8136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input153383\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 52s 531us/step - loss: 0.3515 - acc: 0.8579\n",
      "49260/49260 [==============================] - 9s 189us/step\n",
      "98520/98520 [==============================] - 15s 147us/step\n",
      "Epoch 1/1\n",
      "  270/98520 [..............................] - ETA: 1:02 - loss: 0.6389 - acc: 0.7778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input153905\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 59s 600us/step - loss: 0.3567 - acc: 0.8688\n",
      "49260/49260 [==============================] - 10s 196us/step\n",
      "98520/98520 [==============================] - 15s 148us/step\n",
      "Epoch 1/1\n",
      "  750/98520 [..............................] - ETA: 24s - loss: 0.6173 - acc: 0.8747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input154721\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 62s 630us/step - loss: 0.2908 - acc: 0.8766\n",
      "49260/49260 [==============================] - 7s 151us/step\n",
      "98520/98520 [==============================] - 15s 148us/step\n",
      "Epoch 1/1\n",
      "  310/98520 [..............................] - ETA: 57s - loss: 0.6365 - acc: 0.8290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input155537\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 62s 628us/step - loss: 0.3296 - acc: 0.8579\n",
      "49260/49260 [==============================] - 7s 142us/step\n",
      "98520/98520 [==============================] - 15s 153us/step\n",
      "Epoch 1/1\n",
      "  140/98520 [..............................] - ETA: 1:53 - loss: 0.6345 - acc: 0.8071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input156355\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 57s 575us/step - loss: 0.2445 - acc: 0.8827\n",
      "49260/49260 [==============================] - 9s 191us/step\n",
      "98520/98520 [==============================] - 15s 157us/step\n",
      "Epoch 1/1\n",
      "  870/98520 [..............................] - ETA: 20s - loss: 0.4661 - acc: 0.8747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input157146\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 56s 566us/step - loss: 0.2334 - acc: 0.8927\n",
      "49260/49260 [==============================] - 8s 154us/step\n",
      "98520/98520 [==============================] - 17s 173us/step\n",
      "Epoch 1/1\n",
      "  140/98520 [..............................] - ETA: 1:47 - loss: 0.6763 - acc: 0.6143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input157937\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 58s 593us/step - loss: 0.2647 - acc: 0.8736\n",
      "49260/49260 [==============================] - 7s 147us/step\n",
      "98520/98520 [==============================] - 15s 154us/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input158728\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 55s 557us/step - loss: 0.3015 - acc: 0.8695\n",
      "49260/49260 [==============================] - 8s 156us/step\n",
      "98520/98520 [==============================] - 17s 168us/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input159460\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 53s 537us/step - loss: 0.2603 - acc: 0.8793\n",
      "49260/49260 [==============================] - 8s 158us/step\n",
      "98520/98520 [==============================] - 18s 186us/step\n",
      "Epoch 1/1\n",
      "  390/98520 [..............................] - ETA: 48s - loss: 0.6299 - acc: 0.7308 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input160192\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 55s 560us/step - loss: 0.2907 - acc: 0.8611\n",
      "49260/49260 [==============================] - 8s 157us/step\n",
      "98520/98520 [==============================] - 14s 146us/step\n",
      "Epoch 1/1\n",
      "  610/98520 [..............................] - ETA: 32s - loss: 0.5413 - acc: 0.7656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input160924\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 63s 641us/step - loss: 0.2211 - acc: 0.8970\n",
      "49260/49260 [==============================] - 8s 153us/step\n",
      "98520/98520 [==============================] - 19s 190us/step\n",
      "Epoch 1/1\n",
      "  440/98520 [..............................] - ETA: 35s - loss: 0.5312 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input162113\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 63s 639us/step - loss: 0.1987 - acc: 0.9045\n",
      "49260/49260 [==============================] - 7s 147us/step\n",
      "98520/98520 [==============================] - 15s 149us/step\n",
      "Epoch 1/1\n",
      "   80/98520 [..............................] - ETA: 3:29 - loss: 0.5985 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input163302\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 68s 695us/step - loss: 0.2529 - acc: 0.8785\n",
      "49260/49260 [==============================] - 7s 151us/step\n",
      "98520/98520 [==============================] - 19s 196us/step\n",
      "Epoch 1/1\n",
      "  360/98520 [..............................] - ETA: 35s - loss: 0.6328 - acc: 0.7806 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input164507\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 50s 505us/step - loss: 0.3069 - acc: 0.8693\n",
      "49260/49260 [==============================] - 8s 168us/step\n",
      "98520/98520 [==============================] - 19s 198us/step\n",
      "Epoch 1/1\n",
      "  640/98520 [..............................] - ETA: 28s - loss: 0.5668 - acc: 0.8656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input165017\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 55s 557us/step - loss: 0.3254 - acc: 0.8766\n",
      "49260/49260 [==============================] - 7s 152us/step\n",
      "98520/98520 [==============================] - 15s 157us/step\n",
      "Epoch 1/1\n",
      "   20/98520 [..............................] - ETA: 16:00 - loss: 0.6829 - acc: 0.8000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input165527\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 54s 551us/step - loss: 0.3390 - acc: 0.8580\n",
      "49260/49260 [==============================] - 9s 184us/step\n",
      "98520/98520 [==============================] - 19s 195us/step\n",
      "Epoch 1/1\n",
      "  540/98520 [..............................] - ETA: 35s - loss: 0.6485 - acc: 0.8444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input166037\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 57s 582us/step - loss: 0.2657 - acc: 0.8834\n",
      "49260/49260 [==============================] - 13s 254us/step\n",
      "98520/98520 [==============================] - 15s 151us/step\n",
      "Epoch 1/1\n",
      "  520/98520 [..............................] - ETA: 26s - loss: 0.5249 - acc: 0.8673"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input166787\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 61s 623us/step - loss: 0.2211 - acc: 0.8987\n",
      "49260/49260 [==============================] - 8s 158us/step\n",
      "98520/98520 [==============================] - 20s 203us/step\n",
      "Epoch 1/1\n",
      "  770/98520 [..............................] - ETA: 23s - loss: 0.6528 - acc: 0.7844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input167537\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 62s 628us/step - loss: 0.2926 - acc: 0.8679\n",
      "49260/49260 [==============================] - 7s 149us/step\n",
      "98520/98520 [==============================] - 15s 157us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 30:54 - loss: 0.6921 - acc: 0.4000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input168286\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 61s 622us/step - loss: 0.3142 - acc: 0.8695\n",
      "49260/49260 [==============================] - 8s 161us/step\n",
      "98520/98520 [==============================] - 20s 199us/step\n",
      "Epoch 1/1\n",
      "  670/98520 [..............................] - ETA: 27s - loss: 0.5898 - acc: 0.8254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input168931\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 56s 569us/step - loss: 0.2763 - acc: 0.8768\n",
      "49260/49260 [==============================] - 8s 154us/step\n",
      "98520/98520 [==============================] - 20s 205us/step\n",
      "Epoch 1/1\n",
      "  790/98520 [..............................] - ETA: 22s - loss: 0.3882 - acc: 0.8696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input169576\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 57s 580us/step - loss: 0.3340 - acc: 0.8580\n",
      "49260/49260 [==============================] - 11s 229us/step\n",
      "98520/98520 [==============================] - 16s 158us/step\n",
      "Epoch 1/1\n",
      "  480/98520 [..............................] - ETA: 30s - loss: 0.6422 - acc: 0.8625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input170221\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 70s 715us/step - loss: 0.2755 - acc: 0.8711\n",
      "49260/49260 [==============================] - 11s 229us/step\n",
      "98520/98520 [==============================] - 15s 155us/step\n",
      "Epoch 1/1\n",
      "  780/98520 [..............................] - ETA: 24s - loss: 0.6040 - acc: 0.8782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input171258\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 76s 774us/step - loss: 0.2632 - acc: 0.8810\n",
      "49260/49260 [==============================] - 7s 150us/step\n",
      "98520/98520 [==============================] - 20s 205us/step\n",
      "Epoch 1/1\n",
      "  720/98520 [..............................] - ETA: 27s - loss: 0.5570 - acc: 0.8556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input172295\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 71s 724us/step - loss: 0.3753 - acc: 0.8579\n",
      "49260/49260 [==============================] - 12s 239us/step\n",
      "98520/98520 [==============================] - 15s 154us/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 5:18 - loss: 0.8578 - acc: 0.1333 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input173334\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 67s 683us/step - loss: 0.2415 - acc: 0.8873\n",
      "49260/49260 [==============================] - 8s 154us/step\n",
      "98520/98520 [==============================] - 15s 149us/step\n",
      "Epoch 1/1\n",
      "  180/98520 [..............................] - ETA: 1:44 - loss: 0.6931 - acc: 0.5722    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input174324\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 68s 692us/step - loss: 0.2252 - acc: 0.8951\n",
      "49260/49260 [==============================] - 9s 187us/step\n",
      "98520/98520 [==============================] - 19s 191us/step\n",
      "Epoch 1/1\n",
      "  410/98520 [..............................] - ETA: 39s - loss: 0.6485 - acc: 0.8000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input175314\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 68s 694us/step - loss: 0.2508 - acc: 0.8863\n",
      "49260/49260 [==============================] - 8s 156us/step\n",
      "98520/98520 [==============================] - 20s 207us/step\n",
      "Epoch 1/1\n",
      "  810/98520 [..............................] - ETA: 23s - loss: 0.6319 - acc: 0.7914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input176304\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 66s 668us/step - loss: 0.2445 - acc: 0.8811\n",
      "49260/49260 [==============================] - 7s 152us/step\n",
      "98520/98520 [==============================] - 21s 209us/step\n",
      "Epoch 1/1\n",
      "  140/98520 [..............................] - ETA: 2:04 - loss: 0.7193 - acc: 0.3571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input177221\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 65s 657us/step - loss: 0.2423 - acc: 0.8867\n",
      "49260/49260 [==============================] - 8s 153us/step\n",
      "98520/98520 [==============================] - 19s 197us/step\n",
      "Epoch 1/1\n",
      "  360/98520 [..............................] - ETA: 42s - loss: 0.6618 - acc: 0.7444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input178138\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 66s 667us/step - loss: 0.2997 - acc: 0.8647\n",
      "49260/49260 [==============================] - 8s 166us/step\n",
      "98520/98520 [==============================] - 20s 203us/step\n",
      "Epoch 1/1\n",
      "  460/98520 [..............................] - ETA: 35s - loss: 0.6792 - acc: 0.6304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input179055\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 77s 783us/step - loss: 0.2186 - acc: 0.9002\n",
      "49260/49260 [==============================] - 8s 152us/step\n",
      "98520/98520 [==============================] - 19s 193us/step\n",
      "Epoch 1/1\n",
      "   60/98520 [..............................] - ETA: 4:33 - loss: 0.6577 - acc: 0.8833 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input180563\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 77s 783us/step - loss: 0.1987 - acc: 0.9052\n",
      "49260/49260 [==============================] - 7s 152us/step\n",
      "98520/98520 [==============================] - 17s 168us/step\n",
      "Epoch 1/1\n",
      "   30/98520 [..............................] - ETA: 8:28 - loss: 0.6639 - acc: 0.8000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input182071\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 74s 752us/step - loss: 0.2426 - acc: 0.8853\n",
      "49260/49260 [==============================] - 8s 155us/step\n",
      "98520/98520 [==============================] - 20s 199us/step\n",
      "Epoch 1/1\n",
      "  590/98520 [..............................] - ETA: 31s - loss: 0.6104 - acc: 0.8814"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input183595\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 56s 567us/step - loss: 0.3109 - acc: 0.8696\n",
      "49260/49260 [==============================] - 8s 164us/step\n",
      "98520/98520 [==============================] - 20s 204us/step\n",
      "Epoch 1/1\n",
      "  620/98520 [..............................] - ETA: 28s - loss: 0.5833 - acc: 0.8726"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input184194\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 56s 573us/step - loss: 0.2810 - acc: 0.8766\n",
      "49260/49260 [==============================] - 14s 283us/step\n",
      "98520/98520 [==============================] - 20s 202us/step\n",
      "Epoch 1/1\n",
      "  840/98520 [..............................] - ETA: 23s - loss: 0.5479 - acc: 0.8571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input184793\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 57s 575us/step - loss: 0.3287 - acc: 0.8619\n",
      "49260/49260 [==============================] - 13s 256us/step\n",
      "98520/98520 [==============================] - 17s 176us/step\n",
      "Epoch 1/1\n",
      "   90/98520 [..............................] - ETA: 2:55 - loss: 0.6589 - acc: 0.7889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input185392\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 67s 679us/step - loss: 0.2369 - acc: 0.8867\n",
      "49260/49260 [==============================] - 12s 235us/step\n",
      "98520/98520 [==============================] - 17s 173us/step\n",
      "Epoch 1/1\n",
      "   80/98520 [..............................] - ETA: 4:10 - loss: 0.6716 - acc: 0.8875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input186291\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 69s 696us/step - loss: 0.2146 - acc: 0.9028\n",
      "49260/49260 [==============================] - 8s 159us/step\n",
      "98520/98520 [==============================] - 21s 215us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 12:18 - loss: 0.7214 - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input187190\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 69s 699us/step - loss: 0.2644 - acc: 0.8787\n",
      "49260/49260 [==============================] - 8s 157us/step\n",
      "98520/98520 [==============================] - 20s 206us/step\n",
      "Epoch 1/1\n",
      "  580/98520 [..............................] - ETA: 30s - loss: 0.6024 - acc: 0.8569"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input188088\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 64s 647us/step - loss: 0.2703 - acc: 0.8714\n",
      "49260/49260 [==============================] - 12s 246us/step\n",
      "98520/98520 [==============================] - 16s 164us/step\n",
      "Epoch 1/1\n",
      "  190/98520 [..............................] - ETA: 1:29 - loss: 0.6645 - acc: 0.7526"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input188856\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 68s 688us/step - loss: 0.2716 - acc: 0.8798\n",
      "49260/49260 [==============================] - 8s 155us/step\n",
      "98520/98520 [==============================] - 17s 177us/step\n",
      "Epoch 1/1\n",
      "  100/98520 [..............................] - ETA: 3:11 - loss: 0.6369 - acc: 0.7500    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input189624\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 67s 683us/step - loss: 0.3221 - acc: 0.8579\n",
      "49260/49260 [==============================] - 8s 167us/step\n",
      "98520/98520 [==============================] - 21s 216us/step\n",
      "Epoch 1/1\n",
      "  300/98520 [..............................] - ETA: 52s - loss: 0.7232 - acc: 0.2733"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input190392\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 82s 832us/step - loss: 0.2914 - acc: 0.8673\n",
      "49260/49260 [==============================] - 11s 213us/step\n",
      "98520/98520 [==============================] - 23s 237us/step\n",
      "Epoch 1/1\n",
      "   70/98520 [..............................] - ETA: 4:27 - loss: 0.6814 - acc: 0.8857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input191650\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 81s 822us/step - loss: 0.2521 - acc: 0.8832\n",
      "49260/49260 [==============================] - 10s 210us/step\n",
      "98520/98520 [==============================] - 19s 190us/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 6:25 - loss: 0.6540 - acc: 0.8250 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input192908\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 82s 829us/step - loss: 0.3103 - acc: 0.8598\n",
      "49260/49260 [==============================] - 8s 160us/step\n",
      "98520/98520 [==============================] - 20s 205us/step\n",
      "Epoch 1/1\n",
      "  420/98520 [..............................] - ETA: 39s - loss: 0.6818 - acc: 0.7262 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input194168\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 74s 748us/step - loss: 0.2273 - acc: 0.8966\n",
      "49260/49260 [==============================] - 13s 262us/step\n",
      "98520/98520 [==============================] - 18s 187us/step\n",
      "Epoch 1/1\n",
      "   40/98520 [..............................] - ETA: 5:37 - loss: 0.6933 - acc: 0.6250 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input195357\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 76s 768us/step - loss: 0.2138 - acc: 0.9020\n",
      "49260/49260 [==============================] - 12s 236us/step\n",
      "98520/98520 [==============================] - 18s 186us/step\n",
      "Epoch 1/1\n",
      "   10/98520 [..............................] - ETA: 14:11 - loss: 0.6895 - acc: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input196546\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 79s 805us/step - loss: 0.2658 - acc: 0.8764\n",
      "49260/49260 [==============================] - 11s 226us/step\n",
      "98520/98520 [==============================] - 17s 173us/step\n",
      "Epoch 1/1\n",
      "   90/98520 [..............................] - ETA: 3:23 - loss: 0.6018 - acc: 0.8778 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input197735\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 74s 751us/step - loss: 0.2480 - acc: 0.8799\n",
      "49260/49260 [==============================] - 8s 159us/step\n",
      "98520/98520 [==============================] - 21s 209us/step\n",
      "Epoch 1/1\n",
      "  710/98520 [..............................] - ETA: 26s - loss: 0.5044 - acc: 0.8676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input198837\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 71s 722us/step - loss: 0.2159 - acc: 0.8898\n",
      "49260/49260 [==============================] - 8s 167us/step\n",
      "98520/98520 [==============================] - 20s 206us/step\n",
      "Epoch 1/1\n",
      "  680/98520 [..............................] - ETA: 29s - loss: 0.6218 - acc: 0.8044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input199939\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 72s 735us/step - loss: 0.2811 - acc: 0.8705\n",
      "49260/49260 [==============================] - 13s 268us/step\n",
      "98520/98520 [==============================] - 20s 202us/step\n",
      "Epoch 1/1\n",
      "   90/98520 [..............................] - ETA: 3:37 - loss: 0.6099 - acc: 0.9111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input201041\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 88s 890us/step - loss: 0.2059 - acc: 0.9049\n",
      "49260/49260 [==============================] - 8s 159us/step\n",
      "98520/98520 [==============================] - 22s 218us/step\n",
      "Epoch 1/1\n",
      "  300/98520 [..............................] - ETA: 1:01 - loss: 0.6464 - acc: 0.8733"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input202868\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 86s 876us/step - loss: 0.1789 - acc: 0.9141\n",
      "49260/49260 [==============================] - 11s 222us/step\n",
      "98520/98520 [==============================] - 24s 243us/step\n",
      "Epoch 1/1\n",
      "  300/98520 [..............................] - ETA: 1:06 - loss: 0.6549 - acc: 0.7300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input204695\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 86s 875us/step - loss: 0.2342 - acc: 0.8899\n",
      "49260/49260 [==============================] - 8s 165us/step\n",
      "98520/98520 [==============================] - 21s 212us/step\n",
      "Epoch 1/1\n",
      "   330/147780 [..............................] - ETA: 1:21 - loss: 0.6105 - acc: 0.8455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input206522\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147780/147780 [==============================] - 134s 904us/step - loss: 0.2130 - acc: 0.9033\n",
      "Best: -0.038747 using {'hidden_layers_dim': 8, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.131919 (0.015349) with: {'hidden_layers_dim': 4, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.105224 (0.032741) with: {'hidden_layers_dim': 4, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 4, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 4, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.117046 (0.034795) with: {'hidden_layers_dim': 4, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 4, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.076113 (0.025231) with: {'hidden_layers_dim': 4, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 4, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.090865 (0.045192) with: {'hidden_layers_dim': 4, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 4, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.130924 (0.014293) with: {'hidden_layers_dim': 4, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.076736 (0.024860) with: {'hidden_layers_dim': 4, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 4, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.069840 (0.034633) with: {'hidden_layers_dim': 4, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.130119 (0.015222) with: {'hidden_layers_dim': 4, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.049824 (0.022867) with: {'hidden_layers_dim': 4, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 4, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.131790 (0.015206) with: {'hidden_layers_dim': 4, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.115016 (0.038597) with: {'hidden_layers_dim': 4, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.117384 (0.006159) with: {'hidden_layers_dim': 4, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.086717 (0.033041) with: {'hidden_layers_dim': 4, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 6, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.064752 (0.013607) with: {'hidden_layers_dim': 6, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 6, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.129957 (0.013747) with: {'hidden_layers_dim': 6, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.079815 (0.017663) with: {'hidden_layers_dim': 6, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131134 (0.016416) with: {'hidden_layers_dim': 6, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.049323 (0.005120) with: {'hidden_layers_dim': 6, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131757 (0.015331) with: {'hidden_layers_dim': 6, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.066274 (0.005075) with: {'hidden_layers_dim': 6, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 6, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.130958 (0.016649) with: {'hidden_layers_dim': 6, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.053837 (0.001911) with: {'hidden_layers_dim': 6, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.107958 (0.011833) with: {'hidden_layers_dim': 6, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.070835 (0.024283) with: {'hidden_layers_dim': 6, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.124022 (0.009493) with: {'hidden_layers_dim': 6, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.050575 (0.015861) with: {'hidden_layers_dim': 6, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.129862 (0.015237) with: {'hidden_layers_dim': 6, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.109866 (0.021301) with: {'hidden_layers_dim': 6, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.051238 (0.009758) with: {'hidden_layers_dim': 6, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.061849 (0.009257) with: {'hidden_layers_dim': 6, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.053756 (0.011163) with: {'hidden_layers_dim': 6, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 8, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.060313 (0.011411) with: {'hidden_layers_dim': 8, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131926 (0.015373) with: {'hidden_layers_dim': 8, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131615 (0.015648) with: {'hidden_layers_dim': 8, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.055819 (0.010350) with: {'hidden_layers_dim': 8, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.104507 (0.036170) with: {'hidden_layers_dim': 8, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.051387 (0.007715) with: {'hidden_layers_dim': 8, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 1}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 8, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.062911 (0.011940) with: {'hidden_layers_dim': 8, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.129266 (0.012605) with: {'hidden_layers_dim': 8, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.103431 (0.019026) with: {'hidden_layers_dim': 8, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.050332 (0.015857) with: {'hidden_layers_dim': 8, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.079936 (0.023751) with: {'hidden_layers_dim': 8, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.050873 (0.000565) with: {'hidden_layers_dim': 8, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 2}\n",
      "-0.131933 (0.015364) with: {'hidden_layers_dim': 8, 'optimizer': 'SGD', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.052152 (0.020708) with: {'hidden_layers_dim': 8, 'optimizer': 'RMSprop', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.107362 (0.003237) with: {'hidden_layers_dim': 8, 'optimizer': 'Adagrad', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.079517 (0.032262) with: {'hidden_layers_dim': 8, 'optimizer': 'Adadelta', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.052138 (0.006857) with: {'hidden_layers_dim': 8, 'optimizer': 'Adam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.073224 (0.020432) with: {'hidden_layers_dim': 8, 'optimizer': 'Adamax', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n",
      "-0.038747 (0.008302) with: {'hidden_layers_dim': 8, 'optimizer': 'Nadam', 'epochs': 1, 'batch_size': 10, 'num_hidden_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', num_hidden_layers=2, hidden_layers_dim=8):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers_dim, input_dim=9, activation='relu'))\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(hidden_layers_dim, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# model.fit(X, Y, epochs=1, batch_size=20)\n",
    "# score = model.score(X_test, y_test)\n",
    "# preds = model.predict(X_test)\n",
    "# score\n",
    "\n",
    "num_hidden_layers = [1, 2, 3]\n",
    "hidden_layers_dim = [4, 6, 8]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "batch_size = [100]\n",
    "epochs = [1]\n",
    "param_grid = dict(\n",
    "    optimizer=optimizer, \n",
    "    num_hidden_layers=num_hidden_layers, \n",
    "    hidden_layers_dim=hidden_layers_dim,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")\n",
    "print(param_grid)\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'type2_min': typeII_err\n",
    "}\n",
    "print(scoring)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring=typeII_err)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10,\n",
       " 'epochs': 1,\n",
       " 'hidden_layers_dim': 8,\n",
       " 'num_hidden_layers': 3,\n",
       " 'optimizer': 'Nadam'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.42857143,  0.75      ],\n",
       "       [ 0.05263158,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.42857143,  0.75      ],\n",
       "       [ 0.10526316,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.42857143,  0.75      ],\n",
       "       ..., \n",
       "       [ 0.47368422,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.42857143,  0.91666669],\n",
       "       [ 0.52631581,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.42857143,  0.91666669],\n",
       "       [ 0.57894737,  0.00104493,  0.        , ...,  0.        ,\n",
       "         0.42857143,  0.91666669]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.astype(numpy.float32)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y.astype(numpy.float32)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input31716\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input31716\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 246s 3ms/step - loss: 0.3063 - acc: 0.8686\n",
      "94848/98520 [===========================>..] - ETA: 9s - loss: 0.3058 - acc: 0.8722Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input32685\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98520/98520 [==============================] - 257s 3ms/step - loss: 0.3025 - acc: 0.8731\n",
      " 4256/98520 [>.............................] - ETA: 3:51 - loss: 0.5469 - acc: 0.8597Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabirkhan/Documents/ML_Experiments/py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input32685\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39904/98520 [===========>..................] - ETA: 3:11 - loss: 0.3486 - acc: 0.8668"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a2a38fdfd03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbagg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbagg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 373\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "bagg = BaggingClassifier(model)\n",
    "results = cross_val_score(bagg, X, Y)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82201583,  0.04880566],\n",
       "       [ 0.03359724,  0.09558127]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, preds) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[  0.00000000e+00,   1.05000000e+02,   8.40000000e+01, ...,\n",
       "            7.41000000e-01,   6.20000000e+01,   1.00000000e+00],\n",
       "         [  2.00000000e+00,   1.22000000e+02,   6.00000000e+01, ...,\n",
       "            7.17000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  8.00000000e+00,   1.33000000e+02,   7.20000000e+01, ...,\n",
       "            2.70000000e-01,   3.90000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+00,   8.70000000e+01,   7.80000000e+01, ...,\n",
       "            1.01000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  5.00000000e+00,   7.70000000e+01,   8.20000000e+01, ...,\n",
       "            1.56000000e-01,   3.50000000e+01,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   1.46000000e+02,   8.20000000e+01, ...,\n",
       "            1.78100000e+00,   4.40000000e+01,   0.00000000e+00]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "         ..., \n",
       "         [   2.   ,  122.   ,   70.   , ...,    0.34 ,   27.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  2.00000000e+00,   1.25000000e+02,   6.00000000e+01, ...,\n",
       "            8.80000000e-02,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   9.20000000e+01,   5.20000000e+01, ...,\n",
       "            1.41000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  7.00000000e+00,   1.05000000e+02,   0.00000000e+00, ...,\n",
       "            3.05000000e-01,   2.40000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  0.00000000e+00,   1.05000000e+02,   6.40000000e+01, ...,\n",
       "            1.73000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  5.00000000e+00,   9.70000000e+01,   7.60000000e+01, ...,\n",
       "            3.78000000e-01,   5.20000000e+01,   1.00000000e+00],\n",
       "         [  8.00000000e+00,   1.08000000e+02,   7.00000000e+01, ...,\n",
       "            9.55000000e-01,   3.30000000e+01,   1.00000000e+00]]),\n",
       "  array([[   1.   ,   89.   ,   66.   , ...,    0.167,   21.   ,    0.   ],\n",
       "         [   0.   ,  137.   ,   40.   , ...,    2.288,   33.   ,    1.   ],\n",
       "         [   3.   ,   78.   ,   50.   , ...,    0.248,   26.   ,    1.   ],\n",
       "         ..., \n",
       "         [  10.   ,  101.   ,   76.   , ...,    0.171,   63.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[   7.   ,  168.   ,   88.   , ...,    0.787,   40.   ,    1.   ],\n",
       "         [   1.   ,   81.   ,   72.   , ...,    0.283,   24.   ,    0.   ],\n",
       "         [   7.   ,  150.   ,   66.   , ...,    0.718,   42.   ,    0.   ],\n",
       "         ..., \n",
       "         [   8.   ,  186.   ,   90.   , ...,    0.423,   37.   ,    1.   ],\n",
       "         [   9.   ,  165.   ,   88.   , ...,    0.302,   49.   ,    1.   ],\n",
       "         [   9.   ,  119.   ,   80.   , ...,    0.263,   29.   ,    1.   ]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "         [   5.   ,  116.   ,   74.   , ...,    0.201,   30.   ,    0.   ],\n",
       "         ..., \n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  1.00000000e+00,   1.47000000e+02,   9.40000000e+01, ...,\n",
       "            3.58000000e-01,   2.70000000e+01,   1.00000000e+00],\n",
       "         [  7.00000000e+00,   1.95000000e+02,   7.00000000e+01, ...,\n",
       "            1.63000000e-01,   5.50000000e+01,   1.00000000e+00],\n",
       "         [  7.00000000e+00,   1.24000000e+02,   7.00000000e+01, ...,\n",
       "            1.61000000e-01,   3.70000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+00,   1.28000000e+02,   4.80000000e+01, ...,\n",
       "            6.13000000e-01,   2.40000000e+01,   1.00000000e+00],\n",
       "         [  2.00000000e+00,   9.60000000e+01,   6.80000000e+01, ...,\n",
       "            6.47000000e-01,   2.60000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   9.10000000e+01,   6.40000000e+01, ...,\n",
       "            1.92000000e-01,   2.10000000e+01,   0.00000000e+00]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "         ..., \n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[   6.   ,  103.   ,   66.   , ...,    0.249,   29.   ,    0.   ],\n",
       "         [   0.   ,  138.   ,   60.   , ...,    0.534,   21.   ,    1.   ],\n",
       "         [   0.   ,  145.   ,    0.   , ...,    0.63 ,   31.   ,    1.   ],\n",
       "         ..., \n",
       "         [   0.   ,  119.   ,   64.   , ...,    0.725,   23.   ,    0.   ],\n",
       "         [   2.   ,  105.   ,   80.   , ...,    0.711,   29.   ,    1.   ],\n",
       "         [   4.   ,  120.   ,   68.   , ...,    0.709,   34.   ,    0.   ]]),\n",
       "  array([[  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   8.90000000e+01,   6.60000000e+01, ...,\n",
       "            1.67000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   1.37000000e+02,   4.00000000e+01, ...,\n",
       "            2.28800000e+00,   3.30000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+01,   1.01000000e+02,   7.60000000e+01, ...,\n",
       "            1.71000000e-01,   6.30000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.22000000e+02,   7.00000000e+01, ...,\n",
       "            3.40000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00]])),\n",
       " (array([[   6.   ,  125.   ,   68.   , ...,    0.464,   32.   ,    0.   ],\n",
       "         [   5.   ,  112.   ,   66.   , ...,    0.261,   41.   ,    1.   ],\n",
       "         [   0.   ,   57.   ,   60.   , ...,    0.735,   67.   ,    0.   ],\n",
       "         ..., \n",
       "         [   2.   ,  100.   ,   68.   , ...,    0.324,   26.   ,    0.   ],\n",
       "         [   1.   ,  163.   ,   72.   , ...,    1.222,   33.   ,    1.   ],\n",
       "         [   8.   ,  125.   ,   96.   , ...,    0.232,   54.   ,    1.   ]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   0.   ,  137.   ,   40.   , ...,    2.288,   33.   ,    1.   ],\n",
       "         ..., \n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  1.00000000e+00,   1.14000000e+02,   6.60000000e+01, ...,\n",
       "            2.89000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  4.00000000e+00,   1.89000000e+02,   1.10000000e+02, ...,\n",
       "            6.80000000e-01,   3.70000000e+01,   0.00000000e+00],\n",
       "         [  9.00000000e+00,   1.54000000e+02,   7.80000000e+01, ...,\n",
       "            1.64000000e-01,   4.50000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  4.00000000e+00,   8.40000000e+01,   9.00000000e+01, ...,\n",
       "            1.59000000e-01,   2.50000000e+01,   0.00000000e+00],\n",
       "         [  5.00000000e+00,   1.11000000e+02,   7.20000000e+01, ...,\n",
       "            4.07000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  5.00000000e+00,   1.17000000e+02,   9.20000000e+01, ...,\n",
       "            3.37000000e-01,   3.80000000e+01,   0.00000000e+00]]),\n",
       "  array([[  1.00000000e+00,   8.50000000e+01,   6.60000000e+01, ...,\n",
       "            3.51000000e-01,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   8.90000000e+01,   6.60000000e+01, ...,\n",
       "            1.67000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  9.00000000e+00,   1.70000000e+02,   7.40000000e+01, ...,\n",
       "            4.03000000e-01,   4.30000000e+01,   1.00000000e+00],\n",
       "         [  9.00000000e+00,   8.90000000e+01,   6.20000000e+01, ...,\n",
       "            1.42000000e-01,   3.30000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00]])),\n",
       " (array([[  1.00000000e+00,   1.30000000e+02,   7.00000000e+01, ...,\n",
       "            4.72000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.22000000e+02,   6.00000000e+01, ...,\n",
       "            7.17000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.12000000e+02,   7.50000000e+01, ...,\n",
       "            1.48000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+00,   9.00000000e+01,   6.80000000e+01, ...,\n",
       "            1.13800000e+00,   3.60000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   1.20000000e+02,   7.00000000e+01, ...,\n",
       "            4.52000000e-01,   3.00000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   1.91000000e+02,   6.80000000e+01, ...,\n",
       "            2.99000000e-01,   3.40000000e+01,   0.00000000e+00]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   1.   ,   89.   ,   66.   , ...,    0.167,   21.   ,    0.   ],\n",
       "         ..., \n",
       "         [  10.   ,  101.   ,   76.   , ...,    0.171,   63.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  10.   ,  122.   ,   78.   , ...,    0.512,   45.   ,    0.   ],\n",
       "         [   0.   ,  132.   ,   78.   , ...,    0.393,   21.   ,    0.   ],\n",
       "         [   0.   ,  152.   ,   82.   , ...,    0.27 ,   27.   ,    0.   ],\n",
       "         ..., \n",
       "         [   8.   ,  110.   ,   76.   , ...,    0.237,   58.   ,    0.   ],\n",
       "         [   7.   ,  150.   ,   78.   , ...,    0.692,   54.   ,    1.   ],\n",
       "         [   0.   ,  146.   ,   82.   , ...,    1.781,   44.   ,    0.   ]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "         ..., \n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  0.00000000e+00,   1.40000000e+02,   6.50000000e+01, ...,\n",
       "            4.31000000e-01,   2.40000000e+01,   1.00000000e+00],\n",
       "         [  0.00000000e+00,   7.40000000e+01,   5.20000000e+01, ...,\n",
       "            2.69000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.53000000e+02,   8.20000000e+01, ...,\n",
       "            6.87000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  3.00000000e+00,   1.21000000e+02,   5.20000000e+01, ...,\n",
       "            1.27000000e-01,   2.50000000e+01,   1.00000000e+00],\n",
       "         [  3.00000000e+00,   7.80000000e+01,   7.00000000e+01, ...,\n",
       "            2.70000000e-01,   3.90000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   9.50000000e+01,   8.20000000e+01, ...,\n",
       "            2.33000000e-01,   4.30000000e+01,   1.00000000e+00]]),\n",
       "  array([[  6.00000000e+00,   1.48000000e+02,   7.20000000e+01, ...,\n",
       "            6.27000000e-01,   5.00000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   8.50000000e+01,   6.60000000e+01, ...,\n",
       "            3.51000000e-01,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+01,   1.15000000e+02,   0.00000000e+00, ...,\n",
       "            1.34000000e-01,   2.90000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+01,   1.01000000e+02,   7.60000000e+01, ...,\n",
       "            1.71000000e-01,   6.30000000e+01,   0.00000000e+00],\n",
       "         [  5.00000000e+00,   1.21000000e+02,   7.20000000e+01, ...,\n",
       "            2.45000000e-01,   3.00000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00]])),\n",
       " (array([[  1.00000000e+00,   1.11000000e+02,   6.20000000e+01, ...,\n",
       "            1.38000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         [  7.00000000e+00,   1.14000000e+02,   6.40000000e+01, ...,\n",
       "            7.32000000e-01,   3.40000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   1.18000000e+02,   5.80000000e+01, ...,\n",
       "            2.61000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  4.00000000e+00,   1.54000000e+02,   7.20000000e+01, ...,\n",
       "            3.38000000e-01,   3.70000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.02000000e+02,   8.60000000e+01, ...,\n",
       "            1.27000000e-01,   2.30000000e+01,   1.00000000e+00],\n",
       "         [  2.00000000e+00,   1.12000000e+02,   6.80000000e+01, ...,\n",
       "            3.15000000e-01,   2.60000000e+01,   0.00000000e+00]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   1.   ,   89.   ,   66.   , ...,    0.167,   21.   ,    0.   ],\n",
       "         ..., \n",
       "         [   2.   ,  122.   ,   70.   , ...,    0.34 ,   27.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  1.00000000e+00,   1.09000000e+02,   6.00000000e+01, ...,\n",
       "            9.47000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  8.00000000e+00,   1.96000000e+02,   7.60000000e+01, ...,\n",
       "            6.05000000e-01,   5.70000000e+01,   1.00000000e+00],\n",
       "         [  3.00000000e+00,   1.50000000e+02,   7.60000000e+01, ...,\n",
       "            2.07000000e-01,   3.70000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  4.00000000e+00,   1.34000000e+02,   7.20000000e+01, ...,\n",
       "            2.77000000e-01,   6.00000000e+01,   1.00000000e+00],\n",
       "         [  7.00000000e+00,   1.96000000e+02,   9.00000000e+01, ...,\n",
       "            4.51000000e-01,   4.10000000e+01,   1.00000000e+00],\n",
       "         [  1.10000000e+01,   1.27000000e+02,   1.06000000e+02, ...,\n",
       "            1.90000000e-01,   5.10000000e+01,   0.00000000e+00]]),\n",
       "  array([[  6.00000000e+00,   1.48000000e+02,   7.20000000e+01, ...,\n",
       "            6.27000000e-01,   5.00000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   8.50000000e+01,   6.60000000e+01, ...,\n",
       "            3.51000000e-01,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  9.00000000e+00,   8.90000000e+01,   6.20000000e+01, ...,\n",
       "            1.42000000e-01,   3.30000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+01,   1.01000000e+02,   7.60000000e+01, ...,\n",
       "            1.71000000e-01,   6.30000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   9.30000000e+01,   7.00000000e+01, ...,\n",
       "            3.15000000e-01,   2.30000000e+01,   0.00000000e+00]])),\n",
       " (array([[  2.00000000e+00,   6.80000000e+01,   6.20000000e+01, ...,\n",
       "            2.57000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   1.13000000e+02,   4.40000000e+01, ...,\n",
       "            1.40000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  1.30000000e+01,   1.53000000e+02,   8.80000000e+01, ...,\n",
       "            1.17400000e+00,   3.90000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  2.00000000e+00,   8.30000000e+01,   6.50000000e+01, ...,\n",
       "            6.29000000e-01,   2.40000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   9.00000000e+01,   8.00000000e+01, ...,\n",
       "            2.49000000e-01,   2.40000000e+01,   0.00000000e+00],\n",
       "         [  6.00000000e+00,   1.83000000e+02,   9.40000000e+01, ...,\n",
       "            1.46100000e+00,   4.50000000e+01,   0.00000000e+00]]),\n",
       "  array([[   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "         [   0.   ,  137.   ,   40.   , ...,    2.288,   33.   ,    1.   ],\n",
       "         ..., \n",
       "         [   2.   ,  122.   ,   70.   , ...,    0.34 ,   27.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ]])),\n",
       " (array([[  1.00000000e+00,   1.28000000e+02,   8.20000000e+01, ...,\n",
       "            1.15000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.11000000e+02,   8.60000000e+01, ...,\n",
       "            1.43000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   1.16000000e+02,   0.00000000e+00, ...,\n",
       "            1.87000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  8.00000000e+00,   1.24000000e+02,   7.60000000e+01, ...,\n",
       "            6.87000000e-01,   5.20000000e+01,   1.00000000e+00],\n",
       "         [  4.00000000e+00,   8.30000000e+01,   8.60000000e+01, ...,\n",
       "            3.17000000e-01,   3.40000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   8.70000000e+01,   5.80000000e+01, ...,\n",
       "            1.66000000e-01,   2.50000000e+01,   0.00000000e+00]]),\n",
       "  array([[  1.00000000e+00,   8.90000000e+01,   6.60000000e+01, ...,\n",
       "            1.67000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   1.37000000e+02,   4.00000000e+01, ...,\n",
       "            2.28800000e+00,   3.30000000e+01,   1.00000000e+00],\n",
       "         [  3.00000000e+00,   7.80000000e+01,   5.00000000e+01, ...,\n",
       "            2.48000000e-01,   2.60000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  6.00000000e+00,   1.90000000e+02,   9.20000000e+01, ...,\n",
       "            2.78000000e-01,   6.60000000e+01,   1.00000000e+00],\n",
       "         [  2.00000000e+00,   8.80000000e+01,   5.80000000e+01, ...,\n",
       "            7.66000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  9.00000000e+00,   1.70000000e+02,   7.40000000e+01, ...,\n",
       "            4.03000000e-01,   4.30000000e+01,   1.00000000e+00]])),\n",
       " (array([[  2.00000000e+00,   8.20000000e+01,   5.20000000e+01, ...,\n",
       "            1.69900000e+00,   2.50000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   9.60000000e+01,   5.60000000e+01, ...,\n",
       "            9.44000000e-01,   3.90000000e+01,   0.00000000e+00],\n",
       "         [  4.00000000e+00,   1.54000000e+02,   6.20000000e+01, ...,\n",
       "            2.37000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+00,   1.40000000e+02,   7.40000000e+01, ...,\n",
       "            8.28000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.97000000e+02,   7.00000000e+01, ...,\n",
       "            1.58000000e-01,   5.30000000e+01,   1.00000000e+00],\n",
       "         [  5.00000000e+00,   7.70000000e+01,   8.20000000e+01, ...,\n",
       "            1.56000000e-01,   3.50000000e+01,   0.00000000e+00]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   89.   ,   66.   , ...,    0.167,   21.   ,    0.   ],\n",
       "         [   0.   ,  137.   ,   40.   , ...,    2.288,   33.   ,    1.   ],\n",
       "         ..., \n",
       "         [  10.   ,  101.   ,   76.   , ...,    0.171,   63.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ]])),\n",
       " (array([[  8.00000000e+00,   1.05000000e+02,   1.00000000e+02, ...,\n",
       "            2.39000000e-01,   4.50000000e+01,   1.00000000e+00],\n",
       "         [  0.00000000e+00,   1.80000000e+02,   9.00000000e+01, ...,\n",
       "            3.14000000e-01,   3.50000000e+01,   1.00000000e+00],\n",
       "         [  9.00000000e+00,   8.90000000e+01,   6.20000000e+01, ...,\n",
       "            1.42000000e-01,   3.30000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  6.00000000e+00,   1.90000000e+02,   9.20000000e+01, ...,\n",
       "            2.78000000e-01,   6.60000000e+01,   1.00000000e+00],\n",
       "         [  5.00000000e+00,   0.00000000e+00,   8.00000000e+01, ...,\n",
       "            3.46000000e-01,   3.70000000e+01,   1.00000000e+00],\n",
       "         [  5.00000000e+00,   4.40000000e+01,   6.20000000e+01, ...,\n",
       "            5.87000000e-01,   3.60000000e+01,   0.00000000e+00]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "         ..., \n",
       "         [   7.   ,  137.   ,   90.   , ...,    0.391,   39.   ,    0.   ],\n",
       "         [   9.   ,  170.   ,   74.   , ...,    0.403,   43.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  1.00000000e+00,   1.43000000e+02,   8.60000000e+01, ...,\n",
       "            8.92000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         [  4.00000000e+00,   8.40000000e+01,   9.00000000e+01, ...,\n",
       "            1.59000000e-01,   2.50000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.25000000e+02,   6.00000000e+01, ...,\n",
       "            8.80000000e-02,   3.10000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  7.00000000e+00,   1.36000000e+02,   9.00000000e+01, ...,\n",
       "            2.10000000e-01,   5.00000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   8.80000000e+01,   5.80000000e+01, ...,\n",
       "            7.66000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.12000000e+02,   7.50000000e+01, ...,\n",
       "            1.48000000e-01,   2.10000000e+01,   0.00000000e+00]]),\n",
       "  array([[  6.00000000e+00,   1.48000000e+02,   7.20000000e+01, ...,\n",
       "            6.27000000e-01,   5.00000000e+01,   1.00000000e+00],\n",
       "         [  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+01,   1.15000000e+02,   0.00000000e+00, ...,\n",
       "            1.34000000e-01,   2.90000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  2.00000000e+00,   1.22000000e+02,   7.00000000e+01, ...,\n",
       "            3.40000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   9.30000000e+01,   7.00000000e+01, ...,\n",
       "            3.15000000e-01,   2.30000000e+01,   0.00000000e+00]])),\n",
       " (array([[   4.   ,  116.   ,   72.   , ...,    0.463,   37.   ,    0.   ],\n",
       "         [   2.   ,  106.   ,   56.   , ...,    0.426,   22.   ,    0.   ],\n",
       "         [   1.   ,  144.   ,   82.   , ...,    0.335,   46.   ,    1.   ],\n",
       "         ..., \n",
       "         [   0.   ,  132.   ,   78.   , ...,    0.393,   21.   ,    0.   ],\n",
       "         [   2.   ,  175.   ,   88.   , ...,    0.326,   22.   ,    0.   ],\n",
       "         [   3.   ,  163.   ,   70.   , ...,    0.268,   28.   ,    1.   ]]),\n",
       "  array([[   0.   ,  137.   ,   40.   , ...,    2.288,   33.   ,    1.   ],\n",
       "         [   5.   ,  116.   ,   74.   , ...,    0.201,   30.   ,    0.   ],\n",
       "         [   3.   ,   78.   ,   50.   , ...,    0.248,   26.   ,    1.   ],\n",
       "         ..., \n",
       "         [  10.   ,  101.   ,   76.   , ...,    0.171,   63.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ]])),\n",
       " (array([[   1.   ,  109.   ,   56.   , ...,    0.833,   23.   ,    0.   ],\n",
       "         [   0.   ,   86.   ,   68.   , ...,    0.238,   25.   ,    0.   ],\n",
       "         [   2.   ,  120.   ,   54.   , ...,    0.455,   27.   ,    0.   ],\n",
       "         ..., \n",
       "         [   9.   ,  170.   ,   74.   , ...,    0.403,   43.   ,    1.   ],\n",
       "         [  11.   ,  138.   ,   74.   , ...,    0.557,   50.   ,    1.   ],\n",
       "         [  10.   ,   68.   ,  106.   , ...,    0.285,   47.   ,    0.   ]]),\n",
       "  array([[  6.00000000e+00,   1.48000000e+02,   7.20000000e+01, ...,\n",
       "            6.27000000e-01,   5.00000000e+01,   1.00000000e+00],\n",
       "         [  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   8.90000000e+01,   6.60000000e+01, ...,\n",
       "            1.67000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  5.00000000e+00,   1.21000000e+02,   7.20000000e+01, ...,\n",
       "            2.45000000e-01,   3.00000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   9.30000000e+01,   7.00000000e+01, ...,\n",
       "            3.15000000e-01,   2.30000000e+01,   0.00000000e+00]])),\n",
       " (array([[  2.00000000e+00,   1.46000000e+02,   0.00000000e+00, ...,\n",
       "            2.40000000e-01,   2.80000000e+01,   1.00000000e+00],\n",
       "         [  2.00000000e+00,   1.02000000e+02,   8.60000000e+01, ...,\n",
       "            1.27000000e-01,   2.30000000e+01,   1.00000000e+00],\n",
       "         [  4.00000000e+00,   1.58000000e+02,   7.80000000e+01, ...,\n",
       "            8.03000000e-01,   3.10000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  8.00000000e+00,   1.86000000e+02,   9.00000000e+01, ...,\n",
       "            4.23000000e-01,   3.70000000e+01,   1.00000000e+00],\n",
       "         [  3.00000000e+00,   1.22000000e+02,   7.80000000e+01, ...,\n",
       "            2.54000000e-01,   4.00000000e+01,   0.00000000e+00],\n",
       "         [  4.00000000e+00,   1.10000000e+02,   6.60000000e+01, ...,\n",
       "            4.71000000e-01,   2.90000000e+01,   0.00000000e+00]]),\n",
       "  array([[  1.00000000e+00,   8.50000000e+01,   6.60000000e+01, ...,\n",
       "            3.51000000e-01,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         [  5.00000000e+00,   1.16000000e+02,   7.40000000e+01, ...,\n",
       "            2.01000000e-01,   3.00000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  9.00000000e+00,   8.90000000e+01,   6.20000000e+01, ...,\n",
       "            1.42000000e-01,   3.30000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   9.30000000e+01,   7.00000000e+01, ...,\n",
       "            3.15000000e-01,   2.30000000e+01,   0.00000000e+00]])),\n",
       " (array([[   2.   ,  158.   ,   90.   , ...,    0.805,   66.   ,    1.   ],\n",
       "         [   0.   ,  104.   ,   64.   , ...,    0.454,   23.   ,    0.   ],\n",
       "         [   0.   ,  105.   ,   68.   , ...,    0.236,   22.   ,    0.   ],\n",
       "         ..., \n",
       "         [   5.   ,  143.   ,   78.   , ...,    0.19 ,   47.   ,    0.   ],\n",
       "         [   1.   ,   71.   ,   78.   , ...,    0.422,   21.   ,    0.   ],\n",
       "         [   7.   ,   83.   ,   78.   , ...,    0.767,   36.   ,    0.   ]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   3.   ,   78.   ,   50.   , ...,    0.248,   26.   ,    1.   ],\n",
       "         [   8.   ,  125.   ,   96.   , ...,    0.232,   54.   ,    1.   ],\n",
       "         ..., \n",
       "         [   2.   ,  122.   ,   70.   , ...,    0.34 ,   27.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[  0.00000000e+00,   1.37000000e+02,   4.00000000e+01, ...,\n",
       "            2.28800000e+00,   3.30000000e+01,   1.00000000e+00],\n",
       "         [  0.00000000e+00,   1.62000000e+02,   7.60000000e+01, ...,\n",
       "            7.59000000e-01,   2.50000000e+01,   1.00000000e+00],\n",
       "         [  6.00000000e+00,   1.48000000e+02,   7.20000000e+01, ...,\n",
       "            6.27000000e-01,   5.00000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  4.00000000e+00,   1.46000000e+02,   8.50000000e+01, ...,\n",
       "            1.89000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.05000000e+02,   5.80000000e+01, ...,\n",
       "            2.25000000e-01,   2.50000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   1.13000000e+02,   4.40000000e+01, ...,\n",
       "            1.40000000e-01,   2.20000000e+01,   0.00000000e+00]]),\n",
       "  array([[   5.   ,  116.   ,   74.   , ...,    0.201,   30.   ,    0.   ],\n",
       "         [   3.   ,   78.   ,   50.   , ...,    0.248,   26.   ,    1.   ],\n",
       "         [   8.   ,  125.   ,   96.   , ...,    0.232,   54.   ,    1.   ],\n",
       "         ..., \n",
       "         [   2.   ,  122.   ,   70.   , ...,    0.34 ,   27.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[   1.   ,  117.   ,   88.   , ...,    0.403,   40.   ,    1.   ],\n",
       "         [   3.   ,  173.   ,   82.   , ...,    2.137,   25.   ,    1.   ],\n",
       "         [   7.   ,  133.   ,   88.   , ...,    0.262,   37.   ,    0.   ],\n",
       "         ..., \n",
       "         [   3.   ,  120.   ,   70.   , ...,    0.452,   30.   ,    0.   ],\n",
       "         [  10.   ,  125.   ,   70.   , ...,    0.205,   41.   ,    1.   ],\n",
       "         [   6.   ,  147.   ,   80.   , ...,    0.178,   50.   ,    1.   ]]),\n",
       "  array([[   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   5.   ,  116.   ,   74.   , ...,    0.201,   30.   ,    0.   ],\n",
       "         [  10.   ,  115.   ,    0.   , ...,    0.134,   29.   ,    0.   ],\n",
       "         ..., \n",
       "         [  10.   ,  101.   ,   76.   , ...,    0.171,   63.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])),\n",
       " (array([[   5.   ,   73.   ,   60.   , ...,    0.268,   27.   ,    0.   ],\n",
       "         [   2.   ,   82.   ,   52.   , ...,    1.699,   25.   ,    0.   ],\n",
       "         [   0.   ,  117.   ,    0.   , ...,    0.932,   44.   ,    0.   ],\n",
       "         ..., \n",
       "         [   2.   ,   92.   ,   76.   , ...,    1.698,   28.   ,    0.   ],\n",
       "         [   7.   ,  102.   ,   74.   , ...,    0.204,   45.   ,    0.   ],\n",
       "         [   1.   ,   89.   ,   24.   , ...,    0.559,   21.   ,    0.   ]]),\n",
       "  array([[  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         [  0.00000000e+00,   1.37000000e+02,   4.00000000e+01, ...,\n",
       "            2.28800000e+00,   3.30000000e+01,   1.00000000e+00],\n",
       "         [  3.00000000e+00,   7.80000000e+01,   5.00000000e+01, ...,\n",
       "            2.48000000e-01,   2.60000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+01,   1.01000000e+02,   7.60000000e+01, ...,\n",
       "            1.71000000e-01,   6.30000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   9.30000000e+01,   7.00000000e+01, ...,\n",
       "            3.15000000e-01,   2.30000000e+01,   0.00000000e+00]])),\n",
       " (array([[   5.   ,  117.   ,   86.   , ...,    0.251,   42.   ,    0.   ],\n",
       "         [   6.   ,  123.   ,   72.   , ...,    0.733,   34.   ,    0.   ],\n",
       "         [   0.   ,  101.   ,   62.   , ...,    0.336,   25.   ,    0.   ],\n",
       "         ..., \n",
       "         [   6.   ,  151.   ,   62.   , ...,    0.692,   28.   ,    0.   ],\n",
       "         [   4.   ,   76.   ,   62.   , ...,    0.391,   25.   ,    0.   ],\n",
       "         [   6.   ,  166.   ,   74.   , ...,    0.304,   66.   ,    0.   ]]),\n",
       "  array([[  6.00000000e+00,   1.48000000e+02,   7.20000000e+01, ...,\n",
       "            6.27000000e-01,   5.00000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   8.50000000e+01,   6.60000000e+01, ...,\n",
       "            3.51000000e-01,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  8.00000000e+00,   1.83000000e+02,   6.40000000e+01, ...,\n",
       "            6.72000000e-01,   3.20000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  9.00000000e+00,   8.90000000e+01,   6.20000000e+01, ...,\n",
       "            1.42000000e-01,   3.30000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.22000000e+02,   7.00000000e+01, ...,\n",
       "            3.40000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   9.30000000e+01,   7.00000000e+01, ...,\n",
       "            3.15000000e-01,   2.30000000e+01,   0.00000000e+00]])),\n",
       " (array([[   1.   ,   99.   ,   58.   , ...,    0.551,   21.   ,    0.   ],\n",
       "         [   0.   ,   98.   ,   82.   , ...,    0.299,   22.   ,    0.   ],\n",
       "         [   0.   ,   91.   ,   68.   , ...,    0.381,   25.   ,    0.   ],\n",
       "         ..., \n",
       "         [   0.   ,  107.   ,   76.   , ...,    0.686,   24.   ,    0.   ],\n",
       "         [   2.   ,  129.   ,   74.   , ...,    0.591,   25.   ,    0.   ],\n",
       "         [   2.   ,  100.   ,   66.   , ...,    0.867,   28.   ,    1.   ]]),\n",
       "  array([[  1.00000000e+00,   8.50000000e+01,   6.60000000e+01, ...,\n",
       "            3.51000000e-01,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   8.90000000e+01,   6.60000000e+01, ...,\n",
       "            1.67000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  5.00000000e+00,   1.16000000e+02,   7.40000000e+01, ...,\n",
       "            2.01000000e-01,   3.00000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  9.00000000e+00,   1.70000000e+02,   7.40000000e+01, ...,\n",
       "            4.03000000e-01,   4.30000000e+01,   1.00000000e+00],\n",
       "         [  2.00000000e+00,   1.22000000e+02,   7.00000000e+01, ...,\n",
       "            3.40000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   9.30000000e+01,   7.00000000e+01, ...,\n",
       "            3.15000000e-01,   2.30000000e+01,   0.00000000e+00]])),\n",
       " (array([[  1.00000000e+01,   1.79000000e+02,   7.00000000e+01, ...,\n",
       "            2.00000000e-01,   3.70000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.20000000e+02,   8.00000000e+01, ...,\n",
       "            1.16200000e+00,   4.10000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   1.11000000e+02,   5.80000000e+01, ...,\n",
       "            4.30000000e-01,   2.20000000e+01,   0.00000000e+00],\n",
       "         ..., \n",
       "         [  3.00000000e+00,   1.06000000e+02,   7.20000000e+01, ...,\n",
       "            2.07000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.01000000e+02,   5.80000000e+01, ...,\n",
       "            6.14000000e-01,   2.30000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.08000000e+02,   6.40000000e+01, ...,\n",
       "            1.58000000e-01,   2.10000000e+01,   0.00000000e+00]]),\n",
       "  array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "         [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "         [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "         ..., \n",
       "         [   2.   ,  122.   ,   70.   , ...,    0.34 ,   27.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ]])),\n",
       " (array([[  1.00000000e+00,   9.10000000e+01,   6.40000000e+01, ...,\n",
       "            1.92000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   1.73000000e+02,   7.80000000e+01, ...,\n",
       "            1.15900000e+00,   5.80000000e+01,   0.00000000e+00],\n",
       "         [  2.00000000e+00,   1.46000000e+02,   0.00000000e+00, ...,\n",
       "            2.40000000e-01,   2.80000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+00,   1.07000000e+02,   5.00000000e+01, ...,\n",
       "            1.81000000e-01,   2.90000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.03000000e+02,   3.00000000e+01, ...,\n",
       "            1.83000000e-01,   3.30000000e+01,   0.00000000e+00],\n",
       "         [  7.00000000e+00,   1.96000000e+02,   9.00000000e+01, ...,\n",
       "            4.51000000e-01,   4.10000000e+01,   1.00000000e+00]]),\n",
       "  array([[   1.   ,   89.   ,   66.   , ...,    0.167,   21.   ,    0.   ],\n",
       "         [   0.   ,  137.   ,   40.   , ...,    2.288,   33.   ,    1.   ],\n",
       "         [   3.   ,   78.   ,   50.   , ...,    0.248,   26.   ,    1.   ],\n",
       "         ..., \n",
       "         [   2.   ,  122.   ,   70.   , ...,    0.34 ,   27.   ,    0.   ],\n",
       "         [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "         [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ]])),\n",
       " (array([[  1.10000000e+01,   1.43000000e+02,   9.40000000e+01, ...,\n",
       "            2.54000000e-01,   5.10000000e+01,   1.00000000e+00],\n",
       "         [  4.00000000e+00,   1.17000000e+02,   6.20000000e+01, ...,\n",
       "            3.80000000e-01,   3.00000000e+01,   1.00000000e+00],\n",
       "         [  4.00000000e+00,   1.58000000e+02,   7.80000000e+01, ...,\n",
       "            8.03000000e-01,   3.10000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  1.00000000e+01,   1.39000000e+02,   8.00000000e+01, ...,\n",
       "            1.44100000e+00,   5.70000000e+01,   0.00000000e+00],\n",
       "         [  8.00000000e+00,   1.55000000e+02,   6.20000000e+01, ...,\n",
       "            5.43000000e-01,   4.60000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   1.11000000e+02,   6.20000000e+01, ...,\n",
       "            1.38000000e-01,   2.30000000e+01,   0.00000000e+00]]),\n",
       "  array([[  1.00000000e+00,   8.90000000e+01,   6.60000000e+01, ...,\n",
       "            1.67000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  3.00000000e+00,   7.80000000e+01,   5.00000000e+01, ...,\n",
       "            2.48000000e-01,   2.60000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+01,   1.68000000e+02,   7.40000000e+01, ...,\n",
       "            5.37000000e-01,   3.40000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  2.00000000e+00,   1.22000000e+02,   7.00000000e+01, ...,\n",
       "            3.40000000e-01,   2.70000000e+01,   0.00000000e+00],\n",
       "         [  5.00000000e+00,   1.21000000e+02,   7.20000000e+01, ...,\n",
       "            2.45000000e-01,   3.00000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00]])),\n",
       " (array([[   6.   ,    0.   ,   68.   , ...,    0.727,   41.   ,    1.   ],\n",
       "         [   0.   ,  121.   ,   66.   , ...,    0.203,   33.   ,    1.   ],\n",
       "         [  12.   ,  100.   ,   84.   , ...,    0.488,   46.   ,    0.   ],\n",
       "         ..., \n",
       "         [   1.   ,   96.   ,  122.   , ...,    0.207,   27.   ,    0.   ],\n",
       "         [   3.   ,  111.   ,   56.   , ...,    0.557,   30.   ,    0.   ],\n",
       "         [   5.   ,  104.   ,   74.   , ...,    0.153,   48.   ,    0.   ]]),\n",
       "  array([[  1.00000000e+00,   8.50000000e+01,   6.60000000e+01, ...,\n",
       "            3.51000000e-01,   3.10000000e+01,   0.00000000e+00],\n",
       "         [  1.00000000e+00,   8.90000000e+01,   6.60000000e+01, ...,\n",
       "            1.67000000e-01,   2.10000000e+01,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   1.37000000e+02,   4.00000000e+01, ...,\n",
       "            2.28800000e+00,   3.30000000e+01,   1.00000000e+00],\n",
       "         ..., \n",
       "         [  0.00000000e+00,   1.23000000e+02,   7.20000000e+01, ...,\n",
       "            2.58000000e-01,   5.20000000e+01,   1.00000000e+00],\n",
       "         [  9.00000000e+00,   1.70000000e+02,   7.40000000e+01, ...,\n",
       "            4.03000000e-01,   4.30000000e+01,   1.00000000e+00],\n",
       "         [  1.00000000e+00,   1.26000000e+02,   6.00000000e+01, ...,\n",
       "            3.49000000e-01,   4.70000000e+01,   1.00000000e+00]]))]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_resample(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scale to all Microsoft courses on edx.org and openedx.microsoft.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e584b3f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4gAAAEaCAYAAABTgXmvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIUMyBKkiRJkgD4/073syhsIOiPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e5a8800f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "#My dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Weeks' : [201605, 201606, 201607, 201608],\n",
    "    'Computer1' : [50, 77, 96, 100],\n",
    "    'Computer2' : [50, 79, 100, 80],\n",
    "    'Laptop1'   : [75, 77, 96, 95],\n",
    "    'Laptop2'   : [86, 77, 96, 40],\n",
    "    'Phone'     : [99, 99, 44, 85],\n",
    "    'Phone2'    : [93, 77, 96, 25],\n",
    "    'Phone3'    : [94, 91, 96, 33]\n",
    "})\n",
    "df2 = df.set_index('Weeks') #Makes the column 'Weeks' the index.\n",
    "\n",
    "colors = df2.applymap(lambda x: 'green' if x>= 80 else 'red') \\\n",
    "        .reset_index().drop(['Weeks'], axis=1)\n",
    "\n",
    "#print(colors)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1, frame_on=True) # no visible frame\n",
    "\n",
    "#ax.xaxis.set_visible(False)    # hide the x axis\n",
    "#ax.yaxis.set_visible(False)    # hide the y axis\n",
    "\n",
    "# hide all axises\n",
    "ax.axis('off')\n",
    "\n",
    "# http://matplotlib.org/api/pyplot_api.html?highlight=table#matplotlib.pyplot.table\n",
    "tbl = table(ax, df2,\n",
    "            loc='center',\n",
    "            cellLoc='center',\n",
    "            cellColours=colors.as_matrix(),\n",
    "            colColours=['black']*len(colors.columns),\n",
    "            rowColours=['black']*len(colors),\n",
    "            #fontsize=14\n",
    "      )\n",
    "\n",
    "\n",
    "# set color for index (X, -1) and headers (0, X)\n",
    "for key, cell in tbl.get_celld().items():\n",
    "    if key[1] == -1 or key[0] == 0:\n",
    "        cell._text.set_color('white')\n",
    "    # remove grid lines\n",
    "    cell.set_linewidth(0)\n",
    "\n",
    "# refresh table\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14847381</td>\n",
       "      <td>20171011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16333494</td>\n",
       "      <td>20171011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14046171</td>\n",
       "      <td>20171019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15785146</td>\n",
       "      <td>20171019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16283709</td>\n",
       "      <td>20171020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  date_key\n",
       "0  14847381  20171011\n",
       "1  16333494  20171011\n",
       "2  14046171  20171019\n",
       "3  15785146  20171019\n",
       "4  16283709  20171020"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_completes = pd.read_csv('../edx-learner-attrition/data/Microsoft+DAT222x+4T2017/course_completions.csv')\n",
    "print(course_completes.shape)\n",
    "course_completes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, date_key]\n",
       "Index: []"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_completes[course_completes['user_id'] == 1037128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_pivot = pd.read_csv('../edx-learner-attrition/data/Microsoft+DAT222x+4T2017/predicted_pivot.csv')\n",
    "real_pivot = pd.read_csv('../edx-learner-attrition/data/Microsoft+DAT222x+4T2017/real_pivot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[14847381 16333494 14046171 15785146 16283709 15225775  1691127 14933835\\n 14657782 15284301 15791426 15787928 15706399 13208401 13803757 16189199\\n  1919588 14286833 15323150 14274931 15427855  3616218   586304 15723552\\n 16239849 16261278 16319035  3635642 11668207 13398821  8836292 14975947\\n 15029841 15229757 15623076 13008556 16066582  4434940  6004394 14419930\\n 15787639 12271508 14947271 16408499  1376439  5311132 14813608  3248862\\n 12178126 16153116 16381774 15942482 16140287  9593743 16474509 12454321\\n 13891136 13920778 13981837 12098236 15324089 16042558 16249012 16527841\\n 10156096 14804836 11654091 16402355   743032 11338447 15154653 15507253\\n 16153487 16210961 16321907 13823702 14459513 15329258  1164473 11530211\\n 13736216 13787919 15030201 13332795 15223218 15454311 16338657 16431110\\n 12189530 12802361 13856463 15099935  8065181 15106080 14533212 15686774\\n 15945956  3004673 11361558  2889464 11199203 16315000  2925444 13855943\\n 14531284  9989328  7775658 13891027 14272403 14579234 13278487 12666771\\n  5619468 12405322  6676756 13103099 15102472 16094971 16178633 12890388\\n 10900219  3974657 13947962 15567929 15336398 15819938 16277238 16285726\\n  8112076  6401782   958351 13915880 15856851 13824327 16322474 16665997\\n 11588511 14829917 16242221 16372754  5088346 15597096 15684998 16703452\\n 13891065 15818742 16584724 13906855 16133242  7800906] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-475742edb97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_pivot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcourse_completes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[14847381 16333494 14046171 15785146 16283709 15225775  1691127 14933835\\n 14657782 15284301 15791426 15787928 15706399 13208401 13803757 16189199\\n  1919588 14286833 15323150 14274931 15427855  3616218   586304 15723552\\n 16239849 16261278 16319035  3635642 11668207 13398821  8836292 14975947\\n 15029841 15229757 15623076 13008556 16066582  4434940  6004394 14419930\\n 15787639 12271508 14947271 16408499  1376439  5311132 14813608  3248862\\n 12178126 16153116 16381774 15942482 16140287  9593743 16474509 12454321\\n 13891136 13920778 13981837 12098236 15324089 16042558 16249012 16527841\\n 10156096 14804836 11654091 16402355   743032 11338447 15154653 15507253\\n 16153487 16210961 16321907 13823702 14459513 15329258  1164473 11530211\\n 13736216 13787919 15030201 13332795 15223218 15454311 16338657 16431110\\n 12189530 12802361 13856463 15099935  8065181 15106080 14533212 15686774\\n 15945956  3004673 11361558  2889464 11199203 16315000  2925444 13855943\\n 14531284  9989328  7775658 13891027 14272403 14579234 13278487 12666771\\n  5619468 12405322  6676756 13103099 15102472 16094971 16178633 12890388\\n 10900219  3974657 13947962 15567929 15336398 15819938 16277238 16285726\\n  8112076  6401782   958351 13915880 15856851 13824327 16322474 16665997\\n 11588511 14829917 16242221 16372754  5088346 15597096 15684998 16703452\\n 13891065 15818742 16584724 13906855 16133242  7800906] not in index'"
     ]
    }
   ],
   "source": [
    "pred_pivot[course_completes['user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_pivot = pred_pivot.set_index('user_id')\n",
    "real_pivot = real_pivot.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colors(s):\n",
    "    ret = 'background-color: {}'\n",
    "    if s == 0:\n",
    "        ret = ret.format('green')\n",
    "    elif s == 1:\n",
    "        ret = ret.format('red')\n",
    "    else:\n",
    "        ret = ret.format('gray')\n",
    "        \n",
    "    return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_pivot_colored = pred_pivot.style.applymap(colors)\n",
    "real_pivot_colored = real_pivot.style.applymap(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_pivot_colored.to_excel('../edx-learner-attrition/data/Microsoft+DAT222x+4T2017/predicted_pivot.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Image size of 920x106082 pixels is too large. It must be less than 2^16 in each direction.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2250\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m   2253\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FigureCanvasAgg.draw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'debug-annoying'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;31m# acquire a lock on the shared font cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_new_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Experiments/py35/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RendererAgg.__init__ width=%s, height=%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'debug-annoying'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Image size of 920x106082 pixels is too large. It must be less than 2^16 in each direction."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe10322bf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1, frame_on=True) # no visible frame\n",
    "\n",
    "#ax.xaxis.set_visible(False)    # hide the x axis\n",
    "#ax.yaxis.set_visible(False)    # hide the y axis\n",
    "\n",
    "# hide all axises\n",
    "ax.axis('off')\n",
    "\n",
    "tbl = table(ax, pred_pivot,\n",
    "    loc='center',\n",
    "    cellLoc='center',\n",
    "    cellColours=pred_colors.as_matrix(),\n",
    "    colColours=['black']*len(pred_colors.columns),\n",
    "    rowColours=['black']*len(pred_colors)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
